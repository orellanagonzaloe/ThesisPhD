

El Modelo Estándar es la teoría que describe las partículas elementales y sus interacciones, desarrollada en la década de los 70 y con grandes predicciones experimentales, tales como el descubrimiento del bosón de Higgs en el año 2012. A partir de su formulación, surgieron nuevas extensiones intentando solucionar diferentes problemáticas del mismo. Una de las extensiones mejor motivadas teóricamente es Supersimetría (SUSY), que introduce un conjunto de partículas nuevas que aún no han sido observadas. Este modelo, entre otras cosas, presenta un escenario favorable para la inclusión de la gravedad al Modelo Estándar, y a su vez, dichas nuevas partículas podrían ser candidatos tanto a materia oscura, como a neutrinos pesados. Esto ha convertido a SUSY en una de las teorías de mayor interés y el mayor objetivo en el ámbito de la física experimental de altas energías.

Esta tesis presenta una búsqueda de nueva física motivada por modelos de SUSY que predicen estados finales con fotones energéticos y aislados, jets y momento transverso faltante elevado. La misma fue realizada utilizando el conjunto de datos de colisiones $pp$, provisto por el Gran Colisionador de Hadrones del CERN, y recolectado por el detector ATLAS durante los años 2015 y 2018, correspondientes a una luminosidad integrada de $139\,\ifb$. En el presente trabajo se desarrollaron y realizaron búsquedas guiadas por modelos de producción fuerte de partículas supersimétricas, en las cuales no se observaron excesos por sobre las predicciones del Modelo Estándar, por lo que se establecieron límites en el número de eventos de nueva física, y adicionalmente límites en la producción de gluinos con masas de \magn{2.2}{TeV}. A su vez, se describe complementariamente el marco para la búsqueda de partículas supersimétricas con producción débil, cuyo estado final es similar al anterior descripto, y obteniéndose una posible sensibilidad de exclusión para partículas con masas de hasta \magn{1.2}{TeV}.
El estudio detallado de los datos requirió además la medida de la eficiencia de selección de los triggers de fotones del detector ATLAS, cuya técnica se describe en el presente trabajo, con resultados actualmente utilizados en todos los análisis con selección online de fotones.


El Gran Colisionador de Hadrones (LHC) es el acelerador de partículas más grande y de mayor energía en todo el mundo, donde paquetes de protones colisionan 40 millones de veces por segundo para producir colisiones protón-protón. El mismo se encuentra en la frontera franco-suiza, aproximadamente a \magn{100}{m} bajo tierra, y consiste en un anillo de \magn{27}{km} de radio por donde circulan protones en direcciones opuestas. Los mismos colisionan en cuatro puntos estratégicos donde se encuentran los detectores principales: ATLAS, CMS, LHCb y ALICE.
Entre los años 2015 y 2018 se realizó la toma de datos denominada Run 2, en la cual los protones colisionaban con una energía de centro de masa de \magn{13}{TeV}, y lográndose recolectar una luminosidad total integrada $139\,\ifb$. En la actualidad el LHC se está preparando para comenzar con el Run 3, que colisionará protones con una energía de centro de masa de \magn{13.6}{TeV}, esperándose alcanzar una luminosidad total integrada de aproximadamente $300\,\ifb$.

Uno de los experimentos más importantes del LHC es ATLAS, un detector de uso general diseñado para realizar tanto mediciones de precisión dentro del Modelo Estándar (SM), como búsquedas de nuevos fenómenos asociados con física a allá del SM, que esperan ser observados en la escala TeV. El detector ATLAS se compone de distintos subdetectores que cumplen diferentes roles en la reconstrucción de las partículas producto de la colisión. El Detector Interno se encarga de medir las trazas de las partículas cargadas, los Calorímetros son los encargados de medir las deposiciones energéticas de fotones, electrones y diferentes hadrones, y finalmente el Espectrómetro de Muones permite medir la trayectoria de los muones. Intercalados entre ellos, se encuentra un poderoso sistema de imanes, que curva la trayectoria de las partículas cargadas. Por último, el detector ATLAS consta de un preciso Sistema de Trigger que filtra aquellos eventos de poco interés, reduciendo así la frecuencia de flujo de datos.



En el año 2012 las colaboraciones ATLAS y CMS publicaron resultados con el descubrimiento del bosón de Higgs, la partícula vinculada con el mecanismo de rompimiento espontáneo de simetría electrodébil, por el cual las partículas elementales adquieren masa. Dicho descubrimiento valió un premio Nobel a los físicos Peter Higgs y François Englert, que postularon el mecanismo que lleva su nombre, y que era una pieza clave del SM. Desde su postulación hasta la fecha, el SM ha realizado fuertes predicciones que han sido verificadas experimentalmente, convirtiéndolo en una de las teorías más importantes de toda la física. Sin embargo, son varios los interrogantes sin respuesta del 
SM, como por ejemplo, el patrón de las diferencias de masa de las partículas fundamentales, y el problema de la jerarquía en la enorme diferencia de 17 órdenes de magnitud entre las dos escalas fundamentales de física: la escala electrodébil y la escala de Planck. Una de las ideas más intensamente investigadas desde el punto de vista teórico entre los modelos más allá del SM, es la Supersimetría (SUSY). En su formulación mínima, SUSY predice que para cada partícula del SM existe un compañero cuyo spin difiere en $1/2$, y un sector de Higgs extendido con cinco bosones respectivos. La nueva simetría propuesta entre bosones y fermiones estabiliza la masa de las partículas escalares, como es el caso del bosón de Higgs. Si las partículas propuestas conservan la paridad R (número cuántico propuesto por la teoría) entonces las partículas SUSY son siempre producidas de a pares y la más liviana (LSP) no puede decaer, con lo cual las LSP primordiales serían candidatos a formar la materia oscura, otro de los misterios para el cual el SM todavía no tiene respuesta. Estas nuevas partículas supersimétricas pueden ser producidas en el LHC si su rango de masas está en la escala del TeV. 

La búsqueda de partículas SUSY en el LHC es entonces el objetivo más general del presente trabajo, en particular dentro del contexto del modelo General Gauge Mediated Symmetry Breaking (GMSB), en base a la cual se obtuvieron los límites más rigurosos en las masas de distintas partículas, en estados finales con fotones, jet provenientes principalmente del decaimiento del bosón de Higgs, y energía transversa faltante, en los canales de producción fuerte. Debido a que el Run 2 del LHC operó a una mayor energía de centro de masa y luminosidad que su antecesor, se posibilitó el acceso a secciones eficaces menores, con la posibilidad de realizar búsquedas dedicadas en canales exclusivos. Esto brinda el marco apropiado para el desarrollo de búsquedas de supersimetría en canales con producción débil, como se desarrolla en también en este trabajo. 

Las nuevas condiciones del Run 2 del LHC, generaron nuevos desafíos para el detector ATLAS, en particular para el Sistema de Trigger, el cual tiene que seleccionar eventos de interés físico, como por ejemplo para la búsqueda de supersimetría con fotones en estado final estudiados en este trabajo, sobre un enorme fondo de eventos. Nuevos criterios y sistemas se implementaron para calcular cantidades físicas en base a varios objetos de triggers, que al mismo tiempo de reducir la frecuencia, permiten guardar los eventos de interés para su posterior análisis. Entre los resultados específicos del presente trabajo se estudiaron las prestaciones de los algoritmos de selección en base a los nuevos criterios en la toma de datos, resultados que fueron luego utilizados por toda la colaboración en todos los estudios que involucran la selección de fotones online que formen parte en distintos estados finales. 


El \textbf{Modelo Estándar} de la física de partículas (SM, por sus siglas en inglés) es la teoría que describe y clasifica a las partículas elementales de la naturaleza, junto con tres de las cuatro interacciones fundamentales conocidas hasta el momento. El mismo fue formulado en la década de los 70, a partir de varios trabajos científicos realizados durante la segunda mitad de ese siglo, entre los que se encuentra principalmente los realizados por Glasgow \cite{Glashow:1961tr}, Salam\cite{salam}, Weinberg\cite{PhysRevLett.19.1264}, Brout, Englert, Higgs\cite{PhysRevLett.13.321, PhysRevLett.13.508,PhysRevLett.13.585}. Para ese momento, el SM incorporaba a todas las partículas conocidas y postulaba la existencia de otras adicionales, lo que motivó al desarrollo de nuevos aceleradores y detectores para realizar dichas búsquedas. El descubrimiento de nuevas partículas e interacciones postuladas por el SM, junto con la medida a alta precisión de distintos parámetros del mismo, han convertido al SM en una teoría ampliamente aceptada por toda la comunidad científica, con una formulación matemática que sirve a su vez para nuevas futuras teorías.


Las partículas en el SM se clasifican a primer orden entre \textbf{bosones} y \textbf{fermiones}. Los bosones de \textit{spin} 1 son los mediadores de las interacciones entre las distintas partículas del modelo. El primero de ellos es el fotón ($\gamma$), mediador de la interacción electromagnética, que afecta a las partículas que tienen carga eléctrica. No hay una fecha exacta del descubrimiento del mismo, pero se puede entender a la descripción del efecto fotoeléctrico por parte del Albert Einstein \cite{einstein}, como la primera formulación con objetos discretos de esta interacción. A su vez están los bosones asociados a la interacción débil, los bosones \Zzero y \Wpm, siendo estos últimos los que gobiernan los intercambios de <<sabor>> de las partículas. Propiamente fueron descubiertos (no sólo su interacción mediante corrientes cargadas y neutras) en 1983 en el Super Proton Synchrotron del CERN \cite{DiLella:2015yit}. Por otro lado se encuentran los gluones ($g$), mediadores de la interacción fuerte de aquellas partículas con carga de <<color>>. Existen tres cargas de color \textit{red}, \textit{green} y \textit{blue}, aunque las antipartículas pueden tener las anti cargas (\textit{antired}, \textit{antigreen} y \textit{antiblue}). Su primera observación experimental se realizó en 1978 en el detector PLUTO del colisionador electrón-positrón DORIS del DESY \cite{gluon}. Finalmente está el bosón de Higgs, partícula asociada al mecanismo Brout-Englert-Higgs que describe el rompimiento espontáneo de simetría electrodébil, asociado a la generación de masas de todas las partículas que componen al SM. El mismo fue descubierto en el 2012 por los experimentos ATLAS y CMS del CERN \cite{HIGG-2012-27, CMS-HIG-12-028}. Cabe mencionar, que la bien conocida interacción gravitatoria, no es incluida en el SM debido a las contradicciones que aparecen al querer combinarla con la teoría de la Relatividad General. En teorías con gravedad cuántica, se hipotetiza la existencia de una partícula mediadora de esta fuerza denominada gravitón ($G$) y que se espera que sea no masiva y de spin 2.

Los fermiones están asociados a las partículas interactuantes o que forman la materia,
esto se debe a que al obedecer la estadística de Fermi-Dirac, no es posible encontrarlos simultáneamente en un mismo estado cuántico y por ende tienden a formar estructuras. A su vez, estos se clasifican en \textbf{leptones} y \textbf{\textit{quarks}}. Los primeros son aquellos que no tienen carga de color y por ende no interactúan fuertemente. Existen seis leptones (junto con sus respectivas antipartículas): electrón ($e$), neutrino electrónico (\nue), muón ($\mu$), neutrino muónico (\numu), tau ($\tau$) y neutrino tauónico (\nutau), los cuales se agrupan en tres generaciones, que son pares de partículas que exhiben propiedades similares. Todos ellos pueden interactuar débilmente, y salvo por los neutrinos, también electromagnéticamente. Los quarks, en cambio, son los fermiones con carga de color, y por ende los que pueden interactuar fuertemente. Existen seis quarks (nuevamente junto con sus respectivas antipartículas) que se agrupan en tres generaciones: \textit{up} ($u$), \textit{down} ($d$), \textit{charm} ($c$), \textit{strange} ($s$), \textit{top} ($t$) y \textit{bottom} ($b$). Debido al efecto del confinamiento de color, los quarks nunca pueden ser observados en la naturaleza, sino que se los observa en estados ligados sin color, denominados hadrones. Cuando el hadrón se forma de un quark-antiquark se los llama mesones, y cuando es un conjunto de tres quarks se los llama bariones. Los bariones más conocidos son los protones ($p$) y neutrones ($n$), compuestos por los quarks de valencia $uud$ y $udd$ respectivamente, donde cada quark toma uno de los tres posibles colores. Toda la materia ordinaria observada (o estable) se compone de electrones, quarks up y down. Cabe destacar que es posible que en el presente texto se omita la diferenciación entre partícula y antipartícula, salvo cuando sea necesario hacerla explícita. Por ejemplo, es posible referirse al positrón como simplemente un electrón con carga positiva.

Existe una clasificación adicional a partir de una propiedad intrínseca de las partículas denominada quiralidad. 
Los fermiones pueden tener dos estados de quiralidad denominados izquierdo y derecho. Si bien en principio estos estados son posibles para todas los fermiones del SM, no se han observado experimentalmente neutrinos con quiralidad derecha\footnote{La quiralidad está íntimamente relacionada con la helicidad, y para partículas no masivas como el neutrino, son equivalentes}. Más aún, se observa que las interacciones cargadas electrodébiles son entre partículas izquierdas, 
en lo que se denomina violación de paridad. Esto da a entender que en la naturaleza no hay una simetría entre las componentes izquierdas y derechas, y que teóricamente merecen un trato diferente.
En la Figura \ref{fig:sm_particles} se muestra un resumen de las partículas del SM junto con algunas de sus propiedades e interacciones.





El SM se formula como una teoría cuántica de campos, en general considerada efectiva, ya que por ejemplo, describe los fenómenos en una escala baja de energía en la que la gravedad es despreciable. A su vez, se compone de teorías de \textit{gauge} en las que a partir de imponer simetrías en el Lagrangiano, no solo están asociadas cantidades conservadas, como bien enuncia el Teorema de Emily Noether \cite{Noether1918}, sino que también implica la existencia de interacciones mediadas por bosones de gauge. A continuación se realiza una breve descripción matemática del SM basada principalmente en la Referencia \cite{gkane}.

Hasta la actualidad, todos los experimentos demuestran que tres simetrías son necesarias y suficientes para describir todas las interacciones conocidas, sin incluir a la gravedad. Estas simetrías, otorgan a las distintas partículas respectivas cargas, que vienen a representar <<etiquetas>> que se les puede dar a las mismas, y que el conjunto de ellas describe en su totalidad a las propiedades de cada una.

El grupo de simetría del SM se define como:


La primer simetría a mencionar es la $SU(2)_L \otimes U(1)_Y$, relacionada con la interacción electrodébil. Los bosones de gauge requeridos por la invarianza de la teoría ante estas transformaciones son el $B_{\mu}$ y los $W_{\mu}^{i}$. El índice $\mu$ está presente debido a que los mismos deben transformarse bajo rotaciones espaciales de la misma forma que la derivada tradicional, garantizando así que las dichas partículas tengan spin 1. El índice $i$ representa cada uno de los tres bosones de spin 1, asociados a los tres generadores de las transformaciones $SU(2)$.


Finalmente, la última simetría requerida es la $SU(3)_C$, con los bosones de gauge asociados $G_{\mu}^{a}$. El índice $a$ representa cada uno de los ocho bosones de spin 1, asociados a los ocho generadores de las transformaciones $SU(3)$. Estos bosones son los gluones, y la teoría que los describe es la cromodinámica cuántica (QCD).

Por su parte, los fermiones se describen mediante campos que definen estados dentro del espacio formado por las distintas simetrías. La simetría $SU(2)$ es análoga al spin, 
las partículas pueden formar singletes, dobletes o tripletes.
En el caso de $SU(2)$ electrodébil, los fermiones izquierdos forman dobletes, y los derechos forman singletes:


donde por simplicidad, se omitieron los fermiones de la segunda y tercera familia.
Esta distinción entre izquierdos y derechos, es lo que hace que aparezca la violación de paridad electrodébil de forma natural en la teoría. El índice que aparece en los estados de los quarks, $\alpha$, es para describir cómo los mismos se transforman en el espacio $SU(3)$, de la misma forma que en $SU(2)$. En $SU(3)$ la representación fundamental son tripletes cuyas componentes representan los tres estados de color posible ($r$, $g$ y $b$). Los quarks forman tripletes,  mientras que los leptones son singletes sin color y por ello no requieren de este índice.


Con esto en mente, el Lagrangiano se comienza a construir semejante al de una partícula libre, pero reemplazando la derivada ordinaria con la covariante, que con las simetrías consideradas, queda de la siguiente forma:


donde $Y$, $\tau$ y $\lambda$ son los respectivos generadores de las transformaciones $U(1)$, $SU(2)$ y $SU(3)$, y $g_1$, $g_2$ y $g_3$ son constantes que representan la intensidad de cada acoplamiento y deben medirse experimentalmente. Se emplea una convención para escribir las ecuaciones de forma más compacta: los términos $D_{\mu}$ que actúen en los fermiones con una representación matricial diferente se anulan. Por ende, los $W_{\mu}^{i}$ (matrices $2\times2$ en $SU(2)$) que actúan sobre leptones derechos (singletes de $SU(2)$) se anulan, y los $G_{\mu}^{a}$ (matrices $3\times3$ en $SU(3)$) actuando sobre leptones (singletes de $SU(3)$) se anulan. Quedando así el término fermiónico del Lagrangiano:


Al desglosar los distintos términos de este Lagrangiano, es posible relacionar algunos de ellos tanto con observaciones experimentales, como con predicciones de la teoría. Por ejemplo, al mirar solo los términos $U(1)$ y $SU(2)$, se puede obtener el Lagrangiano asociado a la teoría electrodébil, donde se obtienen las denominadas corrientes neutras y cargadas, que posteriormente dieron con el descubrimiento de los bosones $W$ y $Z$. A su vez es posible obtener relaciones entre las constantes del modelo, entre las que se encuentra:





donde $Q$ es la carga eléctrica, $Y_W$ es el anterior mencionado generador de $U(1)$, que en este contexto se denomina hipercarga débil, y $T_3$ es la tercer componente del isospin débil. En caso de dobletes de $SU(2)$, los dos estados del mismo toman valores de $T_3$ igual $1/2$ o $-1/2$, y para singletes de $SU(2)$ toma un valor nulo.

De la misma forma se puede obtener el Lagrangiano asociado a QCD, mirando solo los términos $SU(3)$. Aún así no es posible sacar conclusiones de la misma forma que para la teoría electrodébil, debido al confinamiento de los quarks y gluones, que no permiten observarlos de forma aislada en la naturaleza. En la Sección \ref{sec:qcd_pp}, se describen de mejor forma algunos detalles de QCD, que resultan útiles a la hora de entender a las colisiones $pp$.

Por último cabe mencionar que en ningún momento se hizo distinción alguna entre las familias de fermiones. Lo que permite, en las ecuaciones anteriores, reemplazar a los fermiones de la primer familia ($e$, $\nue$, $u$, $d$), por cualquiera de los de otras dos familias ($\mu$, $\numu$, $c$, $s$) o ($\tau$, $\nutau$, $t$, $b$), y las mismas seguirían teniendo validez.
Esto se denomina universalidad leptónica, y es una propiedad que ha sido de interés a lo largo de los años, en diferentes experimentos. Si la universalidad leptónica ha de cumplirse, los acoplamientos a los bosones de gauge debería ser iguales, y la única diferencia entre los leptones reside solo su masa. Una forma de medir este fenómeno es observando las fracciones de decaimiento a leptones de distintas partículas, como $\mu$, $\tau$ y principalmente hadrones con quarks bottom (hadrones $B$). Si bien en general las mediciones de estas fracciones están de acuerdo con las predicciones del SM, medidas recientes han observado desviaciones importantes\footnote{De aproximadamente 3-sigma, lo que en la jerga implica que aún no son significativas como para hablar de descubrimiento.} con respecto al mismo \cite{lepton_uni_1, lepton_uni_2}, lo que ha motivado el estudio de nuevas teorías que las expliquen (por ejemplo \textit{leptoquarks} \cite{leptoquark_1, Okumura:2744026}).




La formulación hasta ahora descripta no incluye en ningún momento las masas de ninguna de las partículas. Esto se debe a que al agregar términos de masa explícitos al Lagrangiano (como por ejemplo $m\psi\hat{\psi}$ o $m_B^2 B^{\mu}B_{\mu}$), el mismo pierde la invarianza electrodébil, ya que la misma sólo se garantiza poniéndole masa nula a todas las partículas. Si se incluyeran a las masas <<a mano>> la teoría termina teniendo cantidades físicas infinitas. 
La forma más adecuada de incluir masas a la teoría
es mediante el mecanismo de Higgs. Para ello se asume, que en el SM, el universo está inmerso en un campo de spin 0, denominado campo de Higgs. El mismo es un doblete en el espacio $SU(2)$, y tiene hipercarga no nula en $U(1)$, pero es un singlete en el espacio de color. 
Los bosones de gauge y los fermiones pueden interactuar con este campo, y en su presencia dejan de tener masa nula. Si bien el Lagrangiano conserva la simetría $SU(2)$ y $U(1)$, el estado fundamental no, en lo que se denomina un rompimiento espontáneo de simetría.

La parte escalar del Lagrangiano de Higgs está dada por:


donde el $\phi$ es un campo escalar complejo en la representación de $SU(2)$:



con hipercarga $U(1)$, $Y=+1$. La derivada covariante en este término, es similar a la descripta en la Ecuación \ref{eq:cov_deriv}, pero sin el término de color, y con los mismos bosones de gauge de $SU(2)$ y $U(1)$. Esta simetría $U(1)_Y$ adicional, es necesaria para que la teoría genere un bosón de gauge no masivo asociado al fotón.
$V(\phi)$ es el potencial de Higgs, que para garantizar la renormalización de la teoría e invarianza de $SU(2)$ y $U(1)$, requiere ser de la forma:


donde $\lambda$ es un parámetro que debe ser mayor a $0$, para garantizar un mínimo del potencial, quedando el comportamiento determinado por el signo del otro parámetro, $\mu$. Para $\mu^2>0$, el campo genera un valor de expectación de vacío (VEV, $v:=\phi^{\dagger}\phi$) no nulo que rompe espontáneamente la simetría. El potencial $V(\phi)$ toma la forma de un sombrero mexicano (Figura \ref{fig:mexican_hat}), y tiene infinitos números de estados degenerados con energía mínima que satisfacen $v = \sqrt{-\mu^2/\lambda}$. De esos estados se elige arbitrariamente el estado:



Debido a la conservación de la carga, solo un campo escalar neutro puede adquirir VEV, por lo que $\phi^0$ se interpreta como la componente neutral del doblete, 
y por ende $Q(\phi)=0$. El electromagnetismo no se modifica por el campo escalar VEV, y la ruptura de simetría se representa como:


Para estudiar el espectro de partículas, se estudia al campo alrededor del mínimo utilizando una expansión en la dirección radial:

Al elegir una dirección particular, tenemos tres simetrías globales rotas, y por el teorema de Goldstone, aparecen tres bosones escalares no masivos. Estos bosones de Goldstone son absorbidos por los bosones $W$ y $Z$, adquiriendo así su respectiva masa, 
mientras que la expansión en la dirección radial da la masa de la excitación $h$, que es la masa del bosón de Higgs. De esta forma, queda la masa de los bosones de la teoría de la forma:


Los términos de acoplamiento del tipo Yukawa al campo de Higgs otorgan masas a los fermiones del SM:




siendo este, ahora sí, invariante electrodébil. 
La constante $g_f$ describe el acoplamiento entre el doblete de Higgs y los fermiones. Al hacer una expansión del campo como se hizo anteriormente, aparecen en el Lagrangiano términos de masas fermiónicos que dan masa a los mismos de la forma:



El mecanismo de Higgs da un cierre al SM, que queda completamente determinado por 19 parámetros\footnote{No se están contando los ángulos de mezcla y masas de los neutrinos} listados en la Tabla \ref{tab:sm_para}, los cuales deben ser medidos experimentalmente.









Como se mencionó anteriormente, QCD \cite{qcd_collider, Tripiana:1433788} es la teoría de campos de gauge renormalizable,
que describe la interacción fuerte entre quarks mediados por gluones. Los gluones son los objetos que generan las transiciones de un quark de color a otro. Las propiedades de los quarks en QCD, son análogas de alguna forma a las del fotón en QED, con la distinción de que estos sí llevan carga (de color). Esto genera que puedan autointeractuar, y además, cambiar la carga de color de los quarks (a diferencia de las partículas cargadas eléctricamente, que si bien pueden emitir o absorber un fotón, esto nunca cambia su carga). Esto se debe principalmente a la estructura no abeliana de su grupo de simetría.
Por otra parte, el grupo de renormalización afecta a la constante de acoplamiento fuerte ($\alpha_s$), que termina dependiendo de la distancia de las cargas o la energía de la interacción (\textit{running coupling constant}). 

En QED, la polarización del vacío es inducida por los
pares virtuales $e^{+}e^{-}$, que apantallan (\textit{screening}) la carga eléctrica y resulta en la disminución del
acoplamiento con la distancia. Por el contrario, los gluones no sólo producen pares $q\bar{q}$ (que causan un efecto análogo al de QED), sino que también crean pares de gluones adicionales,
que tienden a anti-apantallar (\textit{anti-screening}) la carga aparente de color. El efecto neto, es entonces, que el acoplamiento fuerte decrece con la energía y crece con la distancia. Esto da lugar al ya mencionado confinamiento de color, debido a que el potencial del campo de color aumenta linealmente con la distancia, y por lo tanto no se pueden observar quarks ni gluones libres en la naturaleza, solo observarlos en conjuntos sin color. Por otro lado, a pequeñas distancias o altas energías, se produce la libertad asintótica, donde la intensidad de
la interacción fuerte decrece, de tal forma que los quarks y gluones se comportan
esencialmente libres ($\alpha_s \ll 1$), posibilitando así un tratamiento perturbativo. 

Estas propiedades tienen un impacto directo a la hora de producir y observar quarks y gluones en un detector. Por ejemplo, en un colisionador de protones, los quarks y
gluones producidos altas energías, sufren un proceso conocido como <<hadronización>>,
a medida que pierden energía los mismos se van combinando con los quarks y antiquarks creados del vacío, formando hadrones. De esta forma no se detectan quarks o gluones de forma directa, sino que se observan como un chorro o cascada de partículas conocido como jets. Dichas cascadas en general se asemejan a un cono, con su vértice en el quark/gluon inicial, y agrupando todas las partículas producidas en dicha cascada. 




El LHC es principalmente un colisionador de protones, por lo que describir las interacciones que subyacen en la colisión misma, no solo es importante para entender los fenómenos que se producen, sino también para poder generar simulaciones de dichos procesos con una elevada precisión. Las colisiones $pp$, son ventajosas para obtener energías de colisión elevadas, pero con las desventaja de estar gobernadas principalmente por interacciones QCD, que son complejas en su propia naturaleza al realizar una descripción teórica. Para ello se utiliza el <<modelo de partones>>, introducido por Feynman \cite{PhysRevLett.23.1415} y Bjorken \cite{PhysRev.185.1975} a fines de los años 60. 

El modelo de partones propone que a altas energías los hadrones están compuesto por partículas puntuales denominadas partones, que vienen a representar los quarks de valencia, y los quarks, antiquarks y gluones del mar, presentes en el protón. Cada uno de los partones lleva entonces una fracción de la energía y momento del protón, que a priori son desconocidas, lo que representa un problema a la hora de calcular secciones eficaces partónicas, $\sigma(qg\to qg)$.
Se suma además, en el caso de realizar una verificación experimental de la misma, el hecho de que los quarks y gluones del estado final no son observados de forma directa debido a la hadronización. En cambio, se calcula una sección eficaz hadrónica, $\sigma(pp\to jj)$, entre los protones incidentes y los jets del estado final. Para realizar este pasaje se emplea el teorema de factorización \cite{ELLIS1978281}, que permite una separación sistemática
entre las interacciones de corta distancia (de los partones), y las interacciones de larga distancia (responsables del confinamiento de color y la formación de hadrones). El teorema establece que la sección eficaz de producción de cualquier proceso de QCD del tipo $A+B\to X$, puede ser expresada como:





donde $x_i(x_j)$ es la fracción del momento del hadrón $A(B)$ que lleva el partón $a_i(b_j)$, y $\sigma_{a_i b_j \to X}$ es la sección eficaz de la interacción a nivel partónico, calculada a un dado orden en QCD perturbativo (pQCD) y una escala de renormalización $\mu_R$ \cite{Tripiana:1433788,Wahlberg:2005gi}. La escala de renormalización es introducida
para absorber las divergencias ultravioletas, que aparecen en los cálculos perturbativos más
allá del primer orden.

Las funciones $f_{h/n}(x_{n}, \mu_{F}^2)$, llamadas funciones de distribución partónica (PDFs), representan la probabilidad de encontrar un partón de tipo $n$ en el hadrón $h$ con una fracción de
momento $x_n$, dada una escala de factorización $\mu_{F}$. Esta escala, es un parámetro arbitrario,
introducido para tratar singularidades que aparecen en el régimen no perturbativo. Estas
divergencias son absorbidas, en forma similar a la renormalización, dentro de las funciones
de distribución partónicas a la escala $\mu_F$. Si bien las PDFs no pueden ser determinadas
perturbativamente, se puede predecir su dependencia con $Q^2$, por medio de las ecuaciones
de evolución DGLAP (Dokshitzer-Gribov-Lipatov-Altarelli-Parisi) \cite{GRIBOV197178,Lipatov:1974qm,altarelli-parisi}. De esta forma, la
medida experimental de su forma funcional, a un dado $Q^2_0$ fijo, permite obtener predicciones
de las PDFs para un amplio espectro de $Q^2$. En la presente Tesis se consideran las predicciones teóricas a NLO utilizando las parametrizaciones CTEQ \cite{Lai:1999wy,Pumplin:2002vw}, MSTW \cite{Martin:2009iq, Martin:2009bu, Martin_2010} y NNPDF \cite{Ball:2012cx,Ball:2014uwa}.

Luego de la interacción a alta energía, cada partón del estado final comienza a irradiar gluones,
perdiendo así energía. Estos gluones, nuevamente fragmentan en pares $q\bar{q}$ y gluones adicionales, y así sucesivamente, creando una lluvia de partones, de cada vez más bajo \pt. Esto continúa hasta
que la energía es suficientemente baja, y todos los partones se recombinan para formar
mesones y bariones, en lo que se conoce como hadronización. Las bajas transferencias de
energía involucradas en el proceso, son tales que este no puede ser tratado perturbativamente. La dinámica de esta evolución es absorbida en funciones de fragmentación, que
representan la probabilidad de un partón de fragmentar en un determinado hadrón del
estado final. La sección eficaz $\sigma_{AB\to X}$, en la Ecuación \ref{eq:xs_fact}, puede ser modificada entonces para
calcular el proceso $A + B \to C + X$:



donde $C$ es un hadrón, $D_{c_k}$ es la función de fragmentación, que define la probabilidad
de que un partón $c_k$ fragmente en un hadrón $C$, con una fracción $z_C$ de su momento a la
escala de fragmentación (o factorización del estado final) $\mu_{f}$. Esta escala es introducida
de manera similar a $\mu_{f}$ para el estado inicial, a fin de remover las singularidades por
radiación colineal en el estado final.

A lo largo de los años, en los distintos experimentos del LHC, se han realizado medidas de secciones eficaces de distintos procesos del SM. La Figura \ref{fig:sm_xs} muestra el buen acuerdo entre la sección eficaz medida en ATLAS de algunos procesos, y sus predicciones teóricas.








En la sección anterior se describió brevemente la mayoría de las propiedades del SM junto con sus predicciones. A pesar de ser una de las teorías más exitosas de la 
física en general,
naturalmente el modelo tiene un rango de validez. A lo largo de los años la frontera experimental se ha ido expandiendo, observando nuevos (y no tan nuevos) fenómenos que la actual formulación del SM no puede explicar, principalmente en el rango de altas energías.

Una de las principales limitaciones del SM, es la imposibilidad de incluir a la gravedad de la misma forma que incluye a las demás interacciones. No solo incluir al gravitón a la teoría no es suficiente para poder explicar las observaciones, sino que la matemática empleada en el SM\footnote{Y de cualquier teoría cuántica de campos} es prácticamente incompatible con la formulación de la Relatividad General. Por otra parte, en el SM están presentes lo que se denominan problemas de jerarquía \cite{tHooft:1980xss}. 
Un problema de jerarquía en el contexto de física de partículas, se refiere a cuando alguno de los parámetros empleados en el Lagrangiano (masas o constantes de acoplamiento por ejemplo), difiere en varios órdenes de magnitud de su valor efectivo, que es el valor medido en un experimento. El valor efectivo está relacionado con el valor fundamental a través de la renormalización, que aplica correcciones al mismo, y que en general ambos valores son cercanos.
Esto lleva a pensar que la formulación de esa teoría no sea del todo definitiva, y que en cambio, está compensando ciertos defectos, incluyéndolos en ese parámetro tan diferente. 
El caso más conocido tal vez, son los 17 órdenes de magnitud de diferencia entre la escala electrodébil ($M_W\sim 10^{2}\,\gev$) y el escala de Planck ($M_P\sim 10^{19}\,\gev$), en donde los efectos de la gravedad cuántica comienzan a ser comparables con las demás interacciones.

Por otro lado, observaciones cosmológicas sostienen que el SM solo describe casi el 5

La observación de la oscilación de neutrinos, implica que si bien los neutrinos tienen una masa muy pequeña, la misma no es nula, en contraposición con lo que formula el SM. Si bien hay varios mecanismos para incluir las mismas dentro del SM, no hay evidencia suficiente para saber cuál es la forma correcta, 
sumado a que algunos modelos proponen la existencia de nuevas partículas pesadas aún no observadas \cite{gellmann2013,Sawada:1979dis,Glashow:1979nm,Ramond1998}.

Por último, cabe destacar que varias magnitudes de la teoría, y medidas con una elevada precisión, han sido observadas desviándose de los valores predichos por el SM. Uno de especial interés en la actualidad, es la anomalía en la medida del momento dipolar magnético del muon (<<muon $g-2$>>) \cite{Muong-2:2021ojo}. Estas diferencias no necesariamente significan un defecto en el SM, pero muchas veces puede ser una motivación para la formulación de nuevas teorías.


Retomando otro problema de jerarquía, el término de masa del Higgs recibe correcciones virtuales de cada partícula que se acople al campo de Higgs. 
Si el campo de Higgs acopla a un fermión $f$ con un término en el Lagrangiano de la forma $-\lambda_f \bar{f} \phi f$, entonces el diagrama de Feynman que aparece en la Figura \ref{fig:loops} genera una corrección:



donde $\Lambda_{\text{UV}}$ es la escala de energía donde el SM deja de ser válido y nuevos fenómenos físicos pueden ser apreciables. Cualquier fermión del SM puede tomar el rol de $f$, pero la mayor corrección viene de parte del $top$ quark, con un $\lambda_f\sim1$, y un factor $3$ adicional por las cargas de color. Si $\Lambda_{\text{UV}}$ es del orden de $M_P$, las correcciones a la masa del Higgs son casi 30 órdenes de magnitud mayores a su valor medido. 
Inclusive, los demás bosones y fermiones del SM terminan siendo sensibles a esta escala, debido a que obtienen su masa a partir de $\left<H\right>$.


Una forma de solucionar esto es considerando la existencia de un escalar complejo $S$, con masa $m_S$, que acopla al Higgs mediante un término del Lagrangiano $-\lambda_S |\phi|^2|S|^2$, generando en el diagrama de la Figura \ref{fig:loops} una corrección del tipo:

Considerando la diferencia de signos entre el \textit{loop} fermiónico y bosónico, si cada fermión del SM estuviera acompañado por dos campos complejos con $\lambda_S = |\lambda_f|^2$, generaría una cancelación automática de los términos, eliminando de este modo las divergencias generadas. Esto motiva la inclusión de una nueva simetría a la teoría, entre fermiones y bosones, llamada \textbf{Supersimetría} (SUSY). Las secciones siguientes que describen la teoría supersimétrica fueron basadas en la Referencia \cite{Martin:1997ns}.



Una transformación supersimétrica transforma un estado bosónico en uno fermiónico, y viceversa. El operador $Q$ que genera tal transformación, tiene que ser un espinor anticonmutativo:


Los espinores son objetos complejos, por lo que $Q^{\dagger}$ es también un generador de la simetría. Como $Q$ y $Q^{\dagger}$ son operadores fermiónicos (tienen spin $1/2$), supersimetría es una simetría espaciotemporal, y deben cumplir las siguientes reglas de (anti)conmutación:


donde $P^{\mu}$ es el cuadrivector generador de las traslaciones espaciotemporales (los índices sobre los operadores $Q$ y $Q^{\dagger}$ fueron suprimidos intencionalmente).


Los estados de partícula son representaciones irreducibles del álgebra de SUSY, y se denominan <<supermultipletes>>. Cada uno contiene ambos estados bosónico y fermiónico, denominados <<supercompañeros>>. Como el operador $-P^2$ (cuyos autovalores son las masas) conmuta con los operadores $Q$ y $Q^{\dagger}$, y con los operadores de traslación y rotación, los supercompañeros dentro de un supermultiplete deben tener la misma masa. A su vez, como los operadores $Q$ y $Q^{\dagger}$ conmutan con los generadores de las transformaciones de gauge, los supercompañeros deben tener misma carga eléctrica, isospin débil y carga de color.

Cada supermultiplete debe contener igual número de grados de libertad fermiónica y bosónica, $n_F$ y $n_B$ respectivamente. Una forma posible de construir un supermultiplete con estas características es que tenga un solo fermión de Weyl con $n_F=2$ (dos estados de quiralidad),
y dos campos escalares reales cada uno con $n_B=1$ (los cuales se combinan en un campo escalar complejo). Este tipo de supermultipletes de denominan escalares o quirales. Otra posibilidad es combinar un bosón vectorial de spin 1 (bosón de gauge no masivo con dos estados de quiralidad, $n_B=2$), con un fermión de Weyl no masivo de spin $1/2$ (con dos estados de quiralidad, $n_F=2$). Por como se transformar los bosones de gauge, sus supercompañero fermiónicos deben tener las mismas propiedades de las transformaciones de gauge para sus componentes izquierdas y derechas. Este tipo de supermultiplete se denominan vectoriales o de gauge. 
Si incluimos a la gravedad, entonces el gravitón \cite{Anduaga:1433401} de spin 2 ($n_B=2$) tiene un supercompañero con spin $3/2$, y si es no masivo con dos estados de quiralidad $n_F=2$. 
Hay otras posibilidades de partículas para generar supermultipletes, pero en general se terminan reduciendo a combinaciones de supermultipletes quirales y de gauge, excepto en teorías con supersimetrías adicionales. En nuestro caso inicial, la teoría se la denomina SUSY $N=1$, donde $N$ es el número de supersimetrías (o el número de conjuntos de operadores $Q$ y $Q^{\dagger}$). 



El \textbf{Modelo Estándar Supersimétrico Mínimo} (MSSM) es la extensión del SM que requiere incluir la mínima cantidad de partículas para completar los supermultipletes. Los supermultipletes solamente pueden ser escalares o vectoriales, y el spin de cada compañero debe diferir en $1/2$. Solo los supermultipletes escalares pueden contener fermiones cuyas partes izquierdas y derechas se transformen distinto frente a los grupos de gauge, y como los fermiones del SM tienen esta propiedad se los incluye en este tipo de supermultiplete. Cada componente izquierda y derecha de los fermiones son separadas en fermiones de Weyl con diferentes transformaciones de gauge, por lo que cada una tiene su compañero complejo escalar, un bosón de spin 0. Los nombres de estos bosones son iguales al de su fermión correspondiente, pero anteponiendo una <<s>> (por escalar en inglés), y lo mismo ocurre con su símbolo pero con una tilde. Tenemos entonces los los \textit{selectrons} (\selL, \selR), \textit{smuons} (\smuL, \smuR), \textit{squarks} (\squarkL, \squarkR), etc (también vale la terminología \textit{sleptons} ($\tilde{ell}$) o \textit{sfermions} ($\tilde{f}$) para el conjunto). Cabe mencionar que el índice en los sleptons representa la quiralidad del fermión correspondiente, y no su propia quiralidad (que no tienen por ser de spin 0). En el caso de los neutrinos, al ser siempre izquierdos, sus \textit{sneutrinos} no necesitan subíndice salvo para indicar su sabor: \snue, \snumu o \snutau. Las interacciones de los sfermions son las mismas que su correspondiente fermión, por lo que los $\tilde{f}_L$ acoplan con el bosón $W$ pero los $\tilde{f}_R$ no.

El bosón de Higgs debe encontrarse en un supermultiplete escalar debido a que tiene spin 0, pero a su vez el MSSM requiere de la existencia de dos dobletes escalares complejos de Higgs, en lo que se denomina el Modelo de Doble Doblete de Higgs (2HDM) Tipo II. A esos dobletes de $SU(2)_L$, con $Y=1/2$ e $Y=-1/2$, se los denomina $\Hu=(\Hup, \Huzero)$ y $\Hd=(\Hdzero, \Hdm)$ respectivamente. El bosón escalar de Higgs del SM es una combinación lineal de las componentes de isospin débil neutras de ambos dobletes ($\Huzero$ y $\Hdzero$). A los supercompañeros de los bosones se los nombra agregando el sufijo <<ino>> a su nombre, por lo que los supercompañeros de los dobletes de Higgs son los \textit{higgsinos}, $\Hinou=(\Hinoup, \Hinouzero)$ y $\Hinod=(\Hinodzero, \Hinodm)$.

Por otro lado, los bosones de gauge del SM deben estar contenidos en un supermultiplete vectorial, con sus respectivos supercompañeros denominados \textit{gauginos}. El gluon tiene un supercompañero de spin $1/2$, denominado \textit{gluino} ($\gluino$). Por su parte, la simetría $SU(2)_L\times U(1)_Y$ asociada a los bosones de gauge $\Wplus$, $W^0$, $\Wminus$ y $B^0$, tienen sus supercompañeros $\widetilde{W}^+$, $\widetilde{W}^0$, $\widetilde{W}^-$ y $\tilde{B}^0$, llamados \textit{winos} y \textit{bino}. Luego de la ruptura de simetría electrodébil, los estados de gauge $W^0$ y $B^0$ se mezclan en los estados de masa $\Zzero$ y $\gamma$, y de la misma forma lo hacen los $\widetilde{W}^0$ y $\widetilde{B}^0$, para dar lugar al \textit{zino} ($\widetilde{Z}^0$) y \textit{photino} ($\tilde{\gamma}$). Finalmente, y como se mencionó previamente, en caso de incluir a la gravedad, el gravitón ($G$) tiene su respectivo compañero, el \textit{gravitino} ($\gravino$).

En la Tabla \ref{tab:mssm_particles} se resume todas las partículas requeridas por el MSSM, donde vale remarcar que ninguno de los supercompañeros del SM mencionados anteriormente ha sido observado experimentalmente hasta la fecha. Otro comentario de interés, es que tanto el supermultiplete escalar \Hd (\Hdzero, \Hdm, \Hinodzero, \Hinodm), como el de los \sleptonL (\snu, \selL, $\nu_e$, $e_L$) tienen los mismos números cuánticos. Esto podría llevar a pensar que no es necesario incluir un nuevo doblete de Higgs y en cambio utilizar el de los sleptons$_L$. Si bien esto es posible, conlleva a diversos problemas fenomenológicos, como violaciones en el número de leptones y necesidad de neutrinos del SM muy masivos, lo que motiva a descartar esto.



Como se mencionó anteriormente, la formulación presentada hasta ahora del MSSM propone la existencia de nuevas partículas, cuyas masas son iguales a las masas de las partículas del SM. Por ejemplo, el \selL debería tener una masa de \magn{511}{keV}, el photino y gluino masas nulas, y lo mismo aplica para todas las demás partículas del SM, las cuales no superan los \magn{200}{GeV}. Este rango de energía ha sido ampliamente estudiado por distintos experimentos a lo largo de los años, y de existir partículas con esas masas, debería haber sido una tarea fácil detectarlas. Como no ha ocurrido, se asume que dichas partículas o tienen una masa muy baja y son poco interactuantes, o tienen una masa muy grande, en un rango de energía aún no explotado. Se dice entonces que SUSY es una simetría débilmente rota, ya que se necesita esa ruptura para que aparezca la asimetría en masas, pero lo mínimo y necesario para preservar las características que solucionaban el problema de jerarquía. El Lagrangiano efectivo del MSSM toma la forma:




donde $\mathcal{L}_{\text{SUSY}}$ contiene todas las interacciones de gauge y Yukawa, y preserva la invarianza frente a supersimetría, y $\mathcal{L}_{\text{soft}}$ viola supersimetría, pero contiene solo términos de masa y parámetros de acoplamiento con dimensiones positivas de masa. La diferencia de masas que hay entre las partículas del SM y sus supercompañeros, dependerá de la escala de masa más grande asociada al término \text{soft} ($m_{\text{soft}}$). Esta escala no puede ser indiscriminadamente grande ya que se perdería la solución al problema de jerarquía, y  las correcciones a la masa del Higgs serían extremadamente grandes. Se puede estimar 
que $m_{\text{soft}}$, y por ende las masas de los supercompañeros más livianos, deben estar en la escala del TeV. Esta es una de las motivaciones más importantes en las búsquedas experimentales, principalmente en los experimentos del LHC-CERN.



En una teoría supersimétrica renormalizable, la interacción y las masas de todas las partículas están determinadas solamente por las propiedades de sus transformaciones de gauge y por el superpotencial $W_{\text{MSSM}}$. 
Del MSSM hasta ahora tenemos el grupo de gauge, las partículas del mismo y las propiedades de las transformaciones de gauge, resta describir entonces el superpotencial que toma la forma:


Los campos que aparecen son los mismos de la Tabla \ref{tab:mssm_particles}, y las matrices $3\times3$ $\textbf{y}_\textbf{u}$, $\textbf{y}_\textbf{d}$, $\textbf{y}_\textbf{e}$, son los parámetros adimensionales del acoplamientos de Yukawa. Los índices para las transformaciones de gauge y familia de sabores fueron omitidos por practicidad. El último término, con el parámetro $\mu$, es la versión supersimétrica de la masa del Higgs del SM.

Por su parte, el término que describe el rompimiento de supersimetría de la forma más general toma la forma:



donde $M_1$, $M_2$ y $M_3$ son los términos de masa del bino, wino y gluino, $\textbf{a}_\textbf{u}$, $\textbf{a}_\textbf{d}$ y $\textbf{a}_\textbf{e}$ son matrices complejas de $3\times3$ con unidades de masa.
El resto de los términos contienen los términos de masa de los sfermions y sector de Higgs.






En el MSSM la ruptura de supersimetría simplemente se introduce explícitamente. Toda ruptura de una simetría global genera un modo no masivo de Nambu-Goldstone con los mismos números cuánticos que el generador de la simetría rota.
Para el caso de la supersimetría global, el generador es la carga fermiónica $Q_{\alpha}$,
por lo que la partícula de <<Nambu-Goldstone>> será un fermión de Weyl no masivo
neutro, llamado <<goldstino>>.
El rompimiento espontáneo de SUSY requiere un extensión del MSSM, agregando un sector oculto de partículas sin acoplamientos directos con los supermultipletes quirales del sector visible del
MSSM. Estos dos sectores comparten interacciones, que median el rompimiento de
SUSY desde el sector oculto al observable, como se esquematiza en la Figura \ref{fig:hidden_sector}, dando lugar a los términos soft del Lagrangiano del MSSM.
Las interacciones mediadoras entre el sector oculto y el observable pueden ser
de distinta naturaleza, por lo que existen muchos modelos que intentan explicar de
esta forma el rompimiento de SUSY. Uno de ellos es mediante interacciones gravitacionales, con modelos que se enmarcan en lo que se conoce como \textit{Planck scale mediated supersymmetry breaking} (PMSB), debido a que la gravedad entra cerca de la escala de Planck. Si SUSY se rompe en el sector oculto por un valor de expectación de vacío $\langle F \rangle$\footnote{$F$-term proveniente de los campos auxiliares $F$ incluidos al construir el Lagrangiano supersimétrico}, entonces los términos soft en el sector visible serán:




Para $m_{\text{soft}}$ del orden de ${\smallsim}100\,\gev$, la escala de rompimiento de SUSY en el sector oculto es $\sqrt{\langle F \rangle}\sim 10^{11}\,\gev$.
Cuando se tiene en cuenta la gravedad, SUSY deber ser una simetría local y la
teoría se conoce como supergravedad. En este caso, el gravitón de espín $2$ tiene un
supercompañero fermión de espín $3/2$, el gravitino, inicialmente no masivos. Una vez
que SUSY es espontáneamente rota, el gravitino absorbe al goldstino, adquiriendo
masa, que se convierte en sus componentes longitudinales (helicidad $\pm1/2$). La masa
del gravitino $m_{3/2}$, se puede estimar de la Ecuación \ref{eq:pmsb}, y se espera que sea comparable a la masa de las partículas del MSSM (entre \magn{100}{GeV} y \magn{1000}{GeV} aproximadamente).

Si se considera interacciones de gauge electrodébiles y QCD ordinarias, se tienen
los modelos \textit{Gauge Mediated Supersymmetry Breaking} (GMSB) \cite{Dine:1981gu, ALVAREZGAUME198296, Nappi:1982hm} (se considera aquí que estas interacciones siempre dominan sobre gravedad). Los términos
soft del MSSM provienen de diagramas a un loop que involucran partículas mensajeras, que son nuevos supermultipletes quirales que se acoplan al VEV $\langle F \rangle$ que
rompe SUSY, y tienen a su vez interacciones $SU(3)_C \times SU(2)_L \times U(1)_Y$ que generan
la conexión con el MSSM. Se tiene en este caso:


donde $\alpha_a$ es el factor de loop para diagramas de Feynman involucrando interacciones de gauge, y $M_\text{mens}$ es la escala característica de las masas de los campos mensajeros. En
caso que $M_\text{mens}$ y $\left< F \right>$ sean comparables, se pueden tener $m_{\text{soft}}$ en el correcto
orden de magnitud con sólo $\sqrt{\left< F \right>}\sim 10^4\,\gev$.

El marco más general se conoce como General Gauge Mediation (GGM), en el
cual se define al mecanismo de mediación por campos de gauge como el límite en
que las constantes de acoplamiento del MSSM $\alpha_a\to 0$, la teoría se desacopla en el
MSSM y un sector oculto separado que rompe SUSY. Como característica principal, la masa del
gravitino es $m_{3/2} \ll M_W$, típicamente del orden del eV, lo que implica que es la \textbf{Partícula Supersimétrica Más Liviana} (LSP, por sus siglas en inglés) 
de la teoría. Es interesante notar que debido a que la LSP es siempre el gravitino, la
partícula más liviana del MSSM es la NLSP (\textbf{Siguiente Partícula Supersimétrica Más Liviana}) de la teoría, y su naturaleza determina
entonces el estado final que se encuentra en un colisionador.



El superpotencial de la Ecuación \ref{eq:susy_potential} es mínimamente suficiente para producir la fenomenología necesaria para el modelo. Sin embargo, existen otros términos que se pueden incluir que si bien cumplen los requisitos, no se los incluye debido a que violan el número bariónico ($B$) o el leptónico ($L$). Por ejemplo:



Los supermultipletes $Q$ tienen $B=+1/3$, los $\bar{u}_i$ y $\bar{d}_i$ tienen $B=-1/3$ y el resto $B=0$, en cambio los $L_i$ tienen $L=+1$, los $\bar{e}_i$ tienen $L=-1$ y el resto $L=0$. Por lo que la primer igualdad de la Ecuación \ref{eq:lb_viol} viola el número bariónico en una unidad, y la segunda el número leptónico en una unidad. En caso de cumplirse esa relación el protón tendría la posibilidad de decaer, por ejemplo a un pion y un electrón, en una fracción muy pequeña de tiempo. Esto contradice las observaciones experimentales, donde se ponen cotas superiores al tiempo de vida media mayores a $10^{32}$ años. Motivada por esta y otras conservaciones, se podría postular directamente la conservación de $B$ y $L$ directamente en el MSSM, pero esto sería un retroceso con respecto al SM, donde esta conservación sale de forma 
<<accidental>>
. Para resolver esto se introduce una nueva simetría, que elimina la posibilidad de una violación de $B$ y $L$:


donde $s$ es el spín de la partícula. Las partículas del SM más los bosones de Higgs tienen $P_R = +1$, mientras que el resto de las partículas del MSSM tienen $P_R = -1$. Si $P_R$ se conserva, no puede haber mezcla entre las partículas con $P_R$ opuestos, y cada vértice de interacción de la teoría debe tener un número par de partículas con $P_R=-1$. Esto a su vez implica que la LSP debe ser completamente estable. En caso de ser la LSP neutra, debe interactuar débilmente con la materia ordinaria y por ende es un candidato interesante para materia oscura. Por otro lado, cada partícula supersimétrica que no sea la LSP, debe decaer a un estado con número impar de partículas supersimétricas, que eventualmente termina en la LSP. También a partir de esta simetría se puede concluir que en experimentos de colisión las partículas supersimétricas son producidas de a número par, generalmente de a dos.





Como ocurre en el SM, los estados de gauge que se muestran en la tabla \ref{tab:mssm_particles} no son necesariamente los estados de masa que se pueden observar experimentalmente, sino combinaciones de los mismos. En el MSSM no es una tarea trivial obtener los distintos autovalores de masa, ya que ahora hay dos dobletes complejos de Higgs, y varios conjuntos de partículas con los mismos números cuánticos que pueden dar una mezcla. La Figura \ref{fig:susy_part} resume los estados de gauge y masa del MSSM, los cuales se listan a continuación.





Los higgsinos y los gauginos electrodébiles se mezclan debido a la ruptura de la simetría electrodébil. Los higgsinos neutrales (\Hinouzero y \Hinodzero) y los gauginos neutrales ($\widetilde{B}$, $\widetilde{W}^0$) se combinan para formar cuatro estados de masa llamados neutralinos (\ninoone, \ninotwo, \ninothree, \ninofour). Los higgsinos cargados (\Hinoup y \Hinodm) y los winos ($\widetilde{W}^+$, $\widetilde{W}^-$) se combinan para formar dos estados de masa con carga, llamados charginos (\chinoonepm, \chinotwopm). Por convención se utiliza el subíndice para ordenarlos de forma ascendente a partir de su masa. En general se supone al neutralino más liviano, \ninoone, como la LSP ya que es la única partícula del MSSM que es buen candidato a materia oscura\footnote{Esto no ocurre en modelos con gravitinos más livianos, o con violación de la paridad R}. A partir de los estados de gauge, los valores de las masas se obtienen entonces diagonalizando las matrices que entran en el término de masa del Lagrangiano. En el caso de los neutralinos, la matriz $4\times4$ no es fácil resolver analíticamente. Una de las posibles aproximaciones, propone que la ruptura de simetría electrodébil se puede considerar como una pequeña perturbación en la matriz de masa de los neutralinos. Asumiendo entonces que: 



se obtienen neutralinos prácticamente <<bino-like>> ($\ninoone \approx \widetilde{B}$), <<wino-like>> ($\ninotwo \approx \widetilde{W}^0$) y <<higgsino-like>> ($\ninothree, \ninofour \approx (\Hinouzero \pm \Hinodzero)/\sqrt{2}$), con autovalores:


donde $M_1$ y $M_2$ se asumen reales y positivos, y $\mu$ real que puede ser tanto positivo como negativo. 
Un parámetro que aparece en las masas es el ángulo $\beta$, que se define a partir de los valores de expectación de vacío de $H_u^0$ y $H_d^0$:


El subíndice de cada neutralino debe ser acomodado de tal forma de que queden ordenados por su masa. La misma aproximación se puede realizar con los charginos\footnote{Aunque esta matriz sí tiene solución analítica: \\ $m_{\chinoonepm, \chinotwopm}^2 = \frac{1}{2} \left[ |M_2|^2 + |\mu|^2 + 2 m_W^2 \mp \sqrt{(|M_2|^2 + |\mu|^2 + 2 m_W^2)^2 - 4 |\mu M_2 - m_W^2 \sin{2\beta}|^2} \right]$} que terminan siendo <<wino-like>> y <<higgsino-like>> con masas:





El gluino no puede mezclarse con ninguna otra partícula del MSSM debido a
que es un fermión de color de ocho componentes. La masa la obtiene del término de ruptura de SUSY incluido en $\mathcal{L}_{\text{soft}}$, cuyo parámetro de masa es $M_3$.

Para el caso de los squarks y sleptons, como en principio todo escalar con la misma carga eléctrica, paridad R y color puede mezclarse entre sí, los estados de masa se obtienen diagonalizando las tres matrices de masa cuadrada de $6\times6$ para los squarks de tipo <<up>> ($\tilde{u}_L$, $\tilde{c}_L$, $\tilde{t}_L$, $\tilde{u}_R$, $\tilde{c}_R$, $\tilde{t}_R$), de tipo <<down>> ($\tilde{d}_L$, $\tilde{s}_L$, $\tilde{b}_L$, $\tilde{d}_R$, $\tilde{s}_R$, $\tilde{b}_R$), sleptons cargados ($\tilde{e}_L$, $\tilde{\mu}_L$, $\tilde{\tau}_L$, $\tilde{e}_R$, $\tilde{\mu}_R$, $\tilde{\tau}_R$), y la matriz de $3\times3$ para sleptons neutros ($\tilde{\nu}_e$, $\tilde{\nu}_{\mu}$, $\tilde{\nu}_{\tau}$). Las partículas de la tercer familia tienen masas bastante diferentes a las de la primera y segunda, e inclusive tienen mezclas significativas principalmente mediante los pares ($\tilde{t}_L, \tilde{t}_R$), ($\tilde{b}_L, \tilde{b}_R$) y ($\tilde{\tau}_L, \tilde{\tau}_R$), cuya combinación genera los $\tilde{t}_{1,2}$, $\tilde{b}_{1,2}$ y $\tilde{\tau}_{1,2}$ respectivamente. En cambio, los de la primera y segunda familia forman siete pares casi degenerados sin mezcla ($\tilde{e}_R, \tilde{\mu}_R$), ($\tilde{\nu}_e, \tilde{\nu}_\mu$), ($\tilde{e}_L, \tilde{\mu}_L$), ($\tilde{u}_R, \tilde{c}_R$), ($\tilde{d}_R, \tilde{s}_R$), ($\tilde{u}_L, \tilde{c}_L$) y ($\tilde{d}_L, \tilde{s}_L$).







Los campos de Higgs escalares en le MSSM se componen de dos dobletes de $SU(2)$ complejos, con ocho grados de libertad. Cuando ocurre la ruptura de simetría electrodébil tres de ellos son los bosones de Nambu-Goldstone, que se convierten en los modos longitudinales de los bosones $Z^0$ y $W^{\pm}$. Los cincos restantes consisten en dos escalares neutrales CP-par $h^0$ y $H^0$, un escalar neutral CP-impar $A^0$, y dos escalares cargados $H^+$ y $H^-$. Por convención $h^0$ es el más liviano, y se lo designa como bosón de Higgs del SM. Las masas de los mismos se pueden escribir como:





A continuación se describen los posibles decaimientos de las partículas supersimétricas. En general se asume que se conserva la paridad R y se considera al \ninoone como la LSP, aunque también se describe el caso donde el \gravino es la LSP.



Los posibles decaimientos de los neutralinos y charginos pueden ser:




Los estados en corchetes son los que están mayormente suprimidos cinemáticamente. Puede ocurrir también que todos estos decaimientos a dos cuerpos estén cinemáticamente prohibidos para un cierto gaugino, principalmente \chinoonepm y \ninotwo. En ese caso pueden ocurrir decaimientos a tres cuerpos de forma \textit{off-shell} a partir de bosones de gauge, escalares de Higgs, sleptons y squarks:



donde $f$ es una notación genérica para los leptones y quarks, y $f'$ es el otro miembro del multiplete de $SU(2)_L$. La Figura \ref{fig:susy_three_body_decays} muestra los diagramas de decaimientos a los estados finales más comunes de los neutralinos y charginos.




El gluino solo puede decaer a través de un squark, ya sea \textit{on-shell} o virtual. Si el decaimiento a
dos cuerpos está abierto, este va a dominar debido a que el acoplamiento gluino-quark-squark tiene intensidad de QCD.
En el caso de que todos los squarks sean más pesados que el gluino, este va a decaer solo vía
squarks virtuales.





Por otro lado, los decaimientos posibles de los sfermions son:





Como se mencionó anteriormente, en modelos como GGM la LSP es el gravitino. En general, el decaimiento $\widetilde{X} \to X\gravino$ no compite frente a los otros posibles decaimientos de la sparticles, excepto cuando esta es la NLSP, ya que esta necesariamente debe decaer al gravitino más su supercompañero. De particular interés es cuando la NLSP es el $\ninoone$, en ese caso los posibles decaimientos son a $\gamma\gravino$, $Z\gravino$, $h^0\gravino$, $A^0\gravino$ y $H^0\gravino$. De estos decaimientos, los últimos dos son muy poco probables cinemáticamente.
, y el primero es el único cinemáticamente garantizado 
Los decaimientos a $Z^0$ y $h^0$, están sujetos a una fuerte supresión cinemática proporcional a $ (1-m_{Z}^{2}/m_{\ninoone}^{2})^4$ y $(1-m_{h^{0}}^{2}/m_{\ninoone}^{2})^4$, pero aún juegan un papel importante en la fenomenología si $ \langle F \rangle $ no es demasiado grande (${\lesssim}10^9\,\tev$), \ninoone tiene un contenido considerable de zino o higgsino, y $m_{\ninoone}$ es significativamente mayor que $m_{Z}$ o $m_{h^{0}}$. 
En general, la probabilidad de decaimiento del \ninoone depende de los parámetros de mezcla de los neutralinos, del ángulo de Weinberg y también de su masa, a partir de los parámetros que las definen en las Ecuaciones \ref{eq:nc_mass}.



Asumiendo la conservación de la paridad R, en colisionadores de hadrones, las partículas supersimétricas pueden producirse de a pares a partir de colisiones de partones con interacciones fuertes:




o interacciones electrodébiles:


En la Figura \ref{fig:sp_production} se puede observar los principales diagramas de Feynman de las distintas producciones. En la Figura \ref{fig:susy_xs} se muestras las secciones eficaces de producción de los distintos procesos, donde se manifiesta que la producción electrodébil tiene una sección eficaz notablemente menor a la fuerte.



Las búsquedas de SUSY realizadas por la colaboración ATLAS hasta la fecha han impuesto límites en la sección eficaz de producción y masas de las partículas supersimétricas, considerando diferentes tipos de producción y canales de decaimiento. La Figura \ref{fig:susy_xs_limits} resume algunos de estos resultados obtenidos hasta Junio de 2021.







El Gran Colisionador de Hadrones (\textit{\textbf{L}arge \textbf{H}adron \textbf{C}ollider} (LHC)) \cite{Evans:1129806} es el acelerador de hadrones de la Organización Europea para la Investigación Nuclear (CERN, por su antigua sigla en francés), ubicado en la frontera entre Francia y Suiza. El mismo consiste en un anillo de \magn{27}{km} de circunferencia, construido en el mismo túnel en el que funcionaba el acelerador $e^{+}e^{-}$ LEP (entre 1989 y 2000) \cite{LEPbook}, a una profundidad variable entre \magn{50}{m} y \magn{174}{m} de la superficie. El LHC está diseñado para colisionar protones a un máximo de energía de centro de masa\footnote{Definida como la raíz cuadrada de la variable de Mandelstan, $\sqrt{s}=|p_1+p_2|$, donde $p_1$ y $p_2$ representan los cuadrimomentos de las partículas incidentes.} de \com{14}{TeV}. Para ello, el CERN posee un complejo de aceleradores, que en sucesivas etapas, incrementan la energía de los protones, para luego inyectarlos en el LHC y hacerlos colisionar en cuatro puntos distintos, donde se encuentran los detectores más importantes: ATLAS \cite{PERF-2007-01}, CMS \cite{CMS-TDR-08-001}, LHCb \cite{LHCb:2008vvz} y ALICE \cite{ALICE:2008ngc}. En el presente Capítulo se describen las características principales del LHC, junto con la descripción y funcionamiento del detector ATLAS.


La producción de protones comienza extrayendose los electrones de un contenedor con gas de hidrógeno mediante campos magnéticos. Luego los protones pasan por un complejo de aceleradores, que en el pasado funcionaban como experimentos y que actualmente se utilizan para incrementan la energía de los protones en sucesivas etapas, como muestra la Figura \ref{fig:LHC_complex}. Inicialmente los protones son inyectados al acelerador lineal LINAC 2, que mediante cavidades de radiofrecuencia, acelera a los protones a una energía de \magn{50}{MeV}. Desde aquí son dirigidos al \textit{Proton Synchrotron Booster}, que consiste en cuatro anillos superpuestos con un radio de \magn{25}{m}, que aceleran los protones hasta una energía de \magn{1.4}{GeV}. Este último inyecta los protones en el \textit{Proton Synchroton}, de \magn{628}{m} de circunferencia, que acelera los protones a una energía de hasta \magn{26}{GeV} en el \textit{Super Proton Synchroton}. El mismo tiene una circunferencia de \magn{7}{km}, e inyecta protones de hasta \magn{450}{GeV} en ambos anillos del LHC. 



El último de los aceleradores es el LHC, donde los protones circulan en direcciones opuestas por cavidades de ultra alto vacío, a una presión de \magn{$10^{-10}$}{torr}. El mismo cuenta con $1232$ dipolos magnéticos superconductores de \magn{15}{m} de largo, enfriados a \magn{1.9}{K} mediante helio superfluido, que generan un campo magnético de \magn{8.4}{T} y permiten mantener en su órbita circular a los protones. Los dipolos están equipados con sextupolos, octupolos y decapolos, los cuales corrigen las pequeñas imperfecciones del campo magnético en las extremidades de los dipolos. Para aumentar la probabilidad de colisión, existe un sistema de focalización de los haces en las proximidades de los detectores, que estrecha el camino que recorren los protones. El mismo consiste de $392$ cuadrupolos magnéticos que generan campos magnéticos de \magn{6.8}{T}.

Los protones son acelerados mediante cavidades de radiofrecuencia, que generan una diferencia de potencial longitudinal a una frecuencia específica. Mediante esa frecuencia los protones son sincronizados, de tal forma que son acelerados o desacelerados hasta que alcanzan la energía deseada, y en ese punto ya no sufren ninguna aceleración longitudinal, llevándolos así a tener una energía precisa. Por tal motivo, el haz de protones se divide en paquetes discretos denominados \textit{bunches}, cada uno conteniendo del orden de 10$^{11}$ protones. El número de paquetes totales posibles en un haz con un espaciado de \magn{25}{ns}, es de 3564\footnote{Se obtiene al dividir la frecuencia de las cavidades, \magn{400}{MHz}, por la frecuencia de revolución, \magn{11}{kHz}, y considerando que sólo 1 de cada 10 paquetes es llenado para lograr el espaciado deseado}. Considerando los tiempos que se necesitan para en la inyección y descarte del haz, junto con los tiempos que necesita cada detector para procesar la información, no todos los paquetes son llenados, sino que se dejan <<espacios>> definidos por diferentes esquemas, dejando así el número efectivo de paquetes llenos a 2808.

Los aceleradores pueden ser caracterizados no solo por su energía de centro de masa, sino también por su \textbf{Luminosidad Instantánea} ($\mathcal{L}$), que mide el número de colisiones por unidad de área que ocurren en un período de tiempo, y se define como: 



donde $\sigma$ es la sección eficaz de la colisión y $R$ el número de colisiones. La expresión se puede escribir para el caso de un acelerador circular como el LHC, donde $f_{\text{rev}}$ es la frecuencia de revolución ($\sim$\magn{11}{kHz}), $n_{b}$ es el número de paquetes por haz, $N_{i}$ es el número de partículas en cada paquete, y $A$ es la sección efectiva del haz, que puede expresarse en término de los parámetros del acelerador como:



donde $\epsilon_{n}$ es la emitancia transversal normalizada (la dispersión transversal media de las partículas del haz en el espacio de coordenadas e impulsos), $\beta^{*}$ es la función de amplitud en el punto de interacción (relacionada al poder de focalización de los cuadrupolos), $\gamma$ es el factor relativista de Lorentz, y $F$ es un factor de reducción geométrico, debido al ángulo de cruce de los haces en el punto de interacción.

El número total de eventos esperados para un dado proceso con una sección eficaz $\sigma$, se obtiene como:




donde al factor integral se lo conoce como \textbf{Luminosidad Integrada}.

El LHC comenzó a funcionar en 2009 en lo que se denominó \textit{Run 1}. Durante el 2011 se realizaron colisiones a una energía de de centro de masa de \magn{7}{TeV}, y durante el 2012 a \magn{8}{TeV}, logrando finalmente recolectar una luminosidad total integrada de \magn{28.2}{\ifb} \cite{DAPR-2011-01,DAPR-2013-01}, que era apta para análisis físicos\footnote{El término <<apto para física>> hace referencia a los datos que pasaron una selección de calidad mínima para ser empleados en análisis físicos, y naturalmente son menores a los detectados por ATLAS y más aún a los proveídos por el LHC.}. En el 2013 finaliza la toma de datos y comienza el \textit{Long shutdown 1}, período que se utilizó para realizar distintas actualizaciones tanto al LHC como a los detectores, y preparándose así para la siguiente toma de datos. En el 2015 comenzó el \textit{Run 2}, que operaba a una energía de de centro de masa de \magn{13}{TeV}, y proveyó una luminosidad total integrada de \magn{139}{\ifb} \cite{lumi_13tev}, para luego finalizar en el 2018 y dar lugar al \textit{Long shutdown 2}. Este último estaba previsto con una duración de dos años, pero dada la situación epidemiológica de COVID-19, el mismo se terminó extendiendo hasta 2022.
Los planes a futuro del LHC prevén un \textit{Run 3} a \magn{13.6}{TeV} de tres años de duración aproximada, y luego ingresar en un nuevo período de inactividad para realizar las mejoras necesarias para el \textit{High Luminosity} LHC (HL-LHC). En la Figura \ref{fig:lhc_periods} se puede observar un diagrama de los períodos del LHC desde el Run 1 hasta el HL-LHC. La Figura \ref{fig:run2_lumi} muestra la luminosidad total integrada acumulada durante los días correspondientes al Run 2.


Las nuevas condiciones del Run 2 significaron un desafío para la toma de datos, en particular el incremento de \textit{pile-up}, que se define como el número promedio de interacciones por cruce de paquetes ($\langle \mu \rangle$). Cuando se cruzan dos paquetes de protones, varios protones de los mismos pueden interactuar, generando múltiples vértices primarios (\textit{in-time} pile-up). Esto dificulta la reconstrucción de los objetos del evento, debido a que la trayectoria de los mismos debe estar correctamente asociada a su respectivo vértice. Inclusive puede ocurrir la superposición de señales provenientes del paquete anterior o posterior (\textit{out-of-time} pile-up), lo que implica una dificultad adicional. Dichos efectos están contemplados en los distintos algoritmos de reconstrucción de objetos que se describen en el Capítulo \ref{cap:objects}. En la Figura \ref{fig:pileup} se puede observar el número promedio de interacciones por cruce de paquetes durante el Run 2, allí se observa que en promedio se tuvieron 30 interacciones por cruce, y hasta un máximo de 70. 




ATLAS (\textit{\textbf{A} \textbf{T}oroidal \textbf{L}HC \textbf{A}pparatu\textbf{S}})  \cite{PERF-2007-01} es uno de los experimentos multipropósito del LHC, diseñado para estudiar las colisiones protón-protón (y también colisiones de iones pesados) a altas energías provistas por el LHC. El mismo tiene una simetría aproximadamente cilíndrica y está compuesto de distintos subdetectores, que cumplen diversas funciones en la identificación de las partículas producidas durante las colisiones. 

En la zona más próxima al haz se encuentra \textbf{Detector Interno de Trazas} (ID), cuyo objetivo principal es reconstruir la trayectoria de las partículas cargadas. Está compuesto del \textit{Insertable B-Layer} (IBL), un detector de píxeles, un detector de bandas de silicio (SCT) y un detector de radiación de transición (TRT). A su vez, envolviendo al ID, se encuentra un solenoide superconductor que genera un campo magnético de \magn{2}{T}, el cual curva la trayectoria de las partículas cargadas permitiendo así medir su impulso. A continuación se ubica el sistema de calorímetros compuesto por el \textbf{Calorímetro Electromagnético} (ECAL) que mide principalmente la energía depositada por fotones y electrones, y el \textbf{Calorímetro Hadrónico} (HCAL) para medir la energía de los jets y hadrones. En la parte más externa, se encuentra el \textbf{Espectrómetro de Muones} (MS), diseñado para detectar la producción de muones y además medir su momento. Este último es el que le da a ATLAS su tamaño característico de \magn{45}{m} de largo y \magn{25}{m} de alto. Intercalado con el MS, se encuentra un sistema de imanes toroidales, que generan un campo magnético de \magn{4}{T} para curvar la trayectoria de los muones hacia el final del detector.

El detector ATLAS se divide geométricamente en dos regiones, la parte central denominada \textit{barrel} y la región extrema denominada \textit{endcap}. En la región barrel los detectores se ubican en forma de cilindros concéntricos alrededor del eje del haz, mientras en la región endcap se disponen como discos perpendiculares a la dirección del haz. La Figura \ref{fig:atlas_1} muestra todas las componentes que integran al detector ATLAS, que son descriptas en detalle en las siguientes secciones.




El sistema de coordenadas de ATLAS corresponde a un sistema cartesiano, cuyo origen coincide con el punto de interacción nominal ubicado en el centro del detector. El eje $z$ está orientado hacia la dirección del haz, el eje $x$ se define desde el punto de interacción hacia el centro del anillo del LHC, y el eje $y$ se define apuntando hacia la superficie terrestre. Es conveniente además, definir un sistema de coordenadas cilíndricas donde el radio $R$ representa la distancia perpendicular al haz, el ángulo azimutal $\phi$ es medido alrededor del eje del haz, y $\theta$ es el ángulo con respecto al eje $z$. La Figura \ref{fig:atlas_coordinates} muestra un diagrama del sistema de coordenadas de ATLAS.



Una variable utilizada en física experimental de altas energías es la rapidez:



donde $E$ es la energía total de la partícula, y $p_{z}$ es la componente en la dirección del haz de su impulso\footnote{Esta definición es un caso particular de la rapidez utilizada en relatividad especial, cuando se realiza una transformación en la dirección del haz del sistema de laboratorio, a un sistema donde la partícula solo se mueve perpendicular al haz.}. En el límite de altas energías, en donde la masa de la partícula es despreciable frente a su momento, es posible aproximarla a la llamada \textbf{Pseudorapidez} ($\eta$):


estando completamente relacionada con el ángulo $\theta$. La razón detrás de esta transformación de coordenadas, se debe a que



 la multiplicidad de partículas producidas es aproximadamente constante como función de $\eta$, y que 
 la diferencia de pseudorapidez entre dos partículas es invariante frente a transformaciones de Lorentz a lo largo de la dirección del haz. 


Como se mencionó anteriormente, al considerar colisiones hadrónicas de altas energías se hace uso del modelo de partones. Los partones acarrean una fracción del momento inicial de los hadrones, que a priori es desconocida. Si bien es posible medir una parte de ese momento, principalmente de los partones interactuantes que conforman la interacción fuerte, hay una fracción que escapa la detección. Esto imposibilita la reconstrucción del movimiento longitudinal del centro de masa en la interacción, y hacer uso de leyes de conservación sobre la cinemática total del evento. En cambio, teniendo en cuenta que el momento total de los partones en la dirección transversal al haz es nulo, el impulso total transverso se debe conservar durante la colisión. Por tal motivo, es común utilizar solo las componentes transversales en la descripción de la cinemática del evento, definidas en términos de la pseudorapidez, como por ejemplo el \textbf{Momento Transverso}:


donde $p$ es el momento de la partícula. De esta forma, es posible describir la cinemática de cada partícula en términos de las variables asociadas ($\pt$, $\eta$, $\phi$).


El detector ATLAS posee un poderoso sistema de imanes \cite{tenKate:409763}, utilizado para curvar la trayectoria de las partículas cargadas, pudiendo así medir tanto su impulso de forma precisa como también su carga. El mismo consta de dos tipos de imanes superconductores, uno en forma solenoidal y otros tres forma toroidal, enfriados a una temperatura de \magn{4.5}{K} para poder producir los fuertes campos magnéticos.

El solenoide rodea al detector interno, y tiene un tamaño de \magn{5.6}{m} de largo y \magn{2.56}{m} de diámetro, 
y con un espesor de apenas \magn{4.5}{cm}. El mismo produce un campo magnético de {$\smallsim$}\magn{2}{T} en la dirección del haz, por lo que las partículas cargadas son curvadas en la dirección de $\phi$. Para minimizar la interacción de las partículas que lo atraviesan y ahorrar la mayor cantidad de material posible, el solenoide comparte la cámara de vacío del calorímetro de argón líquido (LAr) descripto en las siguientes secciones.

Los toroides de ATLAS se componen de ocho bobinas, que generan campos de hasta {$\smallsim$}\magn{4}{T} en la dirección $\phi$, por lo que las partículas que lo atraviesan (prácticamente solo muones) son curvadas en la dirección $\eta$. Los más grandes miden \magn{25.3}{m} de largo y \magn{20.1}{m} de diámetro, y se ubican en la parte más externa del detector, en la región barrel, intercalados con el Espectrómetro de Muones descripto en las siguientes secciones. Los otros dos restantes se encuentran en la región endcap, por fuera de los calorímetros, y miden \magn{5}{m} de largo y \magn{10.7}{m} de diámetro. La Figura \ref{fig:magnet_1} muestra el esquema de imanes del detector ATLAS.


A continuación se describen los subdetectores de ATLAS, compuestos por el detector interno, el calorímetro electromagnético y hadrónico, y el espectrómetro de muones.



El detector interno es el más próximo al haz y su función principal es la reconstrucción de la trayectoria de las partículas cargadas. Esto además se emplea para medir la dirección, momento y carga de las mismas, junto con la reconstrucción de los vértices primarios. Para ello combina detectores de muy alta resolución cerca del haz, junto con detectores continuos de trazas en la zona más alejada. 
El principio básico de funcionamiento consiste en \textit{mapear} las coordenadas espaciales, con las señales que dejan las partículas al atravesar las celdas que componen al detector. 
Haciendo uso de su elevada granularidad de celdas, el conjunto de esas señales es reconstruida como una traza mediante algoritmos especializados. 
El detector interno está contenido dentro del solenoide superconductor, y mide \magn{6.2}{m} de largo y \magn{2.1}{m} de diámetro. El ID se compone de tres módulos que se describen a continuación, los cuales se pueden observar esquematizados en las Figuras \ref{fig:pixel_1} y \ref{fig:pixel_23}.




El detector de píxeles se emplea para medir la posición de las trazas de partículas cargadas con la más alta precisión posible, y es de vital importancia para la reconstrucción de los vértices primarios y secundarios. En la región barrel el detector se compone de tres capas cilíndricas, mientras que la endcap de tres discos. La capa más interna, denominada \textit{Insertable B-Layer} (IBL) \cite{ATLAS-TDR-2010-19}, se encuentra a \magn{50.5}{mm} del punto de interacción.
El principio de detección de una partícula cargada es la medida de la deposición de la carga inducida en una capa de silicio por ionización. El sistema contiene un total de $80$ millones de sensores, cada uno con una resolución de \magn{10}{$\mu$m} ($R-\phi$) y \magn{115}{$\mu$m} ($z$). Estos módulos en la región barrel, se encuentran levemente solapados y rotados para proveer una cobertura total en el ángulo azimutal. La Figura \ref{fig:pixel_3} muestra un esquema completo del detector de píxeles.

La inclusión del IBL fue una de las actualizaciones del Run 2 motivada por el incremento de luminosidad del LHC, lo que podía significar un daño por radiación en los detectores internos. En vez de reemplazar las partes del detector de píxeles que podían ser dañadas, se decidió colocar una capa adicional entre el detector de píxeles y la tubería donde circulan los protones. El objetivo del mismo es mejorar la eficiencia en la identificación de trazas, vértices, y en la identificación de quarks bottom, que decaen típicamente fuera del radio del IBL. El IBL está compuesto por $8$ millones de chips de rápida lectura y con sensores de silicio, que detectan el paso de partículas cargadas mediante la deposición de carga inducida. El tamaño de los píxeles es de $50\times250\,\mu$m$^{2}$, con una resolución de \magn{8}{$\mu$m} ($R-\phi$) y \magn{40}{$\mu$m} ($z$). La distancia entre el IBL y la tubería es de \magn{0.2}{mm}, y entre el tubo y el detector de píxeles es de \magn{1.9}{mm}. 



El SCT se encuentra por fuera del detector de píxeles y está diseñado para medir las trazas con alta precisión en la zona intermedia del detector. A diferencia del detector de píxeles, estos sensores de silicio están segmentados en micro bandas. Dado que es más baja la multiplicidad de partículas en esta región, es posible reducir la resolución con respecto a las capas más internas, al costo de aumentar el área de cobertura. La resolución del mismo es de \magn{17}{$\mu$m} ($R-\phi$) y \magn{580}{$\mu$m} ($z$). En la región barrel los módulos de SCT están dispuestos en cuatro capas concéntricas, y levemente solapados y rotados para proveer una cobertura total en el ángulo azimutal. La región endcap consiste en nueve discos transversales al eje del haz.




El TRT es el módulo más externo del ID y está diseñado no solo para detectar partículas cargadas, sino también para distinguir entre partículas pesadas y livianas. El TRT se compone de tubos detectores de \magn{4}{mm} de diámetro, con un gas que se ioniza al ser atravesado por partículas cargadas. Los electrones producidos son colectados por una ánodo, y el tiempo de deriva es una medida de la distancia a la traza del mismo. Además, los tubos están rodeados de fibras de polipropileno con un índice de refracción diferente, por lo que las partículas que atraviesan el detector emiten radiación con una intensidad proporcional a $\gamma=E/m$, permitiendo al TRT  distinguir partículas cargadas pesadas ($\pi^{\pm}$) de aquellas más livianas ($e^{\pm}$). La región barrel contiene $50000$ tubos paralelos al eje del haz y la región endcap $320000$ tubos orientados radialmente, cuya resolución es de \magn{0.17}{mm}.




El sistema de calorímetros de ATLAS está diseñado para medir la energía y la posición de las partículas, mediante la absorción de la energía depositada por las cascadas de partículas secundarias que estas generan en el material del mismo. Además, permite discriminar entre jets producidos por quarks o gluones de los electrones y fotones, detectar aquellas partículas neutras que no dejaron trazas en el ID, y realizar la selección online de eventos potencialmente interesantes (Ver Sistema de trigger). Gracias a su amplia cobertura y a que absorbe la energía de prácticamente todas las partículas producidas (salvo muones), es de gran utilidad para poder medir el desbalance de energía transversa, magnitud discriminatoria de mucha utilidad en la mayoría de análisis de física más allá del SM. Dicho sistema se compone de un calorímetro electromagnético (ECAL), dedicado principalmente a la medida de las deposiciones de partículas como fotones y electrones (partículas interactuantes principalmente vía interacción EM), y otro hadrónico (HCAL) dedicado a las cascadas de partículas producto de la hadronización de los quarks o gluones (jets) (partículas interactuantes principalmente vía interacción fuerte). La Figura \ref{fig:calo_1} muestra un esquema de los calorímetros del detector ATLAS.



El ECAL consiste en un calorímetro de muestreo (inhomogéneo) no compensado, que utiliza plomo como material absorbente y argón líquido (LAr) como medio activo. Consiste en varias placas de plomo dispuestas en forma de acordeón que se colocan de forma alterna inmersas en LAr. Las partículas incidentes interactúan con el plomo, creando una lluvia de partículas cargadas y neutras. Las partículas cargadas ionizan el medio activo, donde los electrones liberados son colectados en un electrodo central de kaptón/cobre hacia donde derivan por acción del campo eléctrico aplicado. La señal total en el medio activo es así proporcional a la energía total real de la partícula incidente. La ventaja de este método es la reconstrucción detallada de la forma de la cascada, al costo de no poder reconstruir la totalidad de la energía de la cascada debido al espacio que existe entre placa y placa.


El ECAL está dividido en dos mitades dentro de la región barrel ($\eta < 1.475$) y en dos componentes (una a cada lado) en la región endcap ($1.375 < |\eta| < 3.2$). 
En la región de transición\footnote{Denominada también como región \textit{crack}.} entre el barrel y el endcap, comprendida entre $1.37 < |\eta| < 1.52$, se encuentra una zona no instrumentada donde se encuentra el cableado del detector. 
Allí naturalmente la calidad de detección es menor, y es por ese motivo que la mayoría de los análisis excluye a los candidatos a fotones o electrones que estén reconstruidos en esta región.

En la región diseñada para medidas de precisión ($\eta < 2.5$, excluyendo el crack),
el ECAL está segmentado en tres capas longitudinales. La primera capa consiste de
bandas con fina granularidad (en la dirección de $\eta$), para discriminar entre fotones
aislados y pares de fotones espacialmente cercanos provenientes del decaimiento
$\pi^0\to\gamma\gamma$. Para los electrones y fotones con alta energía transversa, la mayoría
de la energía se colecta en la segunda capa, que tiene una granularidad lateral de
$0.025 \times 0.025$ en $(\eta, \phi)$. La tercer capa se encarga de la energía depositada en las
colas de la lluvia.
El espesor del ECAL es mayor a 22 longitudes de radiación ($X_0$) en la región
barrel, y mayor a $24X_0$ en los endcap, donde una longitud de radiación se define
como la distancia promedio a la cual la energía de un electrón se reduce a $E_0/e$, siendo $e$ su carga y $E_0$ su energía inicial. Para el caso de los fotones, una reducción similar se obtiene a
$9/7$ de $X_0$. Por tanto, toda la energía electromagnética es absorbida en el ECAL y
sólo parte de la componente hadrónica llega al HCAL. En la Figura \ref{fig:ecal} se observa un segmento del ECAL donde se muestra la granularidad de cada una de sus capas.



El HCAL es un conjunto de calorímetros que rodean al ECAL, extendiendo la aceptancia del calorímetro de ATLAS hasta cubrir prácticamente la totalidad de ángulo sólido del punto de colisión. El primero de los calorímetros se denomina \textit{Tile Calorimeter}, y es un calorímetro de muestreo que utiliza acero como material absorbente y tejas centelladoras plásticas como material activo. Se encuentra en la región barrel, y está dividido en dos partes que tienen una cobertura de $|\eta|<1.0$ y $0.8<|\eta|<1.7$ respectivamente. Las tejas centelladoras están dispuestas en un arreglo
periódico, y se conectan a una fibra óptica que trasporta la luz producida por el paso
de partículas hacia un tubo fotomultiplicador. Este arreglo se extiende radialmente, de los
\magn{2.28}{m} a los \magn{4.25}{m}. 
A cada lado de la región endcap, se encuentra un calorímetro hadrónico de muestreo (HEC) con placas de cobre como absorbente y argón líquido como material activo. El mismo consiste en dos ruedas con un radio de \magn{2.3}{m}, una atrás de la otra, con las placas planas de cobre dispuestas perpendicularmente al eje del haz. Finalmente se encuentra el \textit{Forward Calorimeter} (FCAL), un calorímetro de muestreo que extiende la cobertura del sistema a $|\eta|<4.9$, coaxial
al eje del haz y ubicado a \magn{4.7}{m} a cada lado del punto de interacción. El material
principal de los módulos es argón líquido (con cobre o tungsteno), y si bien no se
utiliza para mediciones de precisión, provee información para el cómputo de la energía transversa faltante, y la reconstrucción de jets en regiones muy cercanas al eje
del haz.



El HCAL tiene un espesor total mayor a $7.7$ longitudes de interacción
hadrónica ($\lambda$) en la región barrel ($9.7\lambda$ en total si se cuenta el ECAL). De manera
análoga a la longitud de radiación mencionada para el ECAL, una longitud de
interacción hadrónica se define como la distancia promedio sobre la cual la energía
de un hadrón se reduce a $1/e$ de su energía inicial. De esta forma, toda la energía
con la que llegan los hadrones al HCAL queda allí depositada.



El espectrómetro de muones (MS) se encuentra situado en la parte más externa del detector ATLAS. Esto se debe a que los muones de alto \pt generados en el punto de interacción tienen un altísimo poder de penetración y son poco interactuantes, siendo las únicas partículas detectables capaces de llegar a este detector. El mismo se encuentra intercalado con el sistema de imanes toroidales, y está diseñado para obtener mediciones de alta precisión de la posición e impulso de los muones, y para una rápida identificación para el sistema de trigger. Este es el subdetector más grande y el que le da a ATLAS su tamaño característico. 

El MS se compone de diferentes tipos de cámaras de detección de muones (ver Figura \ref{fig:muon_1}). Las \textit{Monitored Drift Tubes} (MDTs) son responsables de la mayoría de las medidas de precisión y cubren el rango de $|\eta|<2.7$. Funcionan de forma similar al TRT, con tubos llenos de un gas que ioniza y un ánodo central que recoge los electrones producidos, y el tiempo de deriva se asocia con la distancia a la traza. En la región endcap se encuentran las \textit{Cathode Strip Chambers} (CSCs) que poseen alta resolución espacio-temporal y una cobertura $|\eta|>2.0$. Estas cámaras funcionan midiendo la carga depositada en un ánodo, producto de la cascada de electrones creados cerca del mismo. Las \textit{Resistive Plate Chamber} (RPCs) proveen una estimación rápida del momento de los muones al primer nivel del trigger con una cobertura de $|\eta|<1.05$. Las RPCs miden la descarga ocasionada entre dos placas resistivas paralelas sometidas a una alta diferencia de potencial, tras la ionización del volumen de gas interno causada por el paso de muones energéticos. Finalmente en la región endcap, se encuentran las \textit{Thin Gap Chambers} (TGCs), similares en funcionamiento a las CSCs. Proveen también información al sistema de trigger en esta región y tienen una cobertura de $|\eta|<2.4$.


Como se mencionó anteriormente, el diseño del LHC permite tener una frecuencia de cruce de paquetes de \magn{40}{MHz} y del orden de 30 interacciones promedio por cruce de paquetes ($\langle \mu \rangle$), lo que da una tasa de interacción protón-protón del orden del GHz. Dicha frecuencia, requiere de un ancho de banda de escritura y una capacidad de almacenamiento considerablemente elevados. Aun así, no todos los eventos son de interés para la colaboración, como por ejemplo la colisión elástica de los protones que no genera ningún tipo de decaimiento. El \textbf{Sistema de Trigger} del detector ATLAS \cite{TRIG-2016-01} es el encargado de filtrar eventos de poco interés, y junto con el sistema de adquisición de datos (DAQ), almacena aquellos que potencialmente pueden llegar a ser de interés para los distintos análisis, reduciendo así la frecuencia de flujo de datos al orden del kHz. El sistema de trigger cumple un rol central en el correcto funcionamiento de todo el experimento, ya que en definitiva determina qué tipos de análisis se realizarán y qué nueva física podrá encontrarse. El mismo debe tener una alta eficiencia, para no desechar eventos importantes, pero con el compromiso de mantener el flujo de datos relativamente bajo. 

El sistema de trigger está compuesto por dos niveles consecutivos, capaces de realizar una identificación de partículas cada vez más compleja: un primer nivel de trigger (L1) basado en hardware y luego un trigger de alto nivel basado en software (HLT). La Figura \ref{fig:tdaq} muestra un esquema del sistema trigger y DAQ del detector ATLAS. Una secuencia de algoritmos del L1 y del HLT se denomina cadena de trigger o simplemente \textbf{Trigger}, que impone requisitos sobre las características de los objetos presentes en el evento, y a partir de ellas decide si el evento es aceptado. El conjunto de cadenas de triggers que se emplea durante la toma de datos se denomina \textit{Trigger menu}, que se optimiza previamente para satisfacer las condiciones del LHC sin afectar la adquisición de datos. Para controlar la tasa de eventos aceptados, a algunos triggers se les asigna un valor denominado \textit{prescale}, el cual puede estar tanto en el L1 como en el HLT. Un prescale de valor $n$ implica que uno de cada $n$ eventos es procesado por dicho trigger. El valor del prescale puede ser mayor a uno, reduciendo así la tasa de eventos aceptados (inclusive anulándola por completo), o igual a uno (<<sin prescale>> o \textit{unprescaled}). En este caso todos los eventos son evaluados, y se emplea en general para los triggers principales orientados para su uso en análisis físicos. El prescale es modificado de acuerdo a la luminosidad durante una toma de datos, para mantener la tasa de datos constante, sin saturar el ancho de banda. Para el caso de los triggers con prescale o aquellos directamente deshabilitados, se los puede configurar en un modo denominado \textit{rerun}, para el cual los algoritmos del trigger se corren offline una vez que el evento fue aceptado por otro trigger. A continuación se describen los dos niveles que componen al sistema de trigger.


El primer nivel del trigger \cite{Achenbach:2008zzb} está basado en hardware, y reduce los 40 MHz del LHC a menos de \magn{100}{kHz}, con una latencia de aproximadamente \magn{2.5}{$\mu$s}, tiempo determinado por el limitado tamaño de los \textit{buffers} de memoria, y por el tiempo que le toma a los muones producidos en el evento alcanzar el MS. Utiliza la información recolectada en una región reducida (capas) del calorímetro y del MS, para así reconstruir lo que se denominan Regiones de Interés (RoI). Las RoIs se determinan por la posición de la deposición de energía en el calorímetro por parte de las partículas consideradas de interés (en el caso de los muones es en el MS), permitiendo una rápida reconstrucción de su energía transversa, y aplicando un posible filtro en esta magnitud. El diseño del L1 permite tener una aceptancia en el rango de $|\eta|<2.5$ para electrones, fotones, muones y taus, hasta $|\eta|<3.2$ para jets, y $|\eta|<4.9$ para el cálculo del momento transverso faltante. Las RoIs a su vez sirven como semilla para el HLT, que realiza selecciones más detalladas a partir de las mismas.



Cuando un evento es aceptado por el L1, el mismo pasa a ser analizado por High Level Trigger \cite{ATLAS-TDR-16}, que está basado en software y permite reducir la tasa de eventos que se almacena a \magn{1.5}{kHz}, con una latencia de \magn{0.2}{s}. El mismo utiliza las RoIs previamente reconstruidas por el L1, y ejecuta una secuencia de algoritmos aplicados sobre el objeto candidato. Si uno de los pasos de la secuencia falla en los requisitos, los siguientes pasos no son aplicados para ahorrar tiempo de cómputo. 


Los algoritmos del HLT constan de dos etapas: los algoritmos de reconstrucción rápida, ejecutados primero, y luego los algoritmos de reconstrucción de precisión similar a los que se utilizan en la selección offline. Los algoritmos de reconstrucción rápida, utilizan la información de los calorímetros y de las trazas sólo dentro de la RoI para realizar la selección e identificación de los candidatos, y realizar el rechazo de fondo lo más rápido y temprano posible. Si la partícula candidata pasa los criterios definidos por la selección de reconstrucción rápida, se ejecutan los algoritmos de selección de precisión. Estos tienen acceso a la información del detector fuera de la RoI, con la máxima granularidad e incluyendo detalles sobre la calibración de energía de los calorímetros, la alineación de los subdetectores y el mapa de campo magnético. Los eventos aceptados por el HLT son finalmente grabados a disco y distribuidos, accesibles offline para todos los diferentes estudios y análisis.




La enorme cantidad de datos producidos tanto por las colisiones en el LHC ($\sim$\,PB al año), como por las simulaciones de MC, requiere un sistema de cómputo de alta complejidad, que permita el acceso de los mismos a todos los miembros de la colaboración de una manera ágil y directa. Esto se logra mediante la \textit{Worldwide LHC Computing Grid} (WLCG) \cite{grid}, la cual comparte el poder de procesamiento y la capacidad de almacenamiento entre distintos centros de cómputo distribuidos alrededor del mundo (\textit{Tiers}). Todos los eventos son inicialmente almacenados en el Tier 0 del CERN, el cual contiene el 20

Inicialmente los eventos aceptados por el HLT son almacenados como \textit{Raw Data Objects} (RDOs), los cuales contienen toda la información del detector. Esta información es de poca utilidad para los usuarios, y por ende es procesada en distintos formatos que contienen los objetos finales útiles para los análisis. Para ello se aplican distintos criterios de \textit{sliming} (remoción de los eventos que no son de interés), \textit{skiming} (remoción de la información irrelevante de los objetos) y \textit{thining} (remoción de objetos y/o colecciones de objetos irrelevantes) según los estudios y análisis que se vayan a realizar sobre los datos colectados. Los RDOs son empleados para la reconstrucción y calibración de objetos físicos, que se almacenan en el formato \textit{Event Summary Data} (ESD). Adicionalmente se realiza un procesamiento el cual descarta la mayoría de la información de las celdas, generando el formato \textit{Analysis Object Data} (xAOD), el cual tiene un tamaño de {$\smallsim$}\magn{100}{kB} por evento. Para el Run 2, se empleó un formato adicional denominado \textit{Derived Analysis Object Data} (DAOD), o simplemente derivación. El mismo aplica una selección a los eventos de las xAOD que reduce su tamaño a 10-15\,kB por evento. Estos últimos formatos son archivos accesibles vía el entorno de análisis de datos \texttt{ROOT} \cite{Brun:1997}, que contienen el conjunto de objetos físicos finales reconstruidos, y son empleados por los usuarios para realizar los distintos análisis.
El software de ATLAS se desarrolla dentro un entorno \texttt{C++} común llamado \texttt{ATHENA} \cite{ATLAS-TDR-17, analysistools, Calafiura:865624}, en el que se realiza todo el procesamiento de datos. 

Existen varias derivaciones dependiendo de la selección empleada por cada análisis. Las derivaciones de interés para esta tesis son las denominadas \texttt{EGAM3}, \texttt{EGAM4} y \texttt{SUSY1}. Las \texttt{EGAM3} y \texttt{EGAM4} son utilizadas en esta tesis para la medida de la eficiencia de los trigger de fotones, ya que preseleccionan eventos con bosones $Z$ decayendo radiativamente a partir de electrones o muones respectivamente. La derivación \texttt{SUSY1} es utilizada en esta tesis para preseleccionar los eventos para la búsqueda de supersimetría, y en general selecciona eventos con objetos energéticos. A continuación se lista un resumen de las selecciones que deben cumplir los eventos para ser aceptados por estas derivaciones:




El método de Monte Carlo utiliza una serie de algoritmos computacionales basados en la repetición de eventos con un factor variable y aleatorio, que permite obtener resultados numéricos globales.
En el contexto de la física de partículas, no solo se utiliza para reproducir la aleatoriedad impuesta por la mecánica cuántica, sino también para realizar diferentes aproximaciones, como es el caso de integrales numéricas.
Las simulaciones de Monte Carlo cumplen un rol fundamental a la hora de realizar predicciones de la teoría, teniendo un rol primordial en prácticamente todos los análisis realizados por la colaboración ATLAS. 

Las simulaciones de colisiones de protones se realizan a partir de complejos cálculos basados en la teoría de QCD, y descriptas brevemente en la Sección \ref{sec:qcd_pp}. 
El punto central de la simulación es la dispersión dura (\textit{Hard Scatter}, HS), que se calcula a un dado orden de la teoría de perturbaciones. Para ello se emplean programas generadores de elementos de matriz (ME), las cuales contienen la información de la función de onda para los partones entrantes y salientes, y depende principalmente del acoplamiento
fuerte y la escala de energía de la dispersión. La evolución de los partones producidos se realiza mediante un modelo de lluvia de partones (\textit{Parton Shower}, PS) que conecta la escala dura de los partones de color, con la escala de la hadronización donde se generan los hadrones de color, y es el límite de validez del régimen perturbativo $\Lambda_{\text{QCD}}\sim 1\,\gev$. En esta etapa, los partones son transformados en hadrones primarios aplicando modelos de fragmentación puramente fenomenológicos, generando así una cascada de partículas formada por un gran número de gluones y quarks de baja energía. La combinación entre la dispersión dura y la lluvia partónica debe realizarse
con especial cuidado de no duplicar en la PS los partones ya producidos y evitar
así el doble conteo. Las estrategias principales para esto se conocen como CKKW
(Catani-Krauss-Kuhn-Webber) \cite{Catani_2001, Krauss_2002} y MLM \cite{Mangano_2003}. Finalmente los hadrones son decaídos en partículas estables que son las que observa el detector.

Adicional al HS existen otros procesos QCD que ocurren en la colisión. Debido a que los quarks tienen carga de color, y los gluones tienen acoplamientos triples y cuádruples, puede suceder que los partones emitan quarks o gluones antes de la dispersión dura (\textit{Initial State Radiation}, ISR). De manera análoga, los partones salientes pueden emitir gluones o
producir pares de quarks/anti-quarks (\textit{Final State Radiation}, FSR). A su vez, pueden darse interacciones adicionales entre partones pertenecientes a los protones originales, que no participaron de la dispersión dura, dando lugar a múltiples
interacciones en el evento, mayoritariamente interacciones de bajo momento. La actividad no asociada con la dispersión dura se llama evento subyacente
(\textit{Underlying Event}, UE), que debido a la baja energía de las interacciones que lo
componen, no puede ser tratado de manera perturbativa y se requieren de modelos
fenomenológicos para describirlos. En la Figura \ref{fig:mc_qcd} se puede observar el diagrama de una colisión entre partones, junto con la cascada de partículas producto de la misma.


Algunos de los programas empleados para la generación de eventos y simulación de PS son \texttt{Sherpa} \cite{SherpaGen, Schumann:2007mg, Bothmann:2019yzt}, \texttt{PYTHIA} \cite{Sjostrand:2014zea} y \texttt{MadGraph} \cite{Alwall:2014hca}. A su vez pueden ser utilizados con un conjunto de parámetros ajustados por la colaboración ATLAS, en lo que se denomina \textit{tuning} A14 \cite{ATL-PHYS-PUB-2014-021} para el conjunto de PDFs CTEQ6L1 \cite{Pumplin:2002vw}, MSTW2008LO \cite{Martin:2009iq, Martin:2009bu, Martin_2010}, NNPDF23LO \cite{Ball:2012cx,Ball:2014uwa}. 


Luego de la generación de los eventos y las partículas presentes en el mismo, se simula la interacción de las partículas con el detector. 
Para ello se emplean programas que simulan el paso de partículas a través de la materia, junto con una simulación completa del detector ATLAS, en donde se detalla precisamente los materiales empleados y las dimensiones de los mismos. Este paso se realiza mediante el programa \texttt{Geant4} \cite{Geant4} para simulaciones completas del detector. A su vez, se puede realizar una simulación de la interacción en el calorímetro de forma simplificada para ahorrar recursos y tiempo de cómputo, denominada \texttt{ATLFAST-II} \cite{Richter-Was:683751,Lukas_2012}. Una vez realizada la simulación del detector, los eventos son reconstruidos con los mismos algoritmos empleados para datos. Se realizaron tres campañas de simulaciones, \texttt{mc16a}, \texttt{mc16d} y \texttt{mc16e}, generadas con la distribución prevista de pile-up para los datos tomados en los años 2015+2016, 2017 y 2018 respectivamente.




Si bien las simulaciones logran reproducir muchos procesos con una alta eficacia, es necesario aplicarles a las mismas una serie de correcciones tanto a nivel generador, como en la instancia de reconstrucción de objetos. En general estas correcciones se aplican como pesos a los eventos, los cuales aumentan o disminuyen el impacto de dicho evento en el análisis.

La primera corrección proviene de los generadores que producen los eventos con un peso intrínseco $w_\text{MC}$, propio de los cálculos que hace, los cuales deben ser aplicados siempre a cada evento. Estos pesos en general toman valores cercanos a la unidad pero no hay un impedimento sobre los mismos, que inclusive a veces toman valores negativos. Si bien no ocurre con la mayoría de eventos, esto puede generar que en una selección muy estricta de un análisis la suma total de eventos dé negativa, algo que uno no esperaría en un experimento de conteo. Si bien no es fácil entender de forma directa estos pesos, en general existen para evitar el doble conteo de ciertos procesos. Aún así no pueden ser descartados, y se deben emplear métodos alternativos para lidiar con ellos.

Otra de las correcciones que se aplica a las simulaciones es la normalización por luminosidad. En la Ecuación \ref{eq:lumi_xs} se puede observar la relación entre la luminosidad y el número de eventos de un proceso con una dada sección eficaz. Como las simulaciones son generadas con un número arbitrariamente grande de eventos ($N$), se les aplica un peso a las mismas para que el número de eventos final corresponda al de la producción de dicho proceso un período con una cierta luminosidad, $w_\text{lumi}=\mathcal{L}\sigma/N$. Por otro lado, existe una corrección asociada al pile-up que se debe aplicar a las simulaciones. Las tres campañas de simulaciones fueron generadas con la distribución prevista de pile-up, y mediante la aplicación de un peso a los eventos $w_\text{PU}$, se puede corregir tal distribución para que se asemeje a la observada en datos.

Las simulaciones hacen uso de distintos algoritmos para la reconstrucción general del evento y sus objetos, entre los que se encuentran algoritmos del trigger, y los empleados para la reconstrucción, identificación y aislamiento de objetos. Si bien estos algoritmos son los mismos que se usan para los datos, los resultados obtenidos para las simulaciones no siempre se asemejan a ellos, y por eso se les aplica un factor multiplicativo denominado \textbf{Factor de Escala} (SF). Estos factores son aplicados multiplicativamente al evento en el caso de que el mismo haya hecho uso del algoritmo asociado al factor de escala, generando un peso de la forma: 



donde el último factor engloba correcciones opcionales, como por ejemplo a la carga de los electrones o de reconstrucción en la región forward. En general estos factor son muy cercanos a la unidad y se calculan en función de alguna de las variables del objeto. En la Sección \ref{sec:trig_sf} se describe un método para obtener los factores de escala asociados a la eficiencia del trigger de fotones.

Correcciones adicionales pueden ser aplicadas a las simulaciones. Por ejemplo los cálculos a NLO pueden ser encapsulados en un factor denominado \textit{k-factor}, que es el cociente entre la sección eficaz de un dado proceso a NLO y a LO. Este factor multiplicativo puede ser aplicado a las simulaciones hechas a LO para incluir de alguna forma simple las correcciones a NLO. De forma equivalente se puede obtener un \textit{k-factor} para cálculos de órden superior a NLO. También  puede ocurrir que al generar un cierto proceso, solo sea necesario quedarse con aquellos eventos que pasen con una cierta selección, evitando almacenar el resto de los eventos que no lo hagan. Para ello se aplica un filtro a nivel generador, que en el caso de aplicarlo, es necesaria la corrección de dichas simulaciones a partir de un factor multiplicativo que tenga en cuenta este filtrado.




El diseño del detector ATLAS permite la reconstrucción e identificación de prácticamente todas las
partículas producidas en la colisión $pp$. 
La mayoría de las partículas del SM son inestables, que decaen rápidamente en otras partículas estables. Esto reduce considerablemente las posibles partículas producidas en la colisión que llegan al detector, ya que solo van a ser aquellas que sean estables o vida media suficientemente larga, siendo estas principalmente: $\gamma$, $e^{\pm}$, $\mu^{\pm}$, $\nu$ y algunos hadrones
como $p$, $n$, piones y kaones. El diseño de los distintos subdetectores permite aprovechar las
características de cada una de ellas, haciendo que las mismas depositen señales distintivas, permitiendo así su reconstrucción e identificación. Un esquema de las distintas señales producidas por cada una de las partículas en el detector ATLAS se muestra en la Figura \ref{fig:particulasATLAS}. Todos los procesos de reconstrucción descriptos a continuación se realizan sobre los datos que fueron almacenados en disco (\textit{offline}), a partir de eventos que pasaron los requisitos del sistema de trigger y almacenamiento durante la toma de datos (\textit{online}).


Los electrones y fotones producidos tanto en la colisión $pp$ como aquellos producto del decaimiento de otras partículas, depositan la mayor parte de su energía en el ECAL. Estos depósitos están restringidos a un número de celdas vecinas cuyo conjunto se denomina \textit{cluster}, y que tienen estructuras propias de estas partículas. Los depósitos que dejan ambas partículas son similares y con el objetivo de poder distinguirlas se utiliza además información del detector de trazas. Al ser el fotón una partícula neutra no deja traza en el ID, por lo que los clusters que no están asociados a trazas son considerados fotones, mientras que los que los que sí lo están son considerados electrones. En la Figura \ref{fig:el_reco} se observa una ilustración esquemática del paso de un electrón a través del detector.



Procesos como la producción de pares electrón-positrón ($\gamma\to e^{-}e^{+}$), producto de la interacción de los fotones con el material del detector, pueden dejar trazas o múltiples depósitos, que genera una ambigüedad con el criterio anterior.
El algoritmo de reconstrucción tiene en cuenta esto y puede reconstruir los vértices de dicha conversión, por lo que los clusters asociados a vértices de conversión son considerados fotones. Por otro lado, ciertos procesos (ej. $\pi^{0}\to\gamma\gamma$) pueden generar depósitos que al estar muy juntos, pueden ser erróneamente reconstruidos como uno solo.
Para reducir tal reconstrucción errónea se aplican entonces una serie de criterios de identificación y aislamiento, basados en las formas de los depósitos de energía, que permiten discriminar este tipo de procesos.
La reconstrucción de electrones y fotones comienza de forma simultánea para ambos, identificando los clusters y trazas presentes. Luego de forma independiente se agrupan los clusters y se verifica si cumplen los requisitos para ser asignados como fotón o electrón. Finalmente se calibran y reconstruyen variables asociadas a los mismos de forma simultánea nuevamente.



La reconstrucción de electrones y fotones en el detector ATLAS se realiza utilizando un algoritmo para la reconstrucción de clusters dinámicos de tamaño variable, denominados \textit{topo-clusters} que se agrupan además en \textit{superclusters}\cite{EGAM-2018-01}. Durante Run 1 el algoritmo reconstruía clusters de tamaño fijo \cite{PERF-2013-04, PERF-2013-05, Lampl:1099735}, que si bien tenían una respuesta lineal energética y un estabilidad frente a pile-up, no permitía reconstruir eficientemente la energía de fotones \textit{bremsstrahlung} o de electrones/positrones producto de la creación de pares. La implementación de superclusters durante el Run 2, junto con la calibración de la energía descripta en la Referencia \cite{PERF-2017-03} permite solucionar esto sin perder la linealidad y estabilidad de los clusters de tamaño fijo.



El algoritmo comienza buscando las celdas en el ECAL y el HCAL con una señal\footnote{Para los topo-clusters electromagnéticos la medida de la señal se realiza en la escala electromagnética, que es la escala adecuada para medir los depósitos de energía de las partículas producidas en lluvias electromagnéticas de forma correcta} 
cuatro veces mayor al ruido esperado dadas las condiciones de luminosidad y pile-up del Run 2. A partir de ellas agrega las celdas vecinas cuya señal sea dos veces mayor al ruido,
que a su vez son utilizadas en la siguiente iteración del algoritmo, que se repite hasta que no haya más celdas adyacentes que cumplan este requisito. Finalmente se agregan todas las celdas vecinas a las celdas anteriores, independientemente de la intensidad de señal que tengan, formando lo que se denominan topo-clusters \cite{PERF-2014-07, Lampl:1099735}. Los topo-clusters que compartan celdas son unificados, mientras que los topo-clusters que tengan dos máximos locales son divididos. La reconstrucción de electrones y fotones comienza entonces a partir de la formación de los topo-clusters, y utiliza solo la energía de las celdas del ECAL. Para continuar se requiere que dicha energía supere los \magn{400}{MeV} y que la fracción de la misma con respecto a la energía total del topo-cluster sea mayor a $0.5$ (reduciendo una gran parte de los efectos del pile-up).  




La reconstrucción de trazas se realiza utilizando un algoritmo de búsqueda de patrones de trazas estándar
\cite{Cornelissen:1020106, PERF-2017-02, PERF-2017-01} en todo el ID. A su vez, utiliza los depósitos en el ECAL que presenten una forma compatible con la de una lluvia electromagnética para definir regiones de interés. En caso de que el algoritmo anterior falle, se utiliza en estas regiones otro algoritmo de búsqueda de trazas \cite{FRUHWIRTH1987444}, permitiendo reconstruir trazas adicionales. Luego se realiza una serie de ajustes ($\chi^2$ \cite{Cornelissen:1176901}, GSF \cite{ATLAS-CONF-2012-047}) de las trazas permitiendo obtener correctamente los parámetros que la caracterizan. Finalmente las trazas son asociadas a los topo-clusters extrapolando a la misma desde el perigeo hasta la segunda capa del ECAL. Una traza se considera asociada con un topo-clusters si $|\eta_{\text{traza}}-\eta_{\text{cluster}}|<0.05$ y $-0.10<q\cdot(\phi_{\text{traza}}-\phi_{\text{cluster}})<0.05$, donde $q$ es la carga de la traza. A su vez, el momento de la traza es escaleado para que coincida con al energía del topo-cluster asociado. Si múltiples trazas son asociadas a un mismo topo-cluster se clasifica a las mismas utilizando criterios de calidad, siendo la mejor clasificada la que se utiliza para reconstruir a los electrones. 

Los vértices de conversión son reconstruidos a partir de pares de trazas con cargas de signo opuesto y consistentes con el decaimiento de una partícula sin masa. Adicionalmente se pueden reconstruir vértices de conversión a partir de una sola traza que no haya dejado señal en las capas más internas del ID. En ambos casos se busca que la traza tenga altas probabilidad de ser un electrón en el TRT \cite{ATLAS-CONF-2011-128} pero baja en el SCT. Es esperado que las trazas de los vértices de conversión estén muy cerca una de otra, en general compartiendo \textit{hits}, haciendo que una de las trazas no llegue a reconstruirse. Para ello se utilizan trazas con requisitos de asociación a topo-clusters más relajados que los anteriormente descriptos, y con distintos criterios de ambigüedad ante solapamiento. Finalmente los vértices son asociados a los topo-clusters, y en caso de múltiples vértices asociados a un mismo topo-cluster se prioriza aquellos reconstruidos a partir de dos trazas y cuyo radio sea menor.



La reconstrucción de los superclusters para electrones y fotones se realiza de forma independiente y en dos etapas: primero se encuentran los topo-clusters semilla 
y luego se le adjuntan los topo-clusters satélites producidos generalmente por \textit{bremsstrahlung} o por la división de topo-clusters. El algoritmo comienza ordenando todos los topo-clusters por \ET, y comienza por el más energético a verificar si pasan los requerimientos para ser un topo-clusters semilla asignados para electrones o fotones. En el caso de los electrones el requisito es tener \ET mayor a \magn{1}{GeV} y una traza asociada con al menos cuatro \textit{hits} en el SCT, mientras que el de los fotones es tener \ET mayor a \magn{1.5}{GeV} y ningún requisito sobre trazas o vértices de conversión. Cuando un topo-clusters pasa estos requisitos se busca sus topo-clusters satélites asociados y el mismo no puede ser utilizado como satélite en las siguientes iteraciones. Los topo-clusters satélites son aquellos que se encuentran dentro de una ventana de $\Delta\eta\times\Delta\phi=0.075\times0.125$ alrededor del centro del topo-cluster inicial. Para electrones además se consideran topo-clusters satélites aquellos que se encuentran dentro de una ventana de $\Delta\eta\times\Delta\phi=0.125\times0.3$ cuya traza mejor ajustada coincide con la traza mejor ajustada del topo-cluster inicial. Para fotones convertidos además se consideran topo-clusters satélites aquellos que compartan el vértice de conversión con el topo-cluster inicial. El conjunto de un topo-cluster semilla junto con sus satélites asignados se denominan supercluster. En la Figura \ref{fig:superclusters_sat} se muestra el esquema de formación de superclusters para fotones y electrones. Para limitar la sensibilidad de los superclusters al pile-up, el tamaño de cada topo-cluster constituyente es restringido a un máximo de $0.075$ ($0.125$) en la dirección de $\eta$  en la región barrel (endcap). 


Como el algoritmo se utiliza de forma independiente tanto para electrones como para fotones, puede ocurrir que un mismo supercluster se asocie tanto a un electrón como a un fotón simultáneamente. En ese caso se utilizan una serie de criterios de ambigüedad que permiten determinar si el candidato es un electrón o un fotón. En el caso que aún no pasen los criterios de ambigüedad, el candidato es guardado como electrón y fotón simultáneamente, pero marcados como ambiguos y es decisión de cada análisis incluirlos en el mismo. 




Finalmente se calibra la energía de los superclusters, las trazas son nuevamente ajustadas pero ahora utilizando los superclusters anteriores, y la energía es recalibrada teniendo en cuenta este nuevo último ajuste siguiendo el procedimiento descripto en la Referencia \cite{PERF-2017-03}.



Como se mencionó anteriormente, distintos criterios de identificación son utilizados para poder discriminar los objetos prompt
de aquellos que no lo son. Para ello se definen una serie de variables (\textit{Shower shapes}) basadas en la información del calorímetro y del ID, que mediante distintas técnicas permiten la correcta identificación de los objetos. Finalmente se definen diferentes puntos de trabajo (\textit{Working Points}, WP) que permiten mejorar la pureza de los objetos seleccionados al costo de tener una menor eficiencia de selección.

La identificación de electrones tiene como principal objetivo discriminar los electrones prompt de los fotones convertidos, de jets que depositaron energía en el ECAL y de electrones producidos en el decaimiento de hadrones de sabor pesado. Esta identificación se basa en un método de likelihood que utiliza algunas de las variables descriptas en la Tabla \ref{tab:phIDVars}, y cuyas PDFs se obtienen de eventos con decaimientos de $J/\Psi$ \cite{tesis_fer} y $Z$ para electrones de bajo y alto \ET respectivamente \cite{PERF-2016-01}. Para electrones se definen tres WPs: \texttt{Loose}, \texttt{Medium} y \texttt{Tight}, cuyas eficiencias de identificación promedio son  93\%, 88

La identificación de fotones está diseñada para seleccionar eficientemente fotones prompt. y rechazar los fotones falsos provenientes de jets, principalmente del decaimiento de mesones livianos ($\pi^{0}\to\gamma\gamma$). La identificación se basa en una serie de cortes rectangulares sobre las variables presentes en la Tabla \ref{tab:phIDVars}. Las variables que utilizan la primer capa del ECAL son esenciales para discriminar los decaimientos del $\pi^{0}$ en dos fotones muy colimados, ya que los depósitos de energía de este decaimiento se extienden en más celdas de este capa en comparación con el depósito de un fotón real. En la Figura \ref{fig:phpizero} se puede observar la comparación de ambos procesos. 
Para la identificación de fotones también se definen tres WPs: \texttt{Loose}, \texttt{Medium} (empleado solamente en la reconstrucción en el HLT) y \texttt{Tight}, cada uno inclusivo con respecto al anterior, y en la Tabla \ref{tab:phIDVars} se muestran las variables empleadas por cada uno de ellos. En la Figura \ref{fig:shower_shapes} se observa un esquema de algunas de las variables empleadas.
Como los depósitos de energía varían debido a la geometría del calorímetros, los tres WPs fueron optimizados para diferentes valores de $|\eta|$, y adicionalmente la selección \texttt{Tight} fue optimizada para distintos valores de \ET. Los depósitos de energía de los fotones convertidos difiere de los no convertidos, debido a la separación angular entre el $e^-$ y el $e^+$ que se amplifica por el campo magnético, y debido a la interacción de los pares con capas más altas del calorímetro, permitiendo optimizar la selección \texttt{Tight} de forma separada para fotones convertidos de los no convertidos. Esto no fue posible para las selecciones \texttt{Loose} y \texttt{Medium} ya que la información que utilizan no permite saber si un fotón es convertido o no. La optimización fue realizada a bajo \ET utilizando simulaciones de decaimientos radiativos del bosón $Z$ junto con datos con eventos con bosones $Z$, y a alto \ET con simulaciones de producción de fotones inclusiva y jets. La eficiencia de identificación para la selección \texttt{Tight} supera el 80






Criterios de aislamiento se pueden aplicar sobre los fotones y electrones para aumentar aún más la calidad de selección de los mismos. A su vez, la presencia de otros objetos cerca del fotón o el electrón puede interferir en la correcta reconstrucción de las variables cinemáticas del mismo, como su energía. El aislamiento de estos objetos se puede cuantizar definiendo variables no solo para los depósitos de energía, sino también para las trazas.

La variable de aislamiento calorimétrico \cite{PERF-2017-01} (\ETcone{X}) se define entonces como la suma de la energía transversa de todas las celdas contenidas en un cono centrado en el topo-cluster, y cuyo radio $\Delta R$ \footnote{$\Delta R=\sqrt{\Delta\phi^2+\Delta\eta^2}$} (en el plano $\eta-\phi$) es igual a X/100. La contribución energética del objeto a asilar se sustrae ignorando las celdas contenidas en un rectángulo en el centro del cono, y cuyos lados miden $\Delta\eta\times\Delta\phi = 5 \times 7$ como muestra la Figura \ref{fig:IDcone}. Las filtraciones energéticas del candidato fuera del rectángulo son tenidas en cuenta junto con los efectos de pile-up \cite{Cacciari_2008}. Para electrones se utiliza un cono de radio $\Delta R = 0.2$ (\ETcone{20}), mientras que para fotones se utiliza uno de $\Delta R = 0.2$ (\ETcone{20}) o $\Delta R = 0.4$ (\ETcone{40}) dependiendo del WP.



La segunda variable de aislamiento se obtiene en base a las trazas de los objetos reconstruidos (\pTcone{XX}), se define como la suma del momento transverso de todas las trazas contenidas dentro de un cono centrado en la traza del electrón o en la dirección del cluster del fotón convertido. La traza asociada al electrón o al fotón convertido son excluidas de esta suma, al igual que aquellas que no pasen una serie de criterios de calidad mínima. Como los electrones producidos en el decaimiento de partículas pesadas pueden estar en cercanía de otras partículas, la variable de aislamiento de trazas utiliza un cono de radio variable, cuyo tamaño se reduce a alto \pt. La variable se denomina \pTvarcone{XX} donde XX es el radio máximo utilizado, que para el caso de los electrones es $\Delta R_{\text{máx}} = 0.2$ (\pTvarcone{20}). En el caso de los fotones el radio del cono mide $\Delta R = 0.2$ (\pTcone{20}).


A partir de estas variables se definen distintos WPs de aislamiento de electrones dependiendo de si se desea mantener constante la eficiencia o si se desea aplicar cortes fijos en las variables de aislamiento. Un ejemplo de WP de aislamiento para electrones es el \texttt{FixedCutLoose} con una eficiencia de selección mayor a 90

 
Quedando por ejemplo el WP \texttt{FixedCutTight} como $\ETiso<0\,\gev$ y $\pTiso<0.05$.




Las partículas que interactúan electromagnéticamente pierden energía por radiación de frenado al atravesar la materia. El muón tiene una masa de aproximadamente 200 veces la masa del electrón, lo que reduce enormemente la energía que pierde en su trayecto, a diferencia de los electrones que pueden ser totalmente absorbidos en el ECAL. A su vez, como el muón no interactúa mediante la fuerza fuerte, tampoco puede ser absorbido en el HCAL. Lo que lo convierte en prácticamente la única partícula detectable capaz de escapar de los calorímetros, y llegar a la parte más externa donde se encuentra justamente el Espectrómetro de Muones (donde tampoco son absorbidos). Lo mismo ocurre con los muones provenientes de los rayos cósmicos en la atmósfera, que logran atravesar la superficie terrestre y llegar a ser detectados de la misma forma. Estos eventos pueden ser caracterizados debido a que en general están alejados del punto de interacción, y además la trayectoria de los mismos va desde la parte externa del detector hasta la más interna.


La reconstrucción de muones se realiza de forma independiente en el detector interno y en el espectrómetro de muones. La información de los distintos subdetectores, que incluye a los calorímetros, se combina para formar a los objetos finales utilizados en los análisis \cite{PERF-2015-10}. La reconstrucción en el ID se realiza de la misma forma que con cualquier otra partícula cargada \cite{Cornelissen:1020106, ATLAS-CONF-2010-072}. La reconstrucción en el MS comienza con una búsqueda de patrones de \textit{hits} para definir segmentos en cada cámara de muones, que luego son combinados con un ajuste de $\chi^2$ global. Luego se combina la información del ID, MS y los calorímetros, utilizando una serie de algoritmos que definen 4 tipos de muones dependiendo del subdetector que se utilizó en la reconstrucción:


Muones Combinados (CB): reconstruidos en el ID y el MS de forma independiente, y luego mediante un ajuste se reconstruye una traza combinada.

Muones Segmentados (ST): trazas del ID que al extrapolarlas al MS tienen asociadas un segmento en el MDT o el CSC. Se definen principalmente para reconstruir aquellos muones de bajo \pt o que atraviesan las regiones del MS con baja aceptancia.

Muones Calorimétricos (CT): trazas del ID que están asociadas a depósitos de energía en el calorímetro compatibles con una partícula mínimamente ionizante. Este tipo de muones son los de menor pureza pero permite detectarlos en regiones donde el MS está parcialmente instrumentado. 
 

Muones Extrapolados (ME): reconstruidos utilizando solo el MS y requiriendo que hayan dejado traza en la región \textit{forward} además de una mínima compatibilidad con el punto de interacción. Se definen principalmente para extender la aceptancia a la región $2.5<|\eta|<2.7$ donde el ID no llega a cubrir.


En caso de solapamiento entre los distintos tipos de muones se resuelve teniendo prioridad por los CB, luego por los ST y finalmente por los CT. Para los ME se priorizan aquellos muones con mejor calidad en el ajuste de la traza y mayor cantidad de \textit{hits}.

La identificación de muones se realiza con el objetivo de discriminar muones prompt. de aquellos producidos principalmente en el decaimientos de piones y kaones, manteniendo una alta eficiencia y garantizando una medida robusta de su momento. Los muones producidos en el decaimiento de hadrones cargados dejan una traza en el ID con una topología enroscada 
que genera discrepancias entre el momento reconstruido en el ID y el reconstruido en el MS. La identificación se realiza aplicando una serie de cortes en diferentes variables \cite{PERF-2015-10} obtenidas a partir del estudio de simulaciones de producción de pares de quarks top. Se definen cuatro WPS, \texttt{Loose}, \texttt{Medium}, \texttt{Tight}, y \texttt{High-pT}, para satisfacer las necesidades de los distintos análisis. Por ejemplo, la selección \texttt{Loose} está optimizada para reconstruir candidatos del decaimiento del bosón de Higgs, la selección \texttt{Medium} es la selección más genérica para todos los análisis, y la selección \texttt{High-pT} está orientada a búsquedas de resonancias de alta masa del $Z'$ y $W'$ \cite{EXOT-2016-18,EXOT-2016-06,EXOT-2016-05,EXOT-2015-04}. 

Finalmente se definen criterios de aislamiento que permiten distinguir aquellos muones producidos en los decaimientos de los bosones $Z$, $W$ y Higgs que en general se producen de forma aislada, de aquellos producidos en los decaimientos semi-leptónicos que quedan embebidos en los jets. Para ello se definen diferentes
WPs, utilizando las mismas variables de aislamiento calorimétrico y de trazas utilizadas para fotones y electrones (\pTvarcone{30} y \ETcone{20}). Algunos WPs de interés para esta tesis están listados en la {Tabla \ref{tab:muon_WPs}}.



Como se mencionó en la Sección \ref{sec:qcd_pp}, debido al confinamiento de color los quarks o gluones, que tienen carga de color no nula, estos no pueden existir libres en la naturaleza. Al producirse quarks o gluones en la colisión estos crean nuevas partículas de color para generar partículas de carga de color nula. Este proceso se denomina hadronización, y produce en el detector un jet de partículas de forma similar a un cono alrededor de la partícula inicial. Como los jets están compuestos de un número elevado de partículas que a su vez dejan trazas y deposiciones de energía, es necesario utilizar algoritmos especiales que permitan reagrupar a todas esas señales en su respectivo jet de forma correcta. Existen dos técnicas para la reconstrucción de los jets, denominadas \texttt{EMTopo} y \texttt{PFlow}, las cuales hacen uso de distinta información del detector para llevarla a cabo. 

La reconstrucción de los \texttt{EMTopo} jets comienza a partir de los depósitos de energía en el calorímetro generando topo-clusters de la misma forma que para electrones y fotones\footnote{En este caso los jets pueden ser calibrados tanto en la escala electromagnética como en la hadrónica (escala LCW), la cual tiene en cuenta las diferencias entre las interacciones electromagnéticas y hadrónicas en el detector ATLAS} \cite{Lampl:1099735}. Luego, los topo-clusters son combinados mediante un algoritmo denominado \textit{anti-$k_t$} \cite{Cacciari:2008gp} que realiza los siguientes pasos:



Este algoritmo tiende a unificar las partículas \textit{soft} con las \textit{hard} y separar a las partículas \textit{hard} entre sí, formando conos de radio $R$ que van a resultar útiles para determinar el solapamiento con otros objetos reconstruidos del evento. La Figura \ref{antikt} muestra esquemáticamente como el algoritmo \textit{anti-$k_t$} tiende a agrupar los distintos topo-clusters. Jets provenientes de quarks o gluones son llamados en general \textit{small-R} jets y se utiliza un $R=0.4$ para su reconstrucción. En cambio, los jets que representan partículas masivas decayendo hadrónicamente son llamados \textit{large-R} y utilizan un $R=1$. Esto ayuda a incluir en el cono la mayoría de las partículas producto de su decaimiento, con la desventaja de ser muy sensible al pile-up que modifica la geometría del mismo.



A continuación los jets pasan por una serie de correcciones y calibraciones antes de reconstruir el objeto final para los análisis. Primero se remueve la contribución por pile-up, en el caso de los \textit{large-R} jets afecta principalmente a las distribuciones angulares que son necesarias para la reconstrucción de su masa invariante, y se remueve utilizando una técnica denominada \textit{grooming} descripta en la Referencia \cite{Krohn_2010}. Para los \textit{small-R} jets primero se realiza una corrección del origen de su vértice y luego se suprime la contribución por pile-up utilizando métodos que tienen en cuenta la densidad de energía de pile-up \cite{PERF-2016-04} junto con variables asociadas a las trazas y al vértice primario \cite{PERF-2014-03}.
A continuación se calibra la energía del jet utilizando simulaciones de MC.
Esto es necesario debido a que gran parte del jet es invisible al detector, por ejemplo cuando el jet se encuentra en las zonas del mismo donde la sensibilidad es baja. 
Adicionalmente, las lluvias hadrónicas producto de las interacciones fuertes producen generalmente la creación de piones. Esto se lleva a cabo mediante la división de los núcleos, la cual requiere una energía de ligadura que escapa a la detección. Para todo ello se tiene en cuenta
la escala de energía del jet (\textit{Jet Energy Scale}, JES) \cite{JETM-2018-05} obtenida a partir de simulaciones de MC. Se calcula entonces un factor de respuesta en \textit{bines} de $|\eta|$ y \pt, que al aplicarlo a los datos corrige la energía de los jets. Para los \textit{large-R} jets se aplica a su vez una corrección similar en la masa necesaria para la correcta reconstrucción de su masa invariante. Los \textit{small-R} jets por su parte pasan por una calibración (\textit{Global Sequential Calibration}, GSC) que mejoran la resolución de energía del jet (\textit{Jet Energy Resolution}, JER). Finalmente se realiza una corrección basada en datos y aplicada exclusivamente a los mismos.
Para reducir el número de jets con una fracción de energía considerable proveniente del pile-up, se hace uso del algoritmo \textit{Jet Vertex Tagger} (JVT) \cite{ATLAS-CONF-2014-018}. El mismo reconstruye un discriminate multivariable, que combina entre otras cosas la \textit{Jet Vertex Fraction} (JVF, fracción de momento entre las trazas asociadas a un jet que provienen de un vértice, y la totalidad de las trazas) y el número de vertices primarios del evento ($N_{Vtx}$). En particular, jets con una gran componente de trazas provenientes del pile-up tienen menos probabilidad de pasar los requisitos del JVT. 





La técnica de \texttt{PFlow} jets \cite{PERF-2015-09} combina las medidas en el detector de trazas y el calorímetro para agruparlas en la detección de una sola partícula. Para ellos hace uso de que la resolución en energía del detector interno es mejor que la del calorímetro, sumado a que el umbral de energía mínima para la reconstrucción de una traza (\magn{400{MeV}}), es menor que la requerida para la de los topo-clusters, incrementando así la aceptancia del detector. El algoritmo entonces remueve durante la reconstrucción del jet la energía en el calorímetro proveniente de hadrones cargados, y usa en cambio el momento medido por el detector interno. Esto mejora la eficacia de la medida de los hadrones cargados, reteniendo solo las medidas calorimétricas de partículas neutras. Inclusive con este método, es posible restar las trazas provenientes del pile-up, sabiendo que las mismas no provienen del mismo vértice. A altas energías, la resolución del calorímetro supera al del detector interno, y la reconstrucción se realiza combinando ambas medidas. La región $|\eta|>2.5$ pierde la cobertura del detector interno, y solo topo-clusters son empleados para la reconstrucción de jets. El método de \texttt{PFlow} jets ha mostrado mejoras en la reconstrucción de jets \cite{PERF-2015-09}, principalmente para aquellos con bajo \pt, y en la reconstrucción de \met \cite{ATLAS-CONF-2018-023}, con mejoras al término \textit{soft} del mismo.




Los decaimientos de los hadrones pesados están gobernados principalmente por el hadrón más pesado en la cascada del decaimiento. Un hadrón $b$ generalmente decae a través de una cascada a un hadrón $c$, que a su vez decae a un hadrón $s$, etc. Esto genera la existencia de múltiples 
vértices, que junto con la información de las trazas y la elevada vida media de los hadrones $b$, son utilizados por distintos algoritmos para poder distinguir los hadrones $b$ de hadrones con sabores más livianos (\textit{b-tagging}). Algunos ejemplos de algoritmos \cite{FTAG-2018-01} son el \texttt{MV2} basado en un \textit{boosted decision tree} y compuesto de clasificadores de bajo nivel, y el \texttt{DL1r} \cite{FTAG-2018-01, ATL-PHYS-PUB-2020-009} basado en una red neuronal profunda que integra múltiples clasificadores de bajo nivel. Para cada algoritmo se definen WPs con distintas eficiencias de selección, que a mayor eficiencia mayor es la probabilidad de identificar otros tipos de jets erróneamente como $b$-jets. Con el WP de 77


El momento transverso faltante es una variable que se utiliza como análogo al momento de las partículas que prácticamente no interactúan con el detector, por ejemplo neutrinos o partículas más allá del SM, ya que estas naturalmente no pueden ser medidas por ninguno de los subdetectores. El momento en la dirección del haz que acarrea cada partón previo a la colisión es desconocido, pero en la dirección transversa al haz se puede considerar que es nulo. Por conservación del momento se puede deducir que luego de la colisión la suma de los momentos en el plano transverso de todas las partículas producidas debería ser nulo, y en caso de no serlo puede ser un indicio de una partícula que escapó la detección. La reconstrucción del momento transverso faltante se basa en esta conservación y se define como menos la suma de los momentos transversos de todas las partículas observadas en el evento. En esta suma se incluyen los electrones, muones, fotones, taus decayendo hadrónicamente y jets reconstruidos con los métodos descriptos en las secciones anteriores. Además se incluye un término (\textit{soft}) que tiene en cuenta el momento en la traza de las partículas que dejaron señal en el ID pero que no llegaron a reconstruirse. Quedando la definición del vector momento transverso faltante como \cite{PERF-2016-07}:


En general no se utilizan las componentes de este vector sino que se utiliza su módulo (\met) y su ángulo ($\phi^{\text{miss}}$), y cuando se menciona al momento transverso faltante se está haciendo referencia a su módulo. Cabe aclarar que esta definición introduce un sesgo a tener \met no nula en eventos donde no se produjo ninguna partícula no interactuante, debido a la incorrecta o insuficiente reconstrucción de todos los objetos presentes en el evento. Este tipo de \met se denomina instrumental, y tiene un rol importante en los procesos de fondo descriptos en el presente trabajo.

Como la reconstrucción se realiza de forma independiente para cada objeto, puede ocurrir que dos objetos distintos compartan algunas celdas calorimétricas. Para evitar el doble conteo, se define el siguiente orden de prioridad: electrones, fotones, taus y jets \cite{PERF-2011-07, PERF-2014-04}. Si alguna de estas partículas comparte celdas con otra de una prioridad mayor, la misma se elimina del cálculo de \met. Los muones son principalmente reconstruidos en el ID y el MS, por lo que el solapamiento con las demás partículas es mínimo y salvo algunos casos particulares ninguno es descartado. Muones no aislados que se solapan con los jets, jets que se solapan mínimamente con otros objetos, o jets reconstruidos a partir de un depósito de energía de muones o de pile-up tienen un tratamiento especial descripto en la Referencia \cite{PERF-2016-07}. En el término \textit{soft} se incluyen solamente aquellas trazas provenientes del vértice principal que no estén asociadas las partículas anteriormente seleccionadas. Los depósitos de partículas neutras \textit{soft} no se incluyen en este término ya que en su mayoría son producto del pile-up y su inclusión reduce el desempeño en la reconstrucción de \met. 

Para que los objetos sean considerados en el cálculo de \met deben cumplir diferentes requisitos mínimos, que son agrupados en una serie de WPs, dependiendo también si se emplearon \texttt{EMTopo} o \texttt{PFlow} jets. Algunas de las selecciones comunes se resumen a continuación:



Una estrategia para llevar a cabo una búsqueda general de nueva física, consiste a grandes rasgos en realizar un experimento de conteo de eventos con características asociadas al modelo de estudio, y su comparación con las predicciones que el Modelo Estándar hace de eventos con las mismas características. En caso de que haya un <<buen acuerdo>> entre las predicciones del SM y los datos observados, es posible afirmar que bajo las condiciones del experimento no hay evidencia de nuevos procesos físicos y que las predicciones del SM son correctas. 
Por otro lado, si se observa una discrepancia significativa (o <<exceso>>) entre las predicciones del SM y
los datos observados, se puede afirmar que el SM tiene una carencia en sus predicciones y que se podría estar en presencia de un nuevo fenómeno físico. Los criterios para definir <<buen acuerdo>> y <<exceso>> requieren evaluaciones estadísticas rigurosas que se explican en el presente Capítulo. 


En el contexto de esta Tesis se denomina <<señal>> a los procesos del modelo teórico que motivan dicha búsqueda, y <<fondo>> a las predicciones del SM. Para poder identificar los eventos de señal es necesario conocer las características del mismo, y luego así, discriminarlos de otros procesos físicos presentes en el experimento. Se utilizan simulaciones de Monte Carlo para modelar la señal, reconstruyendo los observables cinemáticos que caracterizan a los eventos. Aplicando diferentes cortes en esas variables se puede favorecer ciertos procesos y desfavorecer otros, y el conjunto de dichos cortes define una <<región>> en el espacio de observables. Las regiones donde la señal abunda con respecto al fondo, y por ende, donde se esperaría observar un exceso significativo en los datos, se denominan \textbf{Regiones de Señal (SR)}.

En este tipo de experimentos es fundamental un correcto modelado de los procesos de fondo,  para los cuales existen diferentes técnicas. Pueden ser basadas exclusivamente en datos, exclusivamente en simulaciones de Monte Carlo, o basadas en simulaciones y  luego corregidas con los datos. La corrección de las simulaciones con datos, se debe a que las mismas en general son validadas en regiones asociadas al proceso que modelan (SM por ejemplo), y como en este caso es necesario utilizarlas en regiones de señal alejadas o más extremas con respecto a donde fueron validadas, es esperable que esas predicciones en estas regiones no tengan la precisión requerida para realizar una búsqueda de nueva física. Para ello se definen \textbf{Regiones de Control (CR)} donde abundan eventos de algún proceso de fondo de interés, dedicadas a normalizar las simulaciones de ese proceso en particular a los datos observados en la misma, y luego poder extrapolar dicha normalización a las SRs.

Finalmente se definen \textbf{Regiones de Validación (VR)} que justamente se utilizan para validar la estimación de los fondos anteriormente mencionados, junto con la técnica de normalización y extrapolación de las CRs. Es importante destacar que el diseño de todas las regiones se realiza sin utilizar en ningún momento los datos en las SRs, lo que se denomina \textit{blinding}, para evitar así todo tipo de sesgo en el resultado del experimento. Por este motivo, como las CRs y VRs hacen uso de los datos, el diseño de las mismas debe ser ortogonal a las SRs, de tal forma que ningún dato de las SRs sea observado. A su vez, se busca que las CRs y VRs estén libres de contaminación de señal, ya que en caso de estar observándose dicha señal en los datos, la normalización y validación estaría sesgada.
Una vez que la estimación de los fondos son corroboradas mediante su respectiva validación en las distintas VRs, se procede a observar los datos en las SRs (\textit{unblinding}).

Complementario a las regiones del análisis, se define el modelo estadístico que evalúa los resultados obtenidos. El mismo es un concepto central en cualquier conclusión estadística, para la cual asigna una probabilidad a cada resultado posible del mismo. Un ejemplo muy utilizado en física de partículas, es el \textbf{Modelo de Poisson}, que describe el resultado de un experimento de conteo. Su distribución es de la forma:


que define la probabilidad de observar $N$ veces cierto proceso aleatorio, medido en un intervalo fijo de tiempo, donde $\mu$ es el número medio de eventos esperado. La distribución de Poisson es utilizada para describir múltiples fenómenos, como decaimientos radiactivos o cualquier experimento de partículas que conste de contar eventos en un intervalo de tiempo. Es importante mencionar que las probabilidades obtenidas en esta distribución dependen estrictamente del modelo asumido como hipótesis, en este caso representado por el número medio de eventos esperados ($\mu$). De tal forma, que la probabilidad de obtener dicho número observado de eventos en el experimento, va a depender del modelo a estudiar, los cuales por ejemplo pueden ser un modelo que sólo espera fondo, o un modelo que considera la composición de fondo y señal. 

Se define adicionalmente al \textbf{\textit{Likelihood}}, como la probabilidad de ocurrencia de los datos observados ($n$) bajo la hipótesis bajo estudio, y en el caso más simple toma la forma de la distribución de Poisson:



En el marco de esta Tesis se emplearon un conjunto de regiones de señal, las cuales pueden ser consideradas como experimentos de conteo independientes con distribuciones de Poisson, cuyas predicciones se obtienen a partir de simulaciones o técnicas basadas en datos. 
Es posible a su vez, realizar la búsqueda empleando distribuciones en alguna variable, las cuales requieren construir el modelo probabilístico que permita realizar predicciones de la misma. Si bien muchas distribuciones pueden ser derivadas de la teoría analíticamente, en general se utilizan simulaciones para generarlas. Esas simulaciones se describen mediante histogramas de la variable observada, y cada clase puede ser considerada como un experimento de conteo independiente (o región de señal) con una distribución de Poisson. 
Para ambos métodos el likelihood se escribe como el producto de las probabilidades de cada experimento \cite{cowan_book}:


 donde $n_i$ es el número de eventos observados en cada SR. 
El likelihood puede ser utilizado adicionalmente para estimar parámetros de la teoría que estamos estudiando. Por ejemplo, si nuestro modelo está caracterizado por un conjunto de parámetros $\bm{\theta}$, y asumimos que el mismo es verdadero, se esperaría que la probabilidad de observar dichos datos para ese modelo sea máxima cuando los parámetros $\bm{\theta}$ sean lo más próximo a los valores reales del modelo. El \textbf{Estimador de Likelihood Máximo} (MLE, $\hat{\bm{\theta}}$) de un dado parámetro $\bm{\theta}$, son aquellos valores del mismo que maximizan la función likelihood. En general, para facilitar a los algoritmos computacionales de maximización, se busca en cambio el mínimo de menos el logaritmo del likelihood (LLH):

Los estimadores tienen un sesgo proporcional a la inversa del número de mediciones independientes, que en este caso es el número de regiones de señal. Cuando se tiende el número de mediciones a infinito el mismo se vuelve consistente, por lo que el valor estimado de cada parámetro converge al valor verdadero ($\bm{\theta}_0$). En dicho límite, el valor esperado del estimador coincide con $\bm{\theta}_0$ (no sesgado), y adquiere su menor varianza (eficiente) \cite{cowan_book}.

Como se mencionó anteriormente la búsqueda está caracterizada por el conjunto de parámetros del modelo empleado, que se engloban en lo que se denomina <<hipótesis>>. Con el objetivo de descubrir procesos de nueva señal, se define la \textbf{Hipótesis Nula} ($H_0$), la cual se asume como verdadera y va a estar sujeta a prueba, evaluándola contra la \textbf{Hipótesis Alternativa} ($H_1$). En el contexto de esta búsqueda, la hipótesis nula asume que no hay eventos de señal y que todo lo observado debería ser fondo, en lo que se denomina hipótesis de <<solo fondo>>. A diferencia de la alternativa, que sí predice eventos de señal, denominada hipótesis de <<señal+fondo>>. Si los resultados observados en el experimento difieren de los esperados bajo la hipótesis nula, es posible rechazar a la misma, y estar en presencia de un descubrimiento. Caso contrario de no poder rechazarla, es posible poner límites al modelo, donde los roles de las hipótesis se invierten, y ahora la hipótesis nula incluye a la señal y la alternativa solo al fondo.

Las hipótesis pueden estar completamente determinadas o estar caracterizadas por distintos parámetros, y las mismas definen a las funciones de densidad de probabilidad (PDFs\footnote{No confundir con las Funciones de Distribución Partónica, definidas en la Sección \ref{sec:qcd_pp}}) de los distintos observables. Para poder discriminar una hipótesis de otra, se define un estadístico de prueba que es una función de los observables, $t(\textbf{n})$. El mismo tendrá asociada una PDF dependiendo de la hipótesis empleada ($g(t|H)$), que al aplicarle un corte en un dado valor ($t_c$), define una región critica en el espacio de observables. Si el valor de $t$ evaluado en los datos observados ($t_{\text{obs}}$) se encuentra dentro de esa región, la hipótesis nula es rechazada. La Figura \ref{fig:nullh} muestra un esquema de lo anteriormente mencionado, en la que se puede observar la región pintada de azul ($\alpha$), denominada \textbf{Error de Tipo I}, que representa la probabilidad de rechazar $H_0$ siendo esta es verdadera. En cambio, el área roja ($\beta$), denominada \textbf{Error de Tipo II}, representa la probabilidad de aceptar $H_0$ siendo esta falsa. En un caso ideal, el estadístico de prueba obtendría los valores más pequeños posibles para $\alpha$ y $\beta$.



Alternativamente, se puede cuantizar el acuerdo entre el resultado de dicha búsqueda y una hipótesis dada, calculando el \textbf{\textit{p-value}}. El mismo se define como la probabilidad bajo dicha hipótesis, de obtener un resultado igual o menos incompatible con las predicciones de la hipótesis:



Un p-value chico implica una evidencia importante en contra de dicha hipótesis, y la misma se excluye si el p-value observado es menor a un cierto valor previamente definido.  Alternativamente se puede convertir el p-value en la significancia equivalente, $Z$. La misma se define como el número de desviaciones estándar ($\sigma$) que se debe encontrar por encima de su media, una variable con distribución Gaussiana para tener una probabilidad superior igual al p-value:


donde $\Phi^{-1}$ es la inversa de la cumulativa (cuantil) de la distribución Gaussiana. 



El lema de Neyman-Pearson \cite{10.2307/91247} establece que el estadístico de prueba con mayor poder en la contrastación de hipótesis de <<solo fondo>> frente a hipótesis de <<señal+fondo>> (y viceversa), es el cociente de likelihoods:



En este caso la mejor región crítica son aquellos \textbf{n} que satisfacen $t(\textbf{n})>c_\alpha$, donde $c_\alpha$ es una constante que se ajusta para que el tamaño de dicha muestra sea $\alpha$.

El procedimiento común entonces para establecer un descubrimiento en física de partículas, se basa en un experimento frecuentista de significancia, empleando el cociente de likelihoods como estadístico de prueba. Como se mencionó anteriormente, los modelos bajo estudio están caracterizados por un conjunto de parámetros, entre los que se encuentran los <<parámetros de interés>> ($\mu$) y los <<parámetros \textit{nuisance}>> ($\bm{\theta}$). 
En el contexto de una búsqueda de nueva física, el parámetro $\mu$ representa la intensidad de la señal, de tal forma que la hipótesis de <<solo fondo>> corresponde a $\mu = 0$, y la hipótesis de <<señal+fondo>> a $\mu = 1$. En cada región del análisis uno esperaría tener un una media de eventos dada por: $\langle n_i \rangle = \mu s_i + b_i$, donde $s_i$ ($b_i$) es el número esperado de eventos de señal (fondo) en la región $i$. Por otro lado, los parámetros nuisance representan las incertezas sistemáticas, provenientes de defectos en la reconstrucción de los objetos, del modelado del detector o de la teoría. Para reducir el impacto de los mismos, se los incluye como parámetros a ajustar, con la consecuencia de reducir la sensibilidad del experimento.

En el caso donde hay un único parámetro de interés $\mu$, y el resto de parámetros son nuisance $\bm{\theta}$, es conveniente definir el \textit{Profile Likelihood Ratio} (PLR) \cite{Cowan:2010js}:



donde en el denominador, los valores $\hat{\mu}$ y $\hat{\bm{\theta}}$ son los MLE de dichos parámetros. De la misma forma en el numerador, los
parámetros $\doublehat{\bm{\theta}}$ son los valores que maximizan la función likelihood, pero para un valor fijo de $\mu$. Este proceso de elegir valores específicos de los parámetros nuisance para un valor dado de $\mu$, se lo conoce como \textit{profiling}. El PLR depende explícitamente de $\mu$, pero es independiente de los parámetros nuisance que han sido <<cancelados>>
vía el profiling. La presencia de los parámetros nuisance, que son ajustados a los datos, ensanchan la función likelihood como función de $\mu$ respecto a la distribución que tendría si sus valores estuvieran fijos, y reflejan una pérdida de información sobre $\mu$ debido a estos parámetros desconocidos.


De la definición de $\lambda(\mu)$ se puede observar que la misma puede tomar valores solamente entre 0 y 1, donde 1 implica un buen acuerdo entre los datos y el valor hipotetizado de $\mu$. De forma equivalente es conveniente usar el estadístico de prueba:


donde ahora valores grandes de $t_{\mu}$ implica una gran incompatibilidad entre datos y $\mu$.


En muchos análisis, la contribución del proceso de señal al valor medio de eventos, se asume como no negativo, lo que implica que cualquier estimador de 
$\mu$ debería ser no negativo. Aún si no fuese así el caso, es conveniente definir un estimador efectivo $\hat{\mu}$ que maximice el likelihood y que tenga la posibilidad de tomar valores negativos (siempre y cuando los valores medios de Poisson, $\mu s_i + b_i$, sean no negativos). Esto va a permitir más adelante modelar a $\hat{\mu}$ como una variable con distribución Gaussiana. 

Para un modelo con $\mu\ge0$, si se encuentra que su estimador es negativo ($\hat{\mu}<0$), entonces el mejor nivel de acuerdo entre datos y cualquier valor físico de $\mu$ va a ser cuando $\mu=0$. Por lo que se puede redefinir al PLR ($\tilde{\lambda}$) para generar un test estadístico alternativo que tenga en cuenta esto:


Un caso especial de este estadístico de prueba es cuando se analiza la hipótesis de <<solo fondo>> ($\mu=0$), ya que el rechazo de la misma puede llevar al descubrimiento de nueva señal:

Si los datos observados resultan menores a las predicciones del fondo, se tiene $\hat{\mu}<0$. Esto podría significar una evidencia en contra de la hipótesis de <<solo fondo>>, pero en realidad no muestra que los datos estén compuestos de eventos de señal. Con esta definición entonces, la posibilidad de descartar la hipótesis de <<solo fondo>> ocurre solo cuando $\hat{\mu}\ge0$, y en caso contrario $q_{0}=0$. El p-value para este estadístico de prueba queda entonces:


La comunidad de física de partículas define un rechazo de hipótesis de <<solo fondo>> con una significancia superior a los $5\sigma$ ($p=2.87 \cdot 10^{-7}$) como un nivel apropiado para definir un descubrimiento. Para excluir hipótesis de señal se define en cambio a partir de $1.64$ sigmas ($p=0.05$). Cabe destacar que el rechazar la hipótesis de solo fondo es solo parte del proceso de descubrimiento de un nuevo fenómeno. La certeza de que un nuevo proceso está presente va a depender en general de otros factores, como la plausibilidad de una nueva hipótesis de señal, y el grado al cual la misma describe a los datos observados.


Cuando el p-value obtenido es mayor al límite definido para un descubrimiento, no es posible rechazar la hipótesis de <<solo fondo>>, y en ese caso se desea establecer límites sobre el modelo caracterizado. Para ello, se busca en cambio, rechazar la hipótesis <<señal+fondo>>, y encontrar el valor superior de $\mu$ para el cual no es posible realizar dicho rechazo (límite superior). Se define entonces un nuevo estadístico de prueba, donde los roles de la hipótesis de <<solo fondo>> de <<señal+fondo>> son intercambiados:



La razón para poner $\tilde{q}_{\mu} = 0$ para $\hat{\mu}>\mu$ es que cuando se establece un límite superior, el hecho de que $\hat{\mu}>\mu$ representa menos compatibilidad con $\mu$ que los datos obtenidos, y por lo tanto no se considera parte de la región de rechazo de la contrastación. Cabe destacar que el $\tilde{q}_0$ anteriormente definido, no es un caso particular de este estadístico de prueba, ya que $q_0$ es cero si los datos fluctúan hacia abajo ($\hat{\mu}<0$), pero $\tilde{q}_{\mu}$ es cero si los datos fluctúan hacia arriba ($\hat{\mu}>\mu$).

Con este estadístico de prueba se busca encontrar el valor de $\mu$ para el cual deja de ser posible el rechazo de la hipótesis <<señal+fondo>> (o viceversa, hasta que valor de $\mu$ es posible un rechazo de la hipótesis). Para ello se definen el nivel de confianza \cite{Read:2002hq}:


 
donde


siendo $f(q_\mu|\mu)$ la PDF del estadístico de prueba $q_\mu$, y $f(q_\mu|0)$ la PDF bajo la hipótesis de <<solo fondo>>. Cuanto más bajo es el $\text{CL}_{s}$, menos compatibilidad entre los datos y la hipótesis de <<señal+fondo>>. Se define por convención al límite superior ($\mu_{\text{up}}$) como aquel $\mu$ que tiene un $\text{CL}_{s}=5\%$, y se rechazan entonces lo modelos con $\mu$ menores a $\mu_{\text{up}}$. La ventaja de usar $\text{CL}_{s}$ en vez de directamente $\text{CL}_{s+b}$, radica en que este último puede excluir hipótesis de <<señal+fondo>> cuando es semejante a la de <<solo fondo>>, y el número de eventos observados es mucho menor que el de fondo. Sin embargo, esto último no es el objetivo del análisis, ya que en la práctica no se pretende excluir hipótesis de las cuales no se tiene sensibilidad alguna. 





Para hallar el p-value de una hipótesis es necesaria la función densidad de probabilidad del estadístico de prueba. Por ejemplo, para rechazar la hipótesis nula se calcula el p-value, que depende de $f(q_{0}|0)$ como muestra la Ecuación \ref{ec:pvalue_0}. Para poner límites superiores al modelo se necesita $f(q_{\mu}|\mu)$ y $f(q_{\mu}|0)$, como se ve en la Ecuación \ref{ec:pvalue_mu}. Además, se requiere de $f(q_{\mu}|\mu')$ con $\mu\neq\mu'$, para hallar la significancia esperada, empleada en la evaluación a priori de la sensibilidad del análisis (Sección \ref{sec:exp_sig}). En un principio dichas PDFs son desconocidas analíticamente, pero es posible obtenerlas empleando métodos alternativos.
Considerando al PLR de la Ecuación \ref{ec:plr}, determinado por el parámetro $\mu$, que puede ser cero (descubrimiento), o no (límites superiores), y suponiendo que los datos se distribuyen de acuerdo a un parámetro $\mu'$, la distribución $f(q_{\mu}|\mu')$ se puede aproximar utilizando el teorema de Wald \cite{10.2307/1990256}, que muestra que para el caso de un solo parámetro de interés:


donde $\mu$ sigue una distribución Gaussiana con una media $\mu'$ y una desviación estándar $\sigma$, y $N$ representa el tamaño de la muestra. 
En el límite asintótico se puede mostrar que el estadístico de prueba $t_{\mu}$ sigue una distribución de $\chi^{2}$ no central con un grado de libertad, y en ese caso el estadístico de prueba para descubrimiento ($q_{0}$) de la Ecuación \ref{eq:st_q0} puede aproximarse como:



De la misma forma, a su respectivo p-value se lo puede aproximar como $p_{0}=1-\Phi(\sqrt{q_{0}})$ y a la significancia equivalente como $Z_{0}=\sqrt{q_{0}}$. La misma aproximación vale para el estadístico de prueba para límites superiores ($\tilde{q}_{\mu}$) de la Ecuación \ref{ec:st_qmu}, cuya expresión se describe en la Referencia \cite{Cowan:2010js}.

Cuando la estadística es reducida ($\mathcal{O}(10)$), se abandona el régimen asintótico, y las aproximaciones anteriores dejan de tener validez. En ese caso el estadístico de prueba se obtiene a partir de lo que se denominan \textit{toys} o <<psuedo-experimentos>> \cite{Baak:2014wma}. Para ello, se varían los parámetros $\mu_\text{sig}$ y $\bm{\theta}^0$, generando una estadística suficiente para tener una estimación conservadora del p-value, y por ejemplo, si se obtiene un $\text{CL}_{s}>5\%$ para alguna de esas variaciones, no es posible excluir a la hipótesis empleada. Este método puede resultar poco práctico, sobre todo cuando hay un gran número de sistemáticos, por lo que alternativamente se puede realizar una estimación previa de los parámetros $\bm{\theta}^0$ que maximizan el p-value. Para ello se ajustan dichos parámetros a los datos observados, para un dado valor de $\mu_\text{sig}$. Los valores obtenidos para $\bm{\theta}^0$ mediante este ajuste son luego empleados para las siguientes iteraciones que permiten calcular el p-value. El hecho de emplear datos al realizar los toys, genera una dependencia residual en el cálculo de algunos límites, obtenidos solamente a partir de las estimaciones de fondo esperadas.
 




 



El diseño de las regiones de señal para el análisis es un proceso denominado <<optimización>>, que define el conjunto de cortes óptimo para discriminar el fondo de la señal. Tal discriminación es cuantizada mediante la significancia de la Ecuación \ref{ec:sign}, la cual a priori es desconocida. Para estimar el valor de significancia ($Z$) que uno esperaría tener en un dado experimento, con un número de eventos igual a la suma de las estimaciones de fondo y señal ($n=s+b$), se puede emplear la significancia esperada, que se define como la mediana de $Z$. Como el p-value y $Z$ tienen una relación lineal y monotónica \cite{Cowan:2010js}, la mediana de $Z$ se puede obtener a partir de la mediana del p-value.


Por ejemplo, si tenemos una variable $n$ que sigue una distribución de Poisson y tiene media $s+b$, si la media es lo suficientemente grande, es posible aproximar a la misma mediante una distribución Gaussiana, con media $s+b$ y desviación estándar $\sqrt{s+b}$. El p-value para la hipótesis de <<solo fondo>> ($s=0$) dada una observación $n$ es:



Lo que permite obtener la significancia para descubrimiento:


La media de $Z$ coincide con la mediana, y como la media de $n$ es $s+b$, se obtiene:


Esta magnitud fue históricamente empleada en física de partículas para la estimación de la significancia esperada. La misma se puede interpretar como la fracción de eventos esperados de señal, con respecto a la incerteza estadística del número esperado de eventos total, asumiendo la ausencia de señal.

Si el número esperado de eventos de fondo $b$ es desconocido, debe ser incluido como parámetro nuisance. En ese caso $b$ puede ser ajustado libremente al número de eventos observados, y sería imposible rechazar la hipótesis de <<solo fondo>> ($s=0$), a menos que se introduzca información adicional que limite dicho parámetro. Para ello, se incluyen las regiones de control, donde no hay eventos de señal y donde la estimación del fondo en esta región puede relacionarse con la estimación en las regiones de señal. Estas regiones son incluidas a la función likelihood como distribuciones de Poisson, de igual forma que las regiones de señal. Procediendo de forma similar que en el ejemplo anterior, empleando la hipótesis de <<solo fondo>> ($s=0$) y la aproximación mediante <<datos de Asimov>> \cite{Cowan:2010js}, se puede obtener una expresión para la significancia esperada en función de la estimación del fondo, basada en la región de control y su incerteza ($\sigma_b$):



La estimación de la significancia esperada depende también del tipo de experimento que se desea realizar. En la práctica se emplea un método descripto en las Referencias \cite{Linnemann:2003vw, stat_1, ATL-PHYS-PUB-2020-025}, y que se engloba en una función del framework \texttt{ROOT}\footnote{\texttt{ROOT.RooStats.NumberCountingUtils.BinomialExpZ}}, que depende de la estimación de la señal y fondo, junto con su incerteza. 




La función de likelihood empleada para esta tesis se escribe como:




La misma se compone de la distribución de los datos observados en cada SR y CR, y un factor adicional que engloba las incertezas sistemáticas. En una dada región $i$, los datos observados ($n_i$) obedecen la distribución de Poisson con media $\mu_\text{sig} s_i + \bm{\mu}_{\text{bkg}} \cdot \textbf{b}_i$, donde $\mu_\text{sig}$ es la intensidad de señal, $s_i$ es la estimación de señal, $\textbf{b}_i = (b_i^{(1)}, b_i^{(2)}, ...)$ es la estimación de cada fondo, y $\bm{\mu}_{\text{bkg}} = (\mu^{(1)}, \mu^{(2)}, ...)$ son los factores de normalización de cada fondo (cuyo uso se explica más adelante). Las incertezas sistemáticas ($S$) son parametrizadas mediante $\bm{\theta}$, y son incluidas usando la PDF $C_\text{syst} (\bm{\theta}^0, \bm{\theta})$, donde $\bm{\theta}^0$ (generalmente fijadas a 0) son los valores centrales de las medidas auxiliares a partir de los cuales el parámetro $\bm{\theta}$ puede fluctuar al realizar un ajuste. Las variaciones de estos parámetros nuisance tiene un impacto directo en las estimaciones de los fondos y señal ($\textbf{b}_i$ y $s_i$). Para parámetros nuisance independientes, esta PDF es simplemente el producto de cada incerteza, la cual corresponde con una Gaussiana con media $\theta_i^0 - \theta_i$. Los términos para las SRs y CRs fueron separados explícitamente debido a que pueden ser modificados de acuerdo al tipo de ajuste que se realice. 

El estadístico de prueba empleado es el PLR descripto en la Ecuación \ref{ec:plr} a partir del likelihood de la Ecuación \ref{eq:analysis_lh}, y modificado para la evaluación del descubrimiento (Ecuación \ref{eq:st_q0}) o para los límites de exclusión (Ecuación \ref{ec:st_qmu}).

La búsqueda comienza con el diseño de las SRs, a partir de la estimación de los procesos de señal y fondo, buscando maximizar la significancia esperada descripta en la Sección \ref{sec:exp_sig}. Luego se diseñan las CRs, con el objetivo de normalizar los fondos de MC principales a los datos observados, realizando lo que se denomina el \textbf{Ajuste de solo fondo (blinded)}. Para ello se omite en el likelihood el término de las SRs y se fija $\mu_\text{sig}=0$. Con dicho ajuste es obtienen los \textbf{factores de transferencia} ($\bm{\mu}_b$), que se aplican a los respectivos fondos para corregir la estimación de los mismos a los datos en las CRs, y luego extrapolar dicha corrección a todas las regiones del análisis de la forma:



Una ventaja de emplear este método, es que las incertezas sistemáticas en las predicciones de los fondos se cancelan parcialmente en dicha extrapolación. La incerteza total en el número de fondo en cada región, es una combinación de la incerteza estadística de las CRs, y el residual de las incertezas sistemáticas. Para ello se emplea en las CR cortes más relajados, con la idea de aumentar la estadística, sin incrementar las incertezas residuales, lo cual reduce la extrapolación de las incertezas a las demás regiones. La extrapolación de las incertezas sistemáticas es un proceso complejo basado en la construcción de las PDF tanto en las CRs como en las SRs y VRs, la cual se describe en detalle en las Referencias \cite{Baak:2014wma, Cranmer:1456844}. Aquellos fondos que no son normalizados en las CRs dedicadas, simplemente emplean $\mu^{\text{(p)}}=1$. El objetivo final de este ajuste es realizar una estimación adecuada de los fondos en las regiones del análisis, principalmente validando dicho método en las VRs del análisis.

Una vez que se considera que el modelado de los fondos es el adecuado, se procede a observar los datos en las SRs (\textit{unblinding}), el cual permite evaluar si se está en presencia de un descubrimiento o no. Se realiza el ajuste del likelihood ahora con la hipótesis de <<solo fondo>> ($\mu_\text{sig}=0$), pero incluyendo también las SRs (\textbf{Ajuste de solo fondo (unblinded)}). El parámetro $\bm{\mu}_b$ es nuevamente ajustado (junto con los sistemáticos), el cual se espera que dé resultados semejantes al ajuste sin SRs. A su vez, en el denominador del estadístico de prueba, tanto $\bm{\mu}_b$ como $\mu_\text{sig}$ son parámetros ajustables, aunque la contribución de señal es desconocida. Para ello se emplea una señal \textit{dummy}, en donde $s_i=1$, y $\mu_\text{sig}$ representa directamente el número de eventos de señal estimado en cada SR, y en las CR directamente se asume que no hay contribución de señal ($s_i=0$). 
A partir del likelihood obtenido se calcula el estadístico de prueba para descubrimiento observado ($q_{0, obs}$), y con ello el p-value del mismo. Dicho valor permite  afirmar si la observación corresponde al descubrimiento de un nuevo fenómeno.

En caso de no poder confirmar un descubrimiento es posible imponer límites sobre el modelo estudiado, en lo que se denomina \textbf{Ajuste dependiente del modelo}. 
Se realiza entonces el ajuste del likelihood pero ahora empleando la hipótesis de <<señal+fondo>> ($\mu_\text{sig}=1$), y considerando la contribución de la señal tanto en las SRs como en las CRs, la cual se estima mediante simulaciones de MC. Luego se obtiene el estadístico de prueba para límites observado ($q_{\mu, obs}$) y se calcula el $\text{CL}_{s}$. Este ajuste se realiza para cada modelo de señal, que se generan variando algún parámetro (masa por ejemplo), el cual toma valores discretos. El límite se define como aquellos puntos de señal para los cuales se encuentra un $\text{CL}_{s}=5\%$. Generalmente ningún punto del modelo cumple exactamente esta condición y es necesaria realizar una interpolación entre los puntos más cercanos a ese valor de $\text{CL}_{s}$, permitiendo así obtener el límite en función del parámetro de forma continua. Existen dos tipos de límites, el observado y el esperado. El primero emplea en el likelihood los datos observados para calcular el $q_{\mu, obs}$, mientras que en el segundo la integral que se realiza para el $\text{CL}_{s}=5\%$, se realiza desde la mediana de la PDF de la hipótesis de <<solo fondo>>. Esto sería el equivalente a calcular una significancia esperada, pero invirtiendo los roles de la hipótesis <<solo fondo>> y <<señal+fondo>>.
El límite esperado se puede calcular inclusive antes de realizar el unblind, para evaluar la sensibilidad que se obtiene mediante las regiones de señal diseñadas.

Para ampliar el rango de interpretación de los resultados obtenidos en el análisis, es posible a su vez establecer límites independientes del modelo, mediante lo que se denomina \textbf{Ajuste independiente del modelo}. Para ello se establecen límites superiores al número de eventos en cada SR, de forma de poder saber si algún modelo alternativo ya está excluido por el análisis actual, simplemente estimando el número de eventos de dicho modelo en las SRs. Se realiza entonces un ajuste del likelihood, donde nuevamente no se permite contaminación de señal en las CR, y donde se emplea una señal \textit{dummy} en las SRs ($s_i=1$). Como ahora también el parámetro $\mu_\text{sig}$ es ajustable, puede ocurrir que el número de eventos esperado en las SRs se modifique con respecto a las estimaciones iniciales. El estadístico de prueba queda ahora en función de $\mu_\text{sig}$, y el límite en el número de eventos se define como aquel $\mu_\text{sig}$ cuyo $\text{CL}_{s}=5\%$. Para ello se calculan los valores de $\text{CL}_{s}$ probando distintas distribuciones del estadístico de prueba, con valores de $\mu$ discretos, y buscando aquellos que se obtenga un $\text{CL}_{s}$ cercano al 5\%. Luego se realiza una nueva evaluación más refinada empleando el valor obtenido con la iteración anterior. Nuevamente se pueden obtener límites observados y esperados, de la misma forma que se realizó para los límites dependientes del modelo.


En la Sección \ref{sec:trigger} se detalló el funcionamiento del sistema de trigger y su importancia para los distintos análisis que se realizan dentro de la colaboración. La medida precisa de la eficiencia de los triggers es empleada para tener conocimiento del rendimiento de los mismos, y poder entonces determinar la aceptancia de los análisis físicos de interés que involucran cada uno de los triggers utilizados. 
En este Capítulo se discute en particular la medida de la eficiencia de los triggers de fotones, que son de especial importancia para esta tesis, y a su vez se detalla el cálculo de los factores de escala asociados a dichas eficiencia . El método empleado se basa en una muestra de datos con fotones de alta pureza seleccionados a partir de eventos con bosones $Z$ que decaen radiativamente. Este método se utiliza para la medida de la eficiencia de triggers con fotones de bajo \ET, debido a la baja estadística de la muestra cuando los fotones tienen $\ET>60\,\gev$. Complementariamente, para triggers con fotones de mayor \ET, se utiliza otro método denominado \textit{Bootstrap} \cite{tesis_joaco}, que tiene una mayor estadística en ese rango a costa de una menor pureza, el cual no es descripto en esta Tesis.

La medida de la eficiencia de los triggers de fotones y el cálculo de sus factores de escala fueron hechos bajo el marco de la \textit{ATLAS Qualification}, un trabajo obligatorio para nuevos ingresantes, y requisito para ser miembro de la colaboración. Tales resultados son empleados por todos los análisis que usan los triggers de fotones primarios para la selección de eventos.



La reconstrucción de fotones \cite{TRIG-2018-05} (y de forma similar la de electrones) en el trigger comienza en el L1 con la construcción de regiones de interés (RoIs) utilizando sólo la información del calorímetro. A partir de esas RoIs el HLT ejecuta algoritmos de reconstrucción rápida que utilizan adicionalmente información del detector interno dentro de la RoI, permitiendo una selección e identificación inicial de fotones junto con un temprano rechazo de fondo. En el caso de que el candidato cumpla los requisitos de selección rápidos se ejecuta a continuación los algoritmos de precisión, que utilizan información adicional en regiones del detector fuera de la RoI. Estos algoritmos son similares a los utilizados en la reconstrucción offline con la diferencia de que no reconstruyen superclusters de fotones. Utilizando sólo la información del calorímetro es suficiente para tener una alta eficiencia por lo que los mismos son reconstruidos bajo la hipótesis de que no son convertidos, no se reconstruyen trazas ni se aplican criterios de ambigüedad con los electrones, reduciendo notablemente el consumo de CPU. 
A continuación se detallan los mecanismos realizados en ambas etapas del trigger.



Los triggers del L1 utiliza la información del calorímetro en la región central ($|\eta|<2.5$) para construir las RoIs, que consisten en torres (\textit{trigger towers}) de $4\times4$ celdas del calorímetro de $0.1\times0.1$ en $\eta$ y $\phi$. Un algoritmo (\textit{sliding-window} \cite{Lampl:1099735}) busca los conjuntos de celdas de $2\times2$ cuya suma de energía transversa de uno de los cuatro posibles pares de celdas vecinas más cercanas ($1\times2$ o $2\times1$) supere el umbral de energía que define al trigger. La Figura \ref{fig:l1_reco} muestra un esquema de una torre de trigger que emplea dicho algoritmo, mientras que en la Figura \ref{fig:ecal} se detalla la región del ECAL que representa una torre del trigger.


Este umbral puede depender de $\eta$ con una granularidad de $0.1$, en general variando entre \magn{-2}{GeV} y \magn{3}{GeV} con respecto al umbral nominal, y en ese caso se agrega una letra `V' al final del nombre del trigger. A su vez se puede aplicar un rechazo de actividad hadrónica, donde se descarta al candidato si la suma de energía transversa de las celdas en el calorímetro hadrónico de la ventana de $2\times2$ es mayor a \magn{1}{GeV} y supera $\ET/23\,\gev-0.2\,\gev$. En ese caso se agrega una `H' al final del nombre del trigger. Finalmente se puede incluir requisitos de aislamiento que rechazan a los candidatos si la suma de la energía transversa en las 12 celdas alrededor de la ventana de $2\times2$ es mayor a \magn{2}{GeV} y supera $\ET/8\,\gev-1.8\,\gev$, agregando una `I' al nombre del trigger. Por ejemplo, el trigger \texttt{L1\_EM20VHI} tiene un umbral de \magn{20}{GeV} variable en $\eta$ y utiliza el rechazo hadrónico y la selección de aislamiento. Tanto el rechazo hadrónico como la selección de aislamiento se aplican solamente a triggers con umbral mayor a \magn{50}{GeV}.



La reconstrucción en el HLT comienza aplicando algoritmos de reconstrucción rápida para reconstruir clusters con las celdas de las RoIs obtenidas en el L1. Para acelerar el proceso estos algoritmos solo utilizan la segunda capa del calorímetro electromagnético para encontrar la celda con mayor energía transversa de la RoI (semilla). La posición del cluster se obtiene calculando el promedio de las posiciones pesadas por la energía

dentro de una ventana de $3\times7$ ($\Delta\eta\times\Delta\phi = 0.075\times0.175$) centrada en la celda semilla. Para calcular la energía acumulada 
en todas las capas del calorímetro
se utiliza una ventana de $3\times7$ ($\Delta\eta\times\Delta\phi = 0.075\times0.175$) en la región barrel y una ventana de $5\times5$ ($\Delta\eta\times\Delta\phi = 0.125\times0.125$) en el endcap. Adicionalmente se realizan correcciones basadas en los algoritmos de reconstrucción que mejoran la resolución de la posición y energía del cluster. En esta etapa se realizan selecciones solamente basadas en la energía transversa del cluster y en los parámetros $R_{\text{had}}$, $R_{\eta}$ y $E_{\text{ratio}}$.


Si el candidato pasa la selección anterior se utiliza una región levemente mayor a la RoI para ejecutar los algoritmos de precisión. Estos algoritmos son los mismos empleados en la reconstrucción offline \cite{Lampl:1099735} para construir el clusters y técnicas multivariable \cite{PERF-2017-03} para hacer correcciones en su energía. La identificación online de fotones utiliza las mismas variables discriminatorias que en la reconstrucción offline, definiendo tres WPs: \texttt{Loose}, \texttt{Medium} (empleado solamente en el HLT), y \texttt{Tight}.
Adicionalmente es posible incluir requisitos de aislamiento calorimétrico utilizando topo-clusters, de forma similar a la reconstrucción offline. Para ello se reconstruye la totalidad de los topo-clusters presentes en el evento para calcular la densidad de energía del evento en el HLT, necesaria para sustraer el ruido de la señal en el cono de aislamiento. El cono se construye con un radio de  $\Delta R < 0.2\:(0.4)$ alrededor del candidato para el punto de trabajo de aislamiento \texttt{Very-Loose} (\texttt{Tight}), denotado en el nombre del trigger como \texttt{icalovloose} (\texttt{icalotight}). Un fotón en el HLT se considera aislado si la fracción entre la energía transversa del topo-cluster y la del candidato es menor a
10



A continuación se describe la convención de nombres de triggers utilizada en el detector ATLAS.
El nivel del trigger puede ser tanto L1 como HLT. La multiplicidad representa la cantidad de objetos que pretende seleccionar el trigger con esos mismo requisitos. Los posibles tipos de objetos para los triggers de fotones pueden ser `EM' en el caso de triggers del L1 y `g' para el HLT. En el caso de triggers del L1 es posible que incluyan los requisitos `I', `H' o `V' descriptas anteriormente. Los triggers compuestos por la disyunción de otros dos trigger, incluyen ambas componentes en el nombre sucesivamente. Finalmente en los requisitos adicionales se incluye la identificación, y en caso de haber requisito de aislamiento se agrega a continuación. Opcionalmente para los HLT triggers se puede explicitar el trigger del L1 que se utilizó como semilla. En la Figura \ref{fig:trigger_name} se muestra un ejemplo de nomenclatura para el trigger \texttt{HLT\_2g20\_tight\_icalovloose\_L12EM15VHI}, el cual representa un trigger del HLT que selecciona eventos con al menos dos fotones con $\ET>20\,\gev$, ambos que pasen los requisitos de identificación \texttt{Tight} y de aislamiento \texttt{icalovloose}, y adicionalmente se especifica el seed L1 trigger que requiere de dos L1 EM clusters con un umbral dependiente en $\eta$ y centrado en \magn{15}{GeV}, con los requisitos de aislamiento y rechazo hadrónico.

Los principales triggers de fotones se detallan en la Tabla \ref{tab:prim_ph_trig}. El trigger primario de un fotón con menor umbral y sin prescale 
está diseñado para búsquedas de física nueva más allá del SM con fotones de alto \ET, mientras que el de dos fotones se utiliza principalmente para seleccionar eventos con bosones de Higgs decayendo a fotones. Los triggers de dos fotones con umbrales bajos e identificación \texttt{Tight} son empleados para estudios más allá del SM con resonancias de baja masa ($\sim60\,\gev$).



Debido al amplio conocimiento adquirido en las últimas décadas sobre las propiedades del bosón $Z$, el mismo es empleado en la actualidad para realizar medidas de calibración y eficiencia. El decaimiento radiativo del bosón $Z$ ocurre cuando uno de los productos del decaimiento leptónico irradia un fotón ($Z\to l^{+}l^{-}\gamma,\:l=e,\mu$). Este decaimiento en particular se utiliza cuando se desea obtener una muestra de fotones con una elevada pureza, debido a que al reconstruir la masa invariante de los tres objetos y requerir que sea compatible con la del bosón $Z$, la posibilidad de que el fotón haya sido erróneamente reconstruido es muy baja. Teniendo en cuenta la alta pureza de fotones de la muestra esta técnica no requiere de métodos de sustracción de fondo. La desventaja de este método es la baja estadística de eventos con estas características cuando el \pt del fotón es alto, por lo que es utilizado para medir eficiencias de triggers con umbrales menores a \magn{60}{GeV}.

En el contexto de este trabajo, la eficiencia de un determinado trigger se define como la fracción de eventos que pasaron el mismo con respecto al total de eventos presentes en la muestra:


La eficiencia se calcula en función de distintas variables como por ejemplo \ET y $\eta$ del fotón reconstruido offline, o el $\langle\mu\rangle$ del evento. En el caso de una eficiencia teórica en función del \ET la forma de la misma debería ser una función escalón de Heaviside centrada en el valor de corte de \ET del trigger. El sistema de trigger toma una decisión basada en la reconstrucción de objetos online o tiempo real, sin embargo para los análisis físicos los objetos de interés son los reconstruidos offline de mayor precisión. Es por esto que las eficiencias se evalúan con respecto a estos últimos objetos, observando entonces un desvanecimiento de la curva escalón en el valor de selección online, en la llamada región de encendido (\textit{turn-on}) del trigger en estudio. 
Los triggers y algoritmos de reconstrucción e identificación están diseñados para impedir una dependencia de la eficiencia en $\eta$ o $\langle\mu\rangle$, por lo que al expresarla en función de estas variables se espera una curva plana muy cercana a 1. En el caso de triggers compuestos, se calcula la eficiencia de cada componente y la eficiencia total resulta como el producto de ambas.



A los eventos se les solicita tener al menos dos leptones de carga opuesta y un fotón, todos con $\pt>10\,\gev$. El fotón debe estar dentro de la región $|\eta| < 2.37$ y pasar el WP de identificación \texttt{Tight}. Las eficiencias se calculan para los distintos WPs de aislamiento del fotón utilizado, en este caso para \texttt{FixedCutTightCaloOnly} y \texttt{FixedCutLoose}. Los leptones deben estar dentro de la región $|\eta| < 2.47$, pasar el WP de identificación \texttt{Medium}, el de aislamiento \texttt{FixedCutLoose} y su traza asociada tener $|z_0| < 10$ mm y $\sigma(d_0) < 10$. A su vez el evento es rechazado si el $\Delta R$ entre el fotón y alguno de los leptones es menor a 0.2. Finalmente se realiza una selección en la masa invariante de los leptones ($m_{ll}$) y la de los 3 objetos ($m_{ll\gamma}$). En la Figura \ref{mllgmll} se muestra el gráfico de $m_{ll}$ en función de $m_{ll\gamma}$. En la misma se puede observar que la mayoría de los eventos se encuentra en la región $m_{ll}\sim91\,\gev$ y $m_{ll\gamma}\gtrsim96\,\gev$, estos representan eventos en los cuales un bosón $Z$ decayó a un par de leptones, y que adicionalmente en el evento se encontraba un fotón proveniente de otro proceso. En cambio en la región $86\,\gev<m_{ll\gamma}<96\,\gev$ y $40\,\gev<m_{ll}<83\,\gev$ la masa invariante de los pares de leptones no alcanza la del bosón $Z$, pero al combinarlos con el fotón sí lo hace. Al aplicar este último corte se garantiza que un leptón necesariamente haya irradiado y que el fotón provenga del decaimiento del bosón $Z$ y no de otro proceso. En el caso de tener en el evento más de un fotón o más de dos leptones que cumplan los requisitos, se seleccionó el trío cuya masa invariante sea la más cercana a la del bosón $Z$.


La muestra de datos se obtiene a partir de eventos que pasaron los triggers primarios de electrones o muones, utilizando la derivación \texttt{EGAM3} (\texttt{EGAM4}) que preselecciona eventos con dos electrones (muones) y un fotón, con requisitos orientados a este tipo de decaimiento, descriptos en la Sección \ref{sec:lhc_samples}. El uso de triggers de leptones se debe a que si se utilizaran triggers de fotones se podría estar introduciendo un sesgo en las eficiencias.

Finalmente las eficiencias se obtienen contando el número de eventos que pasa esta selección, y que representa el denominador en la Ecuación \ref{eq:trig_eff}. Para el numerador se cuenta cuántos de esos eventos, el fotón presente en el mismo pasó la selección del trigger a evaluar. Las eficiencias fueron calculadas para los triggers listados en la Tabla \ref{tab:trigg_eff}, empleando los datos correspondientes a su año. Las mismas son calculadas en función de \ET para fotones fuera de la región crack, y de $\eta$ y $\langle\mu\rangle$ para fotones con \ET mayor a \magn{5}{GeV} del umbral del trigger. En el caso de que el trigger o una de sus componentes se haya configurado con un prescale, el mismo se emplea en modo rerun para la medida de su eficiencia. En las Figuras \ref{fig:photon_trig_eff_2015}, \ref{fig:photon_trig_eff_2016}, \ref{fig:photon_trig_eff_2017} y \ref{fig:photon_trig_eff_2018} se pueden observar algunos de dichos resultados en función de distintas variables. En general todas son cercanas a la unidad y constantes, salvo para $\langle\mu\rangle$ que se observa una pequeña dependencia algo esperable dado el incremento de colisiones por cruce de haces. Esto se ve en mayor medida en los triggers \texttt{HLT\_g20\_tight\_icalovloose\_L1EM15VHI} y \texttt{HLT\_g22\_tight\_L1EM15VHI} debido a que 
el aislamiento en el L1 no contemplaba ninguna corrección por pile-up.
En la Figura \ref{fig:bs_vs_zrad} se muestra una comparación de las eficiencias calculadas con el método Bootstrap \cite{tesis_joaco} y el método del bosón $Z$ radiativo para un mismo trigger, en la que se puede observar una pequeña mejora en la eficiencia utilizando este último método, lo que motiva a su uso para triggers de bajo \ET.



La incerteza estadística para las eficiencias se obtiene como el intervalo de confianza de un estimador de Bayes con el método de Jeffrey \cite{Casadei_2012}.
Las incertezas sistemáticas se obtienen a partir de las variaciones en las eficiencias al modificar algunas de las selecciones empleadas en el método, principalmente las asociadas a los leptones para evitar un posible sesgo en esa selección. El requisito sobre las masas invariantes se varía de $36\,\gev<m_{ll}<87\,\gev$ a $44\,\gev<m_{ll}<79\,\gev$, y de 
$82\,\gev<m_{ll\gamma}<100\,\gev$ a $88\,\gev<m_{ll\gamma}<94\,\gev$, y se modifica el requerimiento de identificación de los leptones a \texttt{Tight} y \texttt{Medium}, y el de aislamiento a \texttt{FixedCutTight}. En general las incertezas totales toman valores menores al 10\%.




Las simulaciones de Monte Carlo logran reproducir los procesos físicos en general con una alta precisión, pero naturalmente presentan imperfecciones principalmente relacionadas con la simulación de la interacción de las partículas con el material del detector. Estos efectos se traducen en eficiencias distintas (en general más altas) que las respectivas producidas en datos. Con el objetivo de corregir las simulaciones y que se asemejen lo más posible a los datos se calculan los Factores de Escala (SF), que son factores multiplicativos (pesos) aplicados luego a cada evento simulado según corresponda. Para el caso de la eficiencia del trigger de fotones, los SFs se definen como el cociente entre las eficiencias calculadas en datos y las calculadas con simulaciones:


Las eficiencias del trigger para simulaciones se calculan de la misma forma que en datos, pero utilizando simulaciones con procesos de producción de bosones $Z$ decayendo a electrones o muones, los cuales pueden irradiar un fotón. Los SFs con calculados para los mismos triggers que se calculó la eficiencia listados en la Tabla \ref{tab:trigg_eff}, en \textit{bines} de \ET y $\eta$ simultáneamente y para distintos WPs de aislamiento. Las incertezas son calculadas directamente propagando las incertezas de ambos términos del cociente. Para valores de alto \ET el valor del SF es extrapolado a partir de los \textit{bines} de bajo \ET, donde el método del bosón $Z$ radiativo tiene validez. En la región con \ET menor al umbral y en la región del crack, donde las eficiencias son prácticamente nulas, se fijan los SFs igual $1\pm1$. 
 
En las Figuras \ref{fig:SFs_2015_2016} y \ref{fig:SFs_2017_2018} se observan algunos de los SFs calculados para triggers de distintos años con WPs de aislamiento \texttt{FixedCutTightCaloOnly}. 
En todos los casos los SFs son muy cercanos a la unidad, lo que deja en evidencia el buen diseño y construcción del sistema de trigger y las simulaciones. Tanto las eficiencias como los factores de escala obtenidos en este trabajo son utilizados actualmente por todos los análisis de la colaboración ATLAS que utilicen una selección de datos con triggers de fotones del Run 2.






El trabajo realizado en esta Tesis se centra en la búsqueda de Supersimetría en eventos con al menos un fotón energético y aislado, jets y gran cantidad de energía faltante en el estado final. La estrategia general de la búsqueda consiste en el conteo del número de eventos observados, y su comparación con las predicciones del SM, en regiones del espacio de observables ricas en eventos de la señal, siguiendo los métodos descriptos en el Capítulo \ref{cap:statistical}. El objetivo es poder discriminar en los datos observados, aquellos que podrían ser producto de un proceso supersimétrico (señal), de aquellos provenientes de procesos del SM (fondo).



El modelo supersimétrico que motiva a la presente búsqueda consiste en un modelo GGM, y por lo tanto el gravitino es la LSP, cuya masa es del orden de unos pocos eV. La NLSP, en este caso, es el neutralino más liviano, que consiste en un estado de gauge mezcla de bino y higgsino, lo que le permite acoplar a fotones, bosones $Z$ y bosones de Higgs.
Análisis anteriores se centraron en los casos donde el neutralino acopla solamente a fotones y bosones $Z$ \cite{Alonso:2147473, tesis_joaco, Collaboration:2198651, tesis_tony}, o solamente a fotones \cite{Jinnouchi:2233741}. El trabajo realizado en esta Tesis se centra en el caso fenomenológico en el cual el neutralino más liviano decae en proporciones iguales a $\gamma+\gravino$ y a $h+\gravino$. Esto último es posible si el parámetro $\mu$ toma valores negativos, favoreciendo así el decaimiento al Higgs, y reduciendo notablement
el decaimiento al bosón $Z$.
Para la producción de partículas supersimétricas a partir de la colisión $pp$ se considera inicialmente la producción de gluinos. La parte del análisis que comprende la producción electrodébil se describe en el Capítulo \ref{cap:analysis_EWK}. Dichos gluinos decaen subsecuentemente en partículas más livianas, hasta llegar a las NLSP y luego a la LSP.
Como el gluino tiene carga de color y la NSLP no, el decaimiento esta intermediado por squarks virtuales, produciendo jets adicionales.
Cadenas de decaimiento típicas de este modelo se pueden observar en la Figura \ref{fig:phb_feyn}.


En la Sección \ref{sec:susy} se describe al MSSM junto con la gran cantidad de parámetros que lo caracteriza. 
Al realizar el estudio experimental, motivado por un dado modelo, se debe acotar la cantidad de parámetros y sus posibles valores, maximizando de esta forma la producción del estado final buscado. En este contexto, si bien se realizan búsquedas motivadas por un dado modelo, al seleccionar y definir las regiones de señal, se posibilita que el espacio de parámetros seleccionado pueda ser también inclusivo a otros potenciales modelos de nueva física. 

El parámetro característico de este modelo es $\mu$, el cual toma valores negativos para habilitar el decaimiento del \ninoone a Higgs. A su vez, se busca suprimir el decaimiento al bosón $Z$, y que el decaimiento a fotones sea igual de probable que el decaimiento al Higgs. Como se desea tener este comportamiento de forma uniforme para todas las masas de \ninoone estudiadas, es necesario encontrar un valor óptimo de $M_1$ que satisfaga dichas condiciones. Se encuentra que mediante la relación $M_1 \sim |\mu|$,
es posible reducir el decaimiento al bosón $Z$ hasta casi un 10\%, dejando a los otros dos decaimientos aproximadamente en un 45

Las muestras se generan considerando la producción de gluinos a partir de la colisión $pp$, con 0, 1 o 2 jets adicionales de ISR. Para esto, se considera al gluino como la única partícula supersimétrica de color relevante, por lo que las masas de los squarks se fijan a \magn{5}{TeV}, evitando así toda posible producción \textit{on-shell} de los mismos (en lo que se denomina desacoplamiento). Los gluinos producidos en la colisión, decaen a un par de quark-antiquark más algún gaugino como se muestra en la Ecuación \ref{eq:gluino_dec}. Esto ocurre mediante squarks virtuales los cuales son completamente degenerados, pudiendo ser cualquiera de los 12 estados de sabor/quiralidad.
Al igual que con los squarks, se elige la masa de los sleptons igual a \magn{5}{TeV}, evitando así su producción.
Además, se anulan todos los términos de acoplamiento trilineal, y se fija $M_2=3\,\tev$ y $\tan{\beta}=1.5$, observándose reducida sensibilidad del análisis a variaciones de dichos parámetros. Se emplea el bosón de Higgs observado por las colaboraciones ATLAS y CMS \cite{higgs_mass}, con una masa de $m_h=125\,\gev$, y sus decaimientos ocurren de acuerdo a las predicciones del SM, 
donde el predominante es a dos $b$-jets con un $\sim58$
Adicionalmente, se colocan a los estados de masas de los dobletes de Higgs en el régimen de desacoplamiento con una masa de \magn{2}{TeV}.
Se fija la vida media del \ninoone, $c\tau_{\text{NLSP}}<0.1$ mm, de tal forma que decaiga rápidamente para que su vértice no esté considerablemente desplazado del punto de colisión. 
La masa del \gravino, la LSP impuesta por los modelos GGM, se fija del orden de \magn{1}{eV}.
La relación entre los parámetros $M_1$ y $\mu$ determina la masa de los gauginos, y genera que los tres primeros neutralinos estén levemente degenerados junto con el primer chargino. Tanto el \ninofour como el \chinotwopm quedan completamente desacoplados. Los posibles decaimientos del \ninotwo son principalmente a \ninoone junto con un par de quarks, leptones o neutrinos. Los del \ninothree son principalmente a \ninoone junto con un fotón. El \chinoonepm decae principalmente a \ninoone junto con un par de quarks o un par leptón-neutrino. Finalmente se anularon los decaimientos directos del \gluino, \ninotwo, \ninothree y \chinoonepm. Como ejemplo, se muestra en la Figura \ref{fig:mass_spec} un espectro de masas para uno de los puntos de señal con $(M_3, \mu) = (2000\,\gev, -1050\,\gev)$. En la Figura \ref{fig:gluino_decays} se observa la fracción de los posibles decaimiento del \gluino con una masa de \magn{2000}{GeV} en función de la masa del \ninoone. 




Debido a que son varios los análisis que estudian la producción de gluinos, la colaboración ATLAS genera archivos comunes a todos los análisis donde ya tiene almacenada la información de varios eventos de producción de gluinos. Estos archivos se denominan Les Houches Event (LHE) \cite{Alwall:2006yp}, que estandarizan esta etapa de simulación, y sólo resta generar las cadenas de decaimiento de los gluinos de acuerdo a los distintos modelos estudiados. En el presente análisis se omiten las correcciones radiativas así la masa de los los gluinos coincide con el parámetro $M_3$.

Los únicos parámetros libres del modelo son entonces $\mu$ y $M_3$, que determinan la masa de los \ninoone y gluinos respectivamente. Se simulan 80 combinaciones de dichos parámetros (puntos) con 10000 eventos cada uno, donde $150\,\gev<m_{\ninoone}<(m_{\gluino}-50)\,\gev$ y $1200\,\gev < m_{\gluino}<2800\,\gev$. El arreglo completo de puntos de señal (grid) se muestra en la Figura \ref{fig:grid_points}, las cuales se realizan mediante la simulación rápida del detector \texttt{ATLFAST-II}. El espectro de masas completo, las fracciones de decaimiento de las sparticles y los anchos de decaimientos, se calculan a partir del conjunto de parámetros anteriormente mencionados utilizando {\texttt{SUSPECT} v2.43} \cite{Djouadi2007426}, {\texttt{SDECAY} v1.5} \cite{Muhlleitner:2004mka} y {\texttt{HDECAY} v3.4} \cite{Djouadi:1997yw}, que son parte del paquete {\texttt{SUSYHIT} v1.5a} \cite{Djouadi:2006bz}.



La sección eficaz de las muestras se calcula en función de la masa del \gluino a 
NNLO$_\text{approx}$+NNLL \cite{Beenakker:1996ch, Kulesza:2008jb, Kulesza:2009kq, Beenakker:2009ha, Beenakker:2011fu}. La sección eficaz nominal, junto con su incerteza, se obtienen de la combinación 
de las distintas secciones eficaces empleando diferentes conjuntos de PDFs, y modificando los factores de renormalización y factorización, como se describe en la Referencia \cite{Borschensky:2014cia}. En la Tabla \ref{tab:gino_xs} se muestran los valores de las secciones eficaces para las distintas masas de \gluino empleadas para las muestras.




Los procesos del SM que cumplen el rol de fondo para el presente análisis, son aquellos que tienen el mismo estado final que la señal, es decir, fotones, jets y energía transversa faltante. Los mismos pueden ser clasificados en distintos tipos. Por un lado, los procesos que dan lugar a eventos con un fotón y energía faltante real, denominados fondos irreducibles. En ellos, la energía faltante proviene de los neutrinos, y se componen principalmente de partículas que decaen a ellos, junto con fotones y jets de la ISR. Los procesos que cumplen con estos requisitos son la producción de bosones $Z$, $W$ y pares de top quarks que decaen subsecuentemente a bosones $W$:


Adicionalmente, es posible tener procesos que si bien no producen fotones, uno de los objetos presentes en el mismo es erróneamente reconstruido como fotón, y genere un estado final igual al buscado pero con fotones <<falsos>>. Los objetos que pueden ser erróneamente reconstruidos como fotones son los electrones y los jets. Se considera entonces aquellos procesos que no tienen fotones reales, pero se producen electrones o jets junto a neutrinos:



Por último, puede ocurrir que si bien en el proceso no hay neutrinos, una reconstrucción errónea de la energía de los distintos objetos presentes en el evento, genere un desbalance al calcular la energía transversa faltante, y por ende tengan una cantidad no despreciable de la misma, denominada energía transversa faltante instrumental. Este tipo de eventos puede ocurrir también en simultáneo junto con fotones <<falsos>>, por lo que existen diversos procesos que cumplan tales requisitos, entre los que se encuentran:

Como notación simplificada de los fondos del análisis, se omite en el nombre tanto el <<+>> como la producción de jets en la ISR. Cabe destacar que existen más procesos que cumplen las condiciones anteriores, pero que no son considerados para el análisis. Esto se debe a que la sección eficaz es despreciable comparada con los otros procesos considerados, o el estado final se encuentra suprimido por las selecciones básicas del análisis, por lo que su contribución es completamente despreciable. Algunos ejemplos de ellos son la producción doble de bosones ($WW$, $ZZ$, $WZ$), bosones decayendo a quarks ($W(\rightarrow qq)$, $Z(\rightarrow qq)$), producción de top ($t\gamma$, $tW$), entre otros.

Para modelar los fondos con fotones <<reales>> se utilizan simulaciones de MC, mientras que para aquellos con fotones <<falsos>> se utilizan técnicas basadas en datos. 
Para los jets falseando fotones (\textit{jfakes}) se emplea un método denominado \texttt{ABCD}, y para los electrones falseando fotones (\textit{efakes}) un método denominado \texttt{Tag\&Probe}. El modelado de todos los fondos se describe en las siguientes Secciones.



Los fondos del SM con fotones <<reales>> son modelados utilizando simulaciones de MC, los cuales consisten en: \phj, \wph, \ttbarph, \wphph, \zph, \zphph, \phph. Los tres primeros son considerados de mayor impacto en el análisis y por ende son normalizados en respectivas regiones de control. Para el resto de los fondos solo se utilizan las simulaciones con las normalizaciones descriptas en la Sección \ref{sec:mc_weights}. 
Cabe mencionar que también el fondo \znunuph tiene un impacto considerable en algunas regiones del análisis, pero la dificultad de diseñar una región de control para dicho fondo sin contaminación de señal, llevó a la decisión de utilizarlo sin una normalización dedicada. 
La parte del análisis que se basa en la producción electrodébil sí hace uso de una región de control dedicada para este fondo, debido a que el impacto de dicho proceso es mucho más elevado, como se describe en el Capítulo \ref{cap:analysis_EWK}.

Todos los procesos, salvo \ttbarph, fueron simulados utilizando el generador {\texttt{SHERPA} v2.2} \cite{Bothmann:2019yzt}. Los elementos de la matriz se calculan para un máximo de cuatro partones
a LO, y se fusionan con la lluvia de partones de \texttt{SHERPA} \cite{Schumann:2007mg} utilizando la prescripción de \texttt{MEPS@LO} \cite{Hoeche:2012yf}. La muestra de \ttbarph se genera con \texttt{MadGgraph5\_aMC@NLO} \cite{Alwall:2014hca} a segundo orden en teoría de perturbaciones (NLO), con \texttt{Pythia8} para el modelo de la lluvia de partones \cite{Sjostrand:2014zea}. 
Para todas las muestras se utilizó en la simulación del detector el programa \texttt{Geant4}. En la Tabla \ref{tab:mc_samples} se listan todas las muestras de fondos utilizadas en el análisis, las mismas se encuentran segmentadas (\textit{slicing}) según se aclara explícitamente para cada una.



Es posible que un jet sea erróneamente reconstruido como un fotón principalmente cuando proviene de un $\pi^0$. Los piones neutros decaen rápidamente a dos fotones, que naturalmente son reconstruidos en el ECAL. Para poder distinguir el decaimiento de un pión de la producción de un fotón individual, se utiliza la primera capa del calorímetro, que tiene mayor granularidad (Sección \ref{sec:ph_id}). En el caso de que el pión se produzca con un elevado \pt, los dos fotones pueden estar muy colimados y por ende ser prácticamente indistinguibles de un fotón individual. Si bien la identificación \texttt{Tight} se encarga de suprimir en gran parte esta reconstrucción errónea, aún así puede contener una contaminación moderada de este proceso. Este tipo de fondo proviene principalmente de procesos como Multijets, $W+\text{jets}$ o de \ttbar decayendo semi-leptónicamente. 
Como es extremadamente difícil modelar con precisión la tasa de errónea de jets a fotones mediante simulaciones, se emplean técnicas basadas en datos para estimar su contribución.


El método empleado para estimar este fondo se denomina \texttt{ABCD} \cite{Alonso:2233238}. El mismo hace uso de la diferencia que existe en las variables de aislamiento de fotones <<reales>> (señal en este contexto) y la de los <<falsos>> (fondo), para poder selecciones eventos de uno u otro.
En el contexto de este método, se definen fotones aislados como aquellos que pasan el WP \texttt{FixedCutTight},

 $-20\,\gev<\ETiso<0\,\gev$ y $0<\pTiso<0.05$, y los no-aislados aquellos con $8\,\gev<\ETiso<80\,\gev$ o $0.15<\pTiso<1$.
A su vez se utiliza una selección de identificación adicional que discrimina los fotones <<reales>> \texttt{Tight} de los fotones <<falsos>>. Este criterio de identificación se denomina \texttt{Non-Tight} (denominado también \textit{pseudo-photons}, \texttt{Tight-4} o \texttt{LoosePrime}) y consiste en los fotones que pasan la selección \texttt{Loose} pero que no pasan alguno de los criterios de selección \texttt{Tight} que emplea las variables $w_{s3}$, $F_{\text{side}}$, $\Delta E$ o $E_{\text{ratio}}$, por lo que esta selección es un subconjunto de los eventos seleccionados por el trigger de fotones \texttt{Loose}, pero completamente ortogonal a la identificación \texttt{Tight}. 

El método \texttt{ABCD}, utilizado en este análisis en particular, preselecciona eventos con al menos un fotón con $\pt>1450\,\gev$, al menos dos jets y ningún leptón, cuyos requisitos son idénticos a los que se usan en el análisis descriptos en la Sección \ref{sec:selection}. A partir de ello se definen cuatro regiones \cite{ATL-COM-PHYS-2016-1626}:


La Figura \ref{fig:abcd_regions} muestra la distribución de datos en las variables de aislamiento para las regiones A, B, C y D. En la misma se ve explícita la brecha (\textit{gap}) que existe entre las variables de aislamiento para reducir así la contaminación entre las regiones.


El método se basa en asumir que no hay correlación entre las variables de aislamiento y la selección de identificación \cite{tesis_tony}, y que tampoco hay contaminación de eventos de señal en las regiones B, C y D ($N_{B,C,D}=N_{B,C,D}^b$), lo que permite esperar la siguiente relación para los eventos de fondo esperados en la región A: $N_A^b / N_B = N_C/N_D$. 
Reescribiendo la expresión anterior se puede estimar el número de fondo en la región A como:



donde:


son los denominados \textit{Fake Factors} (FF). 
Si bien ambos factores deben dar resultados equivalentes, se encuentra que al usar $\text{FF}_{\text{iso}}$ las incertezas sistemáticas se ven reducidas, y por ende se decide usarlo en el análisis.

Distintas correcciones se realizan sobre este método. El primero es considerar la posibilidad de una contribución de fotones reales de señal en las regiones B, C y D. La misma se estima utilizando simulaciones de MC con fotones <<reales>> a nivel generador, y luego se resta a los datos observados, para asegurar de este modo solo el conteo de eventos de fondo en cada región. Adicionalmente, se considera una posible correlación entre variables calorimétricas y de identificación, que se puede agregar como un factor multiplicativo a la Ecuación \ref{eq:jfakes_ff}. Ese factor se define como:


Debido al blinding del análisis, no es posible conocer el número de eventos observados en la región A, por lo que se calcula el factor $R$ en regiones contenidas dentro de la zona de eventos de fondo, asumiendo que tienen la misma correlación entre ellas:

donde:


cuyas definiciones se basan en tener regiones exclusivamente con fondo, pero con una estadística suficiente.

Con estas correcciones la estimación del fondo final queda de la forma:

Los FFs se estiman en función del \pt y \absEta del fotón, y \met del evento. Distintas fuentes de incertezas sistemáticas son consideradas. 
Una de ellas es cambiar la definición de la identificación \texttt{Non-Tight}, utilizando ahora \texttt{Tight-3}\footnote{Selección \texttt{Loose} que no pasa alguno de los criterios de selección \texttt{Tight} que emplea las variables $w_{s3}$, $F_{\text{side}}$ o $\Delta E$} y \texttt{Tight-5}\footnote{Selección \texttt{Loose} que no pasa alguno de los criterios de selección \texttt{Tight} que emplea las variables $w_{s3}$, $F_{\text{side}}$, $\Delta E$, $E_{\text{ratio}}$ o $w_{\text{stot}}$}. A su vez se consideran los efectos de la correlación residual, calculando los FFs con $R'=1$, e incluyendo las variaciones como sistemáticos.

En la Tabla \ref{tab:jfake_ff} se puede observar los valores de los FFs obtenidos en función del \pt y $|\eta|$ del fotón, y \met del evento. Los valores obtenidos para $R'$ dan cercanos a la unidad con desviaciones cercanas al 10\%.

El fondo para cada región (R) del análisis se estima definiendo una correspondiente región de control de jets (CSJ), que cumple el rol de la región $N_B$, y se define de igual forma que R pero reemplazando los cortes sobre fotones por jets. Se estima la contribución del fondo entonces como:



donde la suma se realiza sobre los distintos intervalos de \absEta, \pt y \met.
En el caso de no observar eventos en alguna CSJ, se hace una estimación conservadora con un evento.




Los fotones y los electrones pueden dejar lluvias electromagnéticas muy
similares en el ECAL, cuya reconstrucción se describe en la Sección \ref{sec:ph_el}.
Si bien los algoritmos de reconstrucción de electrones y fotones están diseñados para poder discriminar uno de otros, una pequeña fracción residual de electrones puede ser reconstruida erróneamente como fotones. Si bien la reconstrucción de los clusters es altamente efectiva, la fracción de electrones mal reconstruidos se debe prácticamente a ineficiencias en la reconstrucción de trazas. Por ejemplo, la traza de un electrón puede ser mal reconstruida como un vértice de conversión, y por ende el mismo es reconstruido como un fotón convertido. O inclusive una errónea asociación de la traza con el cluster puede hacer que el electrón sea reconstruido como fotón no convertido.
Puede ocurrir a su vez, que el cluster no satisfaga los criterios de ambigüedad, y el objeto sea almacenado por duplicado como electrón y fotón. Esto en general es fácil de suprimir aplicando requisitos de solapamiento, aunque dichos requisitos pueden favorecer al electrón, y tener nuevamente en la selección del análisis electrones mal reconstruidos. Este tipo de efectos se ve principalmente en procesos como $W(l\nu)+\text{jets}$, $Z(ee)+\text{jets}$ o \ttbar. Al igual que con los jets, como es prácticamente imposible estimar esta reconstrucción errónea con simulaciones de MC, se utiliza una técnica basada en datos para estimar dicha fracción de reconstrucción errónea. Para ello se utiliza una muestra de eventos con producción de bosones $Z$, como el mismo no puede decaer a $e+\gamma$, los eventos con este par de partículas y que parecieran provenir del decaimiento de un $Z$, son un buen indicio de que ese fotón en realidad es un electrón mal reconstruido. En ese caso se puede estimar una fracción de reconstrucción errónea (Fake Factor, FF) como:



donde $P(e\to e(\gamma))$ es la probabilidad de reconstruir e identificar un electrón <<real>> como un electrón (fotón), $\epsilon(e^{\text{real}}\to e(\gamma)^{\text{reco}})$ es la eficiencia de reconstruir un electrón <<real>> como un electrón (fotón)\footnote{Definida como $\frac{N_{e^{\text{real}}\to e(\gamma)^{\text{reco}}}}{N_{e^{\text{real}}\to e^{\text{reco}}}+N_{e^{\text{real}}\to \gamma^{\text{reco}}}}$}, $\epsilon^{\text{ID}}_{e} (\epsilon^{\text{ID}}_{\gamma})$ es la eficiencia de identificar un electrón (fotón), y $N_{\text{real}\ e\to \text{reco}\ e(\gamma)}$ el número electrones reconstruidos como electrones (fotones). Como no es posible saber a partir de los datos cuándo un electrón es <<real>>, este factor debe ser estimado a partir de los objetos ya reconstruidos y sus eficiencias, magnitudes que sí son mensurables en datos. Para una dada muestra con eventos de bosones $Z$, la probabilidad de reconstruir a sus productos de decaimiento como pares $ee$ o $e\gamma$ esta dada por:



Usando la Ecuación \ref{eq:efake_ff} se puede escribir al FF en función de los objetos reconstruidos a partir de los decaimientos de bosones $Z$:


El método para estimar los FFs, denominado \texttt{Tag\&Probe} \cite{tesis_gonza}, emplea una muestra a partir de todos los datos tomados durante el Run 2, seleccionando aquellos que contengan al menos dos electrones, o al menos un electrón y un fotón. En el caso de múltiples posibles pares en el evento, se utiliza la combinación cuya masa invariante esté más cerca de $m_Z=91.1876\,\gev$ \cite{ParticleDataGroup:2018ovx}. A su vez, se aplica un corte con $\met<40\,\gev$ para suprimir eventos con $W$ decayendo a electrones. Los requisitos de los objetos son idénticos a los que se usan en el análisis descriptos en la Sección \ref{sec:selection}.

Para identificar si el par proviene del decaimiento de un bosón $Z$ se reconstruye su masa invariante, de forma separada, dependiendo si el par es $ee$ o $e\gamma$. Como el FF se calcula en función de $|\eta|$, se obtienen distribuciones de la masa invariante de cada par, para distintos intervalos de $|\eta|$ como se describe en la Figura \ref{fig:efakes_matrix}. En estas selecciones, puede ocurrir que haya un conjunto de eventos que no provenga del $Z$ (fondo no resonante), por lo que se ajusta una función del tipo <<señal+fondo>> a cada distribución de masa. Para la señal se utiliza una función del tipo \textit{Double-sided crystal ball}\footnote{La misma se define como una Gaussiana, cuyas colas se modifican por funciones potenciales. Los parámetros libres son la media y desviación estándar de la Gaussiana, junto con los factores de cada potencia y las fronteras donde se juntan con la Gaussiana} \cite{Das:2016stf} y para el fondo una función Gaussiana.



Una vez ajustada cada distribución, la función se integra alrededor del pico del $Z$ en una ventana definida como $[\mu - 3\sigma, \mu + 3\sigma]$, para obtener el número de eventos que se emplea en los factores de la Ecuación \ref{eq:efake_ff}, para cada intervalo de $|\eta|$. En la Figura \ref{fig:efakes_fit} se puede observar el resultado del ajuste para la distribución de masa invariante de pares $ee$ y $e\gamma$, con $|\eta| \in [0-0.6]$. 
Cabe mencionar que al aplicar un corte en $\pt>145\,\gev$ se introduce un sesgo en la distribución de la masa invariante de los pares, el cual se considera despreciable dada su lejanía con el pico del $Z$. Dicho efecto se observa en la región de masas de \magn{300}{GeV}, para el cual se incluye una función Gaussiana adicional al fondo, centrada en esa zona.




La incerteza sistemática se estima variando las ventanas de integración a $[\mu - 2\sigma, \mu + 2\sigma]$ y $[\mu - 4\sigma, \mu + 4\sigma]$. Adicionalmente se calculan los FFs sin la sustracción del fondo, incluyendo esta variación como un sistemático que contempla el sesgo en la elección de la función de ajuste. Finalmente, como la energía del fotón <<falso>> es reconstruida con algoritmos para fotones, cuando en realidad deberían haber sido empleados los de electrones, se considera esto como una incerteza sistemática. Para ellos se calculan nuevamente los FFs ahora modificando el valor de energía de los fotones en un factor 1.5\%. Dicho valor se obtiene de las diferencias que existen entre el $\mu$ de las distribuciones con pares $ee$ y pares $e\gamma$ \cite{EXOT-2016-32}. Los valores obtenidos para los FFs junto con sus incertezas se pueden observar en la Tabla \ref{tab:efakes_ff}. Se puede ver como los mismos aumentan con $|\eta|$, relacionados con la cantidad de material que atraviesan y con la mayor fracción de reconstrucción de fotones convertidos con una sola traza.


Finalmente, el fondo para cada región (R) del análisis se estima definiendo una correspondiente región de control de electrones (CSE), definida de igual forma que R pero reemplazando los cortes sobre fotones por electrones. Se estima la contribución del fondo entonces como:



donde la suma se realiza sobre los distintos intervalos de \absEta.
En el caso de no observar eventos en alguna CSE, se hace una estimación conservadora con un evento.




El presente análisis hace uso de la totalidad de datos con una energía de centro de masa de \magn{13}{TeV}, tomados con el detector ATLAS durante el Run 2 y que son <<aptos para física>>, acumulando una luminosidad total integrada de $139\,\ifb$. Los datos son seleccionados a partir del trigger \texttt{HLT\_g140\_loose}, que selecciona eventos con al menos un fotón con identificación \texttt{Loose} y con $\ET>140\,\gev$. Este trigger es completamente eficiente \cite{TRIG-2018-05}, superando el 99

Los datos y simulaciones de MC empleados para el análisis son preseleccionados con la derivación \texttt{SUSY1} descripta en la Sección \ref{sec:lhc_samples}, orientada a análisis con moderada actividad hadrónica y con la presencia de un fotón energético entre otras cosas.

Los objetos de interés para esta Tesis son los fotones, jets y leptones (electrones y muones) a los que se les aplica diferentes requisitos offline. Inicialmente se les requiere una selección base (\textit{baseline}),
que se emplea para aplicar un requisito de solapamiento (\textit{overlap removal}) entre los distintos objetos del evento.


Finalmente a los fotones, jets y leptones se les aplica una selección denominada \textit{signal}, siendo estos objetos los que definen las distintas regiones del análisis. Adicionalmente, a los fondos de MC se les aplica una selección a nivel generador para evitar un solapamiento o doble conteo entre muestras. Por ejemplo, a las muestras de \zph, \wph, \ttbarph y \phj, se les solicita que tengan uno y solo un fotón a nivel generador, y en cambio a las \zphph y \wphph dos fotones. 





Los fotones baseline deben pasar la selección de identificación \texttt{Tight}, tener $\ET>25\,\gev$, $|\eta|<2.37$ excluyendo la región del crack. Para los fotones signal se requiere adicionalmente tener $\ET>50\,\gev$, aunque para la selección de las regiones se pide adicionalmente que el fotón leading tenga $\ET>145\,\gev$ para garantizar la eficiencia del trigger. Adicionalmente se les aplica un requisito de aislamiento tanto calorimétrico como de traza, mediante el WP \texttt{FixedCutTight}.




A los electrones baseline se los selecciona con $\pt>10\,\gev$, $|\eta|<2.47$ excluyendo la región crack, y ser originados en el vértice primario. El requerimiento de identificación \texttt{Loose}
es aplicado. Los electrones signal son seleccionados además, con un $\pt>25\,\gev$, aplicando la identificación \texttt{Tight}
y el requisito de aislamiento \texttt{FCLoose}, o \texttt{FCHighPtCaloOnly} si tienen $\pt>200\,\gev$.




Los muones baseline son seleccionados con la identificación \texttt{Medium}, tener $\pt>10\,\gev$, $|\eta|<2.7$ y ser originados del vértice primario. A los muones signal se les requiere adicionalmente tener $\pt>25\,\gev$ y el WP de aislamiento \texttt{FixedCutLoose}.




Se emplean Jets \texttt{EMTopo}, y la selección baseline se define como aquellos con $\pt>30\,\gev$ y $|\eta|<2.8$. Este último requisito no es empleado en el cálculo de \met. Selecciones basadas en las trazas son aplicadas para rechazar jets con $\pt<120\,\gev$ y $|\eta|<2.4$ que se originen de las interacciones de pile-up \cite{ATL-PHYS-PUB-2014-001}. Jets signal son seleccionados con $\pt>30\,\gev$ y $|\eta|<2.5$. La identificación de $b$-jets es empleada en la definición de algunas regiones de control. Para ello se utiliza el algoritmo \texttt{MV2c10}, con una eficiencia de selección de los mismos del 77\%.



Como se mencionó en capítulos anteriores, los algoritmos de reconstrucción pueden fallar en los criterios de ambigüedad entre objetos, decidiendo almacenar simultáneamente a ambos. Una forma de lidiar con esto es eliminando al objeto que comparta alguna región espacial del detector con otro, en lo que se denomina overlap removal \cite{Adams:1743654}. Esta eliminación se hace en diferentes pasos, y en cada uno hay un objeto eliminado en presencia de otro. Los objetos eliminados en un dado paso no influye en los sucesivos.
El overlap removal está diseñado específicamente para el tipo de análisis \cite{ATL-COM-PHYS-2016-1518}, y los diferentes pasos aplicados en el presente análisis se resumen a continuación: 




El cálculo de \met se realiza de acuerdo a lo descripto en la Sección \ref{sec:met}. Los depósitos de energía en el calorímetro son asociados a los objetos de alto \pt en el siguiente orden: electrones, fotones, jets y muones. Las trazas que no fueron asociadas con ninguno de los objetos anteriores, son incluidas en el término soft. Durante el análisis, una vez hecha la selección final de los objetos, se hace una selección de eventos con $\met>50\,\gev$.



Una vez obtenidas las muestras de señal y una primera estimación de los fondos del SM, el objetivo del análisis es poder diseñar regiones de señal que permitan hacer una discriminación entre ambos. Para ello se obtiene del espacio de parámetros, la combinación de requisitos o cortes que maximiza la significancia esperada, descripta en la Sección \ref{sec:exp_sig}.
La optimización se realiza en primer instancia conociendo a priori la cinemática del estado final del fondo y señal, y luego mediante un proceso iterativo para encontrar los valores de cortes más óptimos. Al maximizar significancia, se intenta además, no tener regiones muy dependientes del modelo. Con el objetivo de evitar problemas extremos de baja estadística, que causen problemas en el manejo de los sistemáticos, se buscan regiones que no tengan un número de eventos de fondo nulo, poniéndose un mínimo requerido de tres eventos. A su vez se realiza una estimación conservadora de las incertezas sistemáticas del 30

Una vez definidas las SRs, se procede a la definición de las regiones de control y validación, que determinan un paso clave en el análisis para poder lograr un correcto modelado de los fondos. Las CRs se diseñan para normalizar los fondos principales del análisis, mediante un ajuste de solo fondo a los datos. Las VRs, en cambio, son diseñadas cubriendo el espacio de parámetros entre las SRs y las CRs, con el objetivo de verificar que el modelado de los fondos, y su extrapolación a las VRs, es correcto. Como ambas hacen uso de los datos, es indispensable que las mismas sean ortogonales a las SR para evitar un posible sesgo en los resultados finales, garantizando así las condiciones para el blinding.
A continuación, se detallan los pasos que se siguieron en la optimización, hasta el diseño final de las SRs y sus respectivas CRs y VRs.



El estado final del modelo bajo estudio consiste en al menos un fotón, presencia de jets y elevada \met, como describe el diagrama de la Figura \ref{fig:phb_feyn}. 
Se seleccionan los eventos con al menos un fotón mediante 
el trigger de fotones simple con menor umbral y sin prescale, el cual es el \texttt{HLT\_g140\_loose}, por lo que a todos los fotones leading de las regiones se les solicita tener $\pt>145\,\gev$, donde el trigger es completamente eficiente. Una selección inclusiva en el número de fotones contempla el caso en el que ambos \ninoone decaen mediante fotones. El decaimiento a \gravino produce grandes cantidades de \met, que se espera que sea al menos mayor a $200\,\gev$, un valor bastante más alto que los procesos usuales del SM. 
Los jets del estado final pueden provenir de los decaimientos de los \chinoonepm o \ninotwo, de la ISR o del decaimiento del Higgs, mientras que \met y los fotones provienen del \ninoone.
A grandes rasgos, la cinemática del evento depende de la diferencia de masa entre el \gluino y el \ninoone. 
Cuando la diferencia supera los $\sim 1000\,\gev$, el \gluino decae en etapas sucesivas hasta llegar al \ninoone, generando jets adicionales. A su vez, los productos del decaimiento del \ninoone son poco energéticos. En cambio, cuando la diferencia de masa es menor a $\sim 300\,\gev$ (escenario comprimido), el \gluino decae en mayor proporción de forma directa al \ninoone. En ese caso se generan menos jets en el evento, y tanto el \pt del fotón como \met van a ser relativamente más energéticos.
Estas diferencias entre los distintos puntos de señal motivó al diseño de tres regiones de señal, optimizadas para distintos puntos de señal. La SRH, optimizada para el escenario comprimido, caracterizada por fotones y \met energéticos, y bajo número de jets. En contraposición, está la SRL optimizada para masas de \ninoone baja, y caracterizada por un mayor número de jets, y con fotones y \met no tan energéticos. Finalmente la SRM fue optimizada para un región intermedia y con características conjuntas de las anteriores SRs. La Figura \ref{fig:sr_design} muestra el espacio de puntos para los cuales fue diseñada cada SR. Finalmente, si bien se pueden generar leptones en el estado final, se aplica un veto a los mismos para evitar el solapamiento con otro análisis que realiza una búsqueda similar con fotones y leptones en el estado final \cite{diph_8TeV}.


Definidas las selecciones caracterizadas por el estado final del modelo, se procede a agregar variables que hagan efectiva la discriminación de fondo y señal. Para reducir fondos donde \met es principalmente instrumental, se aplica una separación angular entre la misma y los objetos presentes en el evento. En la señal, \met proviene principalmente de los \gravino, que se espera que no tenga ninguna correlación angular con los fotones y jets. En cambio, en procesos donde \met proviene de la reconstrucción errónea de alguno de los objetos, es esperable que la misma esté alineada con ese mismo objeto. Por ese motivo se aplica un corte inferior en las siguientes variables:



donde las diferencias se definen de tal forma de que resulten siempre entre $0$ y $\pi$.

Para una mejor discriminación entre señal y fondo se emplea la variable $\HT$, definida como:



Para una señal de SUSY con un fotón energético y elevada actividad hadrónica, se pueden esperar valores de \HT superiores a $1500\,\gev$.

Finalmente, para realizar una reducción de fondo adicional, se emplea la variable:



donde $1\le N \le N_{\text{jets}}$, que representa la fracción de \pt de los primeros $N$ jets con respecto a la totalidad de jets. Para el presente análisis se encuentra que la variable \rtf daba los valores más óptimos de significancia. Las señales de SUSY se espera que contengan eventos con múltiples jets energéticos, y por ende valores bajos de \rtf, a diferencia de los procesos del SM con baja actividad hadrónica y de baja energía, con valores de \rtf cercanos a la unidad. Cabe destacar que esta variable sólo es posible definirla en regiones que soliciten al menos cuatro jets.
La definición formal de las regiones de señales se muestra en la Tabla \ref{tab:sr_def}.




En el presente análisis los procesos de mayor impacto son \phj, \wph y \ttbarph, para los cuales se diseñan regiones de control dedicadas a los mismos, denominadas CRQ, CRW y CRT respectivamente. Todas las CRs requieren al menos un fotón con $\pt>145\,\gev$ y luego un conjunto de cortes que no solo generen una buena estadística del fondo a modelar, sino también garanticen la ortogonalidad con las SRs. El corte en \HT en general es reducido con respecto a las SRs para aumentar así la estadística de los fondos, y por la misma razón el corte en \rtf se omite en todas ellas. 
Si bien algunos análisis definen regiones de control dedicadas a diferentes procesos para cada región de señal, en el análisis actual se emplean las mismas CRs para todas las SRs.

La producción de pares de top, en su decaimiento leptónico, genera un estado final con dos $b$-jets, dos leptones y \met. La CRT dedicada a este proceso requiere entonces la presencia de al menos dos $b$-jets y un leptón, garantizando así la ortogonalidad con las SRs. Para evitar contaminación de señal se emplea un corte superior en \met, ya que para este proceso se espera que no sea tan alta como en la señal de SUSY. 

La CRW, dedicada a \wph, se diseña de forma similar a la CRT, pero aplicando un veto en los $b$-jets para evitar una contaminación del fondo de \ttbarph.

En los procesos como \phj, donde \met es principalmente instrumental, se espera que el vector \met esté alineado con alguno de los jets. La CRQ aplica un corte superior en \dphijetmet para incrementar la abundancia de este fondo, garantizando también la ortogonalidad con las SRs. Si bien \met es baja para este proceso, como hay suficiente estadística, se aplica un corte inferior en la misma para tener mayor semejanza con las SRs.

En la Tabla \ref{tab:cr_def} se muestra la definición completa de las tres regiones de control. 




A continuación, se definen las VRs, las cuales fueron diseñadas para verificar el correcto modelado de los fondos, y su extrapolación a las SRs. Las mismas se encuentran en una región intermedia entre las CRs y las SRs, siempre siendo ortogonales a estas últimas. Se diseñaron un conjunto de VRs orientadas al modelado de los fondos de \wph y \ttbarph, otras para el de \phj, y una dedicada al fondo de electrones erróneamente reconstruidos como fotones. Nuevamente se requiere en todas ellas al menos un fotón con $\pt>145\,\gev$.

Se definieron cinco VRs orientadas al fondo de \phj. Estas regiones recuperan el corte en \dphijetmet de las SRs, pero agregan un corte superior en \met de $200\,\gev$, siendo así ortogonales a ellas. Entre dichas regiones se encuentran las VRM1L y VRM1H, orientadas a validar las regiones SRL y SRH respectivamente, emulando sus cortes en \pt del fotón y \met. A su vez la VRM1L incluye un corte similar en \rtf para validar la aplicación del mismo en las SRs. Ambas tienen un corte inferior en \met de $100\,\gev$, y se desprenden de ambas, regiones de validación equivalentes pero con un corte inferior en \met de $150\,\gev$, denominadas VRM2L y VRM2H. Finalmente se encuentra la VRQ, que es un compromiso entre las regiones anteriores, solicitando un reducido número de jets y a su vez el mínimo \pt para fotones. La definición completa de las VRs dedicadas al fondo de \phj se muestra en la Tabla \ref{tab:vrm_def}.


Para los fondos \wph y \ttbarph se definieron cuatro regiones de validación, por lo que todas requieren al menos un leptón, lo que las hace ortogonales a las SRs. La VRL1 y VRL2 estudian la región de bajo \met, validando distintos valores de \HT, mientras que las VRL3 y VRL4 lo hacen para $\met>200\,\gev$. Además VRL4 invierte el corte en \dphijetmet para validar esa variable en regiones con combinaciones distintas de la misma. La Tabla \ref{tab:vrl_def} muestras las definiciones completas de las VRs dedicadas a estos fondos.

Finalmente se encuentra la región de validación para el fondo de electrones erróneamente reconstruidos como fotones (VRE). La misma se diseña sin el veto de leptones y requiriendo al menos un $b$-jet. Como es un fondo donde el fotón es <<falso>>, se espera que su energía no esté correctamente reconstruida y esto genere \met instrumental. En ese caso \met y el fotón deberían estar alineados, por lo que se aplica un corte superior en \dphigammet, garantizando a su vez la ortogonalidad con las SRs. La definición de la misma se puede observar en la Tabla \ref{tab:vre_def}.


Un factor clave en el análisis es la estimación de las incertezas sistemáticas involucradas, las cuales afectan tanto la estimación de los fondos como de la señal en las distintas regiones del análisis. Las mismas son incluidas como parámetros nuisance en los distintos ajustes realizados, llegando a ser del orden de 60 incertezas. Existen dos tipos de incertezas sistemáticas, las experimentales y las teóricas, y se describen a continuación.


Las incertezas experimentales están asociadas a la estimación de los fondos, ya sea por incertezas provenientes de la simulación del detector, en la reconstrucción y calibración de objetos, correcciones por pile-up, medida de la luminosidad, como también en los métodos de estimación de fondos basados en datos.



La luminosidad del LHC durante el Run 2 se mide utilizando el detector de Cherenkov LUCID2 \cite{Avoni:2018iuv}, y se calibra a partir tomas de datos especiales de baja luminosidad del LHC, utilizando el método van der Meer \cite{vanderMeer:296752,Grafstrom:2153734}. La misma es empleada en general para la medida de secciones eficaces, aunque en este análisis cumple un rol fundamental en la normalización de las simulaciones de MC a las condiciones del Run 2 del LHC. La incerteza en la luminosidad integrada combinada del Run 2 es de $1.7$



Las incertezas en la identificación y aislamiento de fotones son estimadas a partir de las diferencias que hay en las \textit{shower shapes}
entre datos y MC, utilizando eventos de bosón $Z$ decayendo radiativamente \cite{PERF-2013-04}. La escala de energía es determinada utilizando eventos de $Z\to ee$ y $J/\Psi \to ee$, y sus correcciones fueron aplicadas como variaciones de una desviación estándar del valor nominal. La misma metodología se emplea para las variaciones en la resolución.
De manera similar se obtienen los sistemáticos asociados a electrones \cite{ATLAS-CONF-2014-032} y muones \cite{PERF-2015-10}, los cuales se determinan a partir de eventos $Z\to l^+l^-$, $J/\Psi\to l^+l^-$ y $W\to l^\pm \nu$. 
Las incertezas consideradas para electrones son la variación de la escala de su momento, y variaciones en la incerteza del factor de escala de reconstrucción, identificación y aislamiento.
Para muones son consideran las variaciones en la escala y resolución de su momento 
, en la dependencia escala del momento con la carga, en la incerteza del factor de escala de reconstrucción y aislamiento, en los factores de escala de asociación entre traza y vértices, y el rechazo de muones con baja resolución de momento.




Las incertezas asociadas a jets se estiman siguiendo la metodología descripta en las Referencias \cite{PERF-2011-03} y \cite{ATLAS-CONF-2015-037}, las cuales provienen de múltiples fuentes y aportan una gran cantidad de parámetros nuisance. 


Entre ellas están la incerteza asociadas a la resolución de energía, obtenida a partir de la variación del \textit{smearing} de jets de MC a datos para la corrección de la resolución, usando eventos con dos jets y datos sin sesgo de trigger mediante conos aleatorios.
La variación de la escala de energía, proveniente de la intercalibración en $\eta$ de eventos con dos jets, balance \zj, balance \phj, balance Multijet, y de incertezas asociadas a la propagación de partículas individuales, el haz de prueba, pile-up, identificación de sabor y fuga hadrónica.
Finalmente se consideran incertezas asociadas a la eficiencia de la identificación de sabor del jet, y de la contaminación residual de jets luego de la supresión de pile-up (JVT) y la elección del generador de MC.




Cuando se modifica la escala o resolución de energía de algún objeto, esta variación se propaga en conjunto al término correspondiente al cálculo de \met. Adicionalmente los sistemáticos asociados al término soft de \met se obtienen mediante la comparación de la escala y resolución entre datos y MC, con distintos generadores y simulaciones de detector.



El factor de corrección de pile-up se obtiene para hacer coincidir el número promedio de interacciones por cruces de haces de las simulaciones de MC con la de datos. El mismo es de $1/1.03$, y la incerteza sistemática se obtiene variando el mismo a $1/0.99$ y $1/1.07$. 



Los métodos para la estimación de fondos de jets y electrones reconstruidos erróneamente como fotones, descriptos en las Secciones \ref{sec:jfakes} y \ref{sec:efakes}, tienen también incertezas asociadas. Una es la incerteza estadística propia de la muestra de control, empleada para la estimación final de estos fondos (Ecuaciones \ref{eq:jfake_cs} y \ref{eq:efake_cs}), y que se incluye como parámetro nuisance. A su vez, la incerteza proveniente del cálculo FFs, descripta en esas mismas Secciones, es considerada como incerteza sistemática.




Las incertezas teóricas afectan a todas las simulaciones de MC, y provienen principalmente tanto del generador empleado, como de los distintos parámetros de la teoría utilizados en los cálculos de las propias simulaciones. 

Para cada muestra de fondo, en cada región del análisis, se calcula una incerteza teórica total que incluye los efectos que más adelante se describen. Contrario a lo que se espera, las incertezas sistemáticas se ven afectadas por la estadística de la muestra, principalmente cuando la región tiene bajo número de eventos. Es por esto, que para el cálculo de las incertezas teóricas, se emplean regiones equivalentes a la del análisis pero relajando varios de sus cortes, eliminando así la dependencia con la estadística. Se emplea una región para sistemáticos común a todas las SRs del análisis, y una para cada grupo de VRs, las cuales se muestran en la Tabla \ref{tab:syst_reg}. Para las CRs se usan exactamente las mismas ya que por definición abundan los eventos de un dado fondo.



Todas las muestras de fondo se generan con un conjunto de pesos internos, que al aplicarlo a cada evento, representan el efecto que generaría la variación de distintos parámetros de la teoría. Las variaciones consideradas para cada muestra son las de la escala de renormalización ($\mu_R$) y factorización ($\mu_F$), la variación de los elementos de matriz (ME) de la PDF nominal, variación de la familia de PDFs empleada en la generación de los eventos, y las incertezas asociadas a la determinación y truncamiento de la constante de acoplamiento fuerte ($\alpha_S$). Para la muestra de \ttbarph, se emplean solo las dos primeras variaciones, las cuales son las predominantes. Tampoco se tienen en cuenta variaciones en la escala de \textit{matching} (CKKW),

 ni en la escala de resumación (QSF), ya que las mismas son despreciables y requieren la generación de muestras adicionales. 


El cálculo de la incerteza teórica total se realiza siguiendo las recomendaciones \texttt{PDF4LHC} \cite{Butterworth:2015oua}.
Para calcular las variaciones de los MEs de las PDFs se emplea \texttt{LHAPDF} \cite{lhapdf} con la PDF nominal, \texttt{NNPDF30\_nnlo\_as\_0118}, y 100 variaciones de sus MEs. Adicionalmente, se emplean dos familias de PDFs alternativas, \texttt{CT14} y \texttt{MMHT2014nnlo68c1}. Para las variaciones de escala, se consideran siete posibles valores: $(\mu_R, \mu_F) = (0.5,0.5), (1,0.5), (0.5,1), (1,1), (2,1), (1,2), (2,2)$, mientras que para las variaciones asociadas a $\alpha_S$, se consideran dos (<<up>> y <<down>>). Todas estas variaciones son combinadas en una sola incerteza total para cada muestra y para cada región, y cuyos valores porcentuales promedio se muestran en la Tabla \ref{tab:syst_values}. Dichas incertezas son del orden del 30-40


Finalmente, se calcularon las incertezas teóricas para las muestras de señal, empleadas al establecer límites al modelo. Las mismas se obtuvieron a partir del número de eventos en cada región, variando la estimación de la sección eficaz en una desviación estándar tanto para arriba como abajo.



A continuación se presentan los resultados obtenidos utilizando el conjunto completo de datos del Run 2 \cite{ATLAS:2021ijy}. Esto incluye los resultados del modelado de fondo en cada una de las regiones, el ajuste de solo fondo en las regiones de control, el acuerdo en las regiones de validación y los valores finales en las regiones de señal. En estas últimas, se presenta además el número de eventos observados unblinded, junto con los límites de exclusión tanto dependientes como independientes del modelo. Los métodos estadísticos empleados son los descriptos en el Capítulo \ref{cap:statistical}, llevados a cabo mediante los programas \texttt{HistFitter} \cite{Baak_2015} y \texttt{HistFactory} \cite{Cranmer:1456844}, dedicados a cálculos para la búsqueda de nueva física.



En la Tabla \ref{tab:bkgonly_cr} se muestran los resultados del ajuste de solo fondo para el conjunto de datos del Run 2. En la misma están listados el aporte que realiza cada fondo antes de realizar el ajuste y después de hacerlo. A su vez, se muestra el número de eventos observados en cada región, la pureza del fondo asociado a cada una de ellas, y el factor de normalización. 
En la CRW el factor de normalización ($\mu_W$) es cercano a la unidad, lo que implica un correcto modelado del fondo previo a la normalización inclusive. El factor de la CRT ($\mu_T$) es superior a la unidad lo que significa una subestimación del fondo de \ttbarph. Esto se explica más adelante, en el análisis de producción débil del Capítulo \ref{cap:analysis_EWK}, como una falta de inclusión de fondos con el mismo estado final que \ttbarph. Al incluir posteriormente el fondo de producción de tops y Higgs decayendo a fotones, se observa un valor más cercano a la unidad. Con respecto al factor de la CRQ ($\mu_Q$), la distancia a la unidad es bastante más significativa, implicando que prácticamente el fondo está doblemente sobre estimado. Si bien este efecto se observa de forma similar en otros análisis \cite{Alonso:2689095}, se concluye que la muestra no está diseñada para regiones de tan elevado \met, y debido a su moderado o bajo impacto en las SRs, esta desviación no se considera crítica. 


En la Figura \ref{fig:cr_dist} se puede observar las distribuciones de distintas variables para cada región de control luego de hacer el ajuste de solo fondo. En las mismas se muestra el aporte de cada fondo, siendo predominante el que corresponde a cada CR, y la comparación con los datos observados. En la Figura \ref{fig:signal_contamination_CR_bb} se observa la contaminación de señal para cada CR, donde se observa que la misma es prácticamente despreciable para todos los puntos de señal.



En las Tablas \ref{tab:bkgonly_result_vrm}, \ref{tab:bkgonly_result_vrm} y \ref{tab:bkgonly_result_vre} se muestran los resultados de la estimación de los fondo en cada región de validación, y algunas de las distribuciones observadas en las mismas se muestran en las Figuras \ref{fig:dist_vrqm_bkgonly} y \ref{fig:dist_vrle_bkgonly}. Se encuentra un buen acuerdo entre los fondos y los datos observados para todas las regiones de validación, dando a entender que la estimación realizada es precisa, y permitiendo continuar con el siguiente paso, donde se comparan los datos observados y las predicciones en las regiones de señal. 




Luego de la estimación de los fondos en las regiones de señal y su correcta validación en sus respectivas VRs, se procedió a la observación de los datos en dichas regiones. En la Tabla \ref{tab:fit_result_sr_unblinded} se muestra el número de eventos observados y la estimación de los fondos en cada región de señal. El número total de eventos observados para la SRL es de $2$, para la SRM de $0$ y para la SRH de $5$, mientras que el número de eventos esperados es de $2.67\pm0.75$, $2.55\pm0.64$ y $2.55\pm0.44$ respectivamente. A su vez, en la Figura \ref{fig:regions_pulls_unblinded} se observa el resumen de la estimación de fondo y datos observados para cada región del análisis. En la Figura \ref{fig:met_n-1_SRL_SRM_SRH_fr2} se observa la distribución de \met para las tres regiones de señal, pero omitiendo el corte en esa variable de las mismas (gráfico N-1), donde se incluye las estimaciones de los fondos, la de los datos observados y la señal. En la Tabla \ref{tab:syst_rel_impact} se muestra el aporte porcentual de cada sistemático en las regiones de señal, siendo predominantes las asociadas a la escala de energía de los jets y las incertezas teóricas.



Dado el buen acuerdo entre la estimación del fondo y los datos observados en las distintas SRs, se establecen límites superiores en el número de eventos de cualquier fenómeno más allá del SM con el estado final del análisis. Los límites se establecen para cada SR con un nivel de confianza del 95\%, utilizando el estadístico de prueba definido en la Ecuación \ref{ec:st_qmu}, y las prescripciones para los $\text{CL}_{s}$ de la Ecuación \ref{eq:cls_limits}. Para ello se realiza un muestreo del número de eventos de señal para un cierto modelo, y se encuentra cuándo el valor de $\text{CL}_{s}$ cae por debajo del 5\%, método descripto en la Sección \ref{sec:flujo_busqueda}. La distribución de los distintos estadísticos de prueba se aproxima mediante la generación de {$\smallsim$}50000 toys.

En la Tabla \ref{tab:model_indep_ul} se muestran los límites superiores en el número de eventos en cada región de señal, tanto observados como esperados. Los límites observados son de $4.73$, $3$ y $7.55$ para las SRL, SRM y SRH respectivamente, lo que implica que cualquier modelo que prediga un número menor de eventos en dichas regiones, ya está excluido por el presente análisis. A su vez, se muestra el límite en la sección eficaz visible, obtenido a partir de dividir el anterior límite por la luminosidad total integrada, y multiplicarlo por la aceptancia (fracción de eventos que pasan todas las selecciones cinemáticas a nivel generador) y la eficiencia (fracción de esos eventos que pasan los cortes a nivel detector). 
Se incluye el p-value de descubrimiento, que para regiones donde la predicción supera lo observado, se fija a un valor de $0.5$. En la región SRH el p-value es de $0.09$, lo que implica que las observaciones son compatibles con la hipótesis de <<solo fondo>>.


Se establecen los límites dependientes del modelo, buscando aquellos puntos del modelo de señal para los cuales $\text{CL}_{s}=0.05$, como se describe en la Sección \ref{sec:flujo_busqueda}. Dichos límites son calculados de forma independiente para cada región de señal, y luego combinados en un límite total. Para ello se elige de cada punto de señal, la región con mejor límite esperado, y se emplea su $\text{CL}_{s}$ para realizar la interpolación. La misma se realiza mediante el módulo \texttt{Scipy} \cite{Virtanen:2019joe}, empleando la interpolación \texttt{multiquadratic} con el parámetro \texttt{smooth} igual a $0.1$. La distribución de los distintos estadísticos de prueba se aproxima mediante la generación de {$\smallsim$}50000 toys.

En la Figura \ref{fig:limit_plot_combined} se muestran los límites observados y esperados, combinando los resultados de las tres regiones de señal, para los que se incluye la variación de las incertezas sistemáticas en $\pm1\sigma$. Para los límites esperados se varían las incertezas experimentales, mientras que para los observados la incerteza en el cálculo de la sección eficaz del modelo. Estos límites excluyen a 95


Las búsquedas de supersimetría están clasificadas por distintas características, en particular en la que se trabaja en esta tesis, está identificada con la producción fuerte de gluinos, y los resultados de la misma se muestran en los Capítulos anteriores. De forma análoga, es posible realizar una búsqueda con el mismo estado final, motivada por un modelo supersimétrico similar al anterior, pero dedicado a la producción electrodébil de partículas.

La metodología empleada para realizar dicha búsqueda consiste nuevamente en el modelado de muestras de señal y fondo, diseño de regiones sensibles a dicho modelo, y una posterior comparación entre las estimaciones de los fondos y los eventos observados. El presente Capítulo describe el diseño de una búsqueda con dichas características, su correspondiente validación y la sensibilidad esperada a una luminosidad de $139\,\ifb$. 



El modelo que motiva a la presente búsqueda es una extensión al descripto en la Sección \ref{sec:signal_samples}. En el mismo se optimizaron los parámetros del modelo para obtener las masas y decaimientos deseados, en los cuales no estaba contemplado el decaimiento a bosones $Z$. La búsqueda electrodébil emplea una estrategia simplificada para generar las muestras de señal. En principio se fijan directamente las masas y los decaimientos de todas las partículas, sin centrarse en los parámetros de la teoría que llevan a tales valores. A su vez, la búsqueda se realiza de forma sensible a posibles decaimientos del \ninoone a fotones, bosones $Z$ y Higgs. La Figura \ref{fig:EWK_GGM_diagrams} muestra los posibles diagramas de decaimiento para la búsqueda electrodébil.



Se generan 12 puntos de señal en función de la masa del \ninoone, cuyos valores son: \magn{150}{GeV}, \magn{250}{GeV}, \magn{350}{GeV}, \magn{450}{GeV}, \magn{650}{GeV}, \magn{750}{GeV}, \magn{850}{GeV}, \magn{950}{GeV}, \magn{1050}{GeV}, \magn{1250}{GeV} y \magn{1450}{GeV}. Los decaimientos del mismos se fijaron a $33\%$ de igual forma para $\gamma + \gravino$, $Z + \gravino$ y $h + \gravino$. Los estados finales de cada evento están caracterizados por los posibles decaimientos de los dos \ninoone de la cadena de decaimiento, los cuales pueden ser $\gamma\gamma$, $\gamma Z$, $\gamma h$, $ZZ$, $Zh$ y $hh$ (omitiendo los \gravino por simplicidad). Al estar permitidos los tres decaimientos, es posible realizar un ponderado de los eventos, con el objetivo de estudiar diferentes combinaciones de fracciones de decaimiento. Es decir, aquellos decaimientos que no sean de interés, se los puede suprimir asignándole un peso nulo, permitiendo estudiar los otros dos restantes. Esto permite analizar diferentes modelos a partir de una misma muestra, y además calcular posibles límites de exclusión en función de las fracciones de decaimiento.
La metodología para asignar dichos pesos a los eventos va a depender, tanto de las fracciones de decaimiento que se deseen estudiar, como del tipo de partículas que se genera en el estado final. A nivel generador de las muestras, es posible saber cuáles partículas se generaron en los decaimientos de los \ninoone, y aplicarle el siguiente peso al evento:



donde $\text{BR}_{X}$ es la fracción de decaimiento del proceso $\ninoone \to \gravino X$ que se desea estudiar.
El peso está normalizado de tal forma que la suma de todas las fracciones de decaimiento de la unidad. En particular, y a modo de estudio preliminar, la búsqueda se centra en dos modelos: el modelo equivalente al estudiado en la producción fuerte, donde el \ninoone decae $50\%$ a $\gamma + \gravino$ y $50\%$ a $\gamma + h$, denominado modelo <<ph+h>> donde $w(0.5, 0, 0.5)$. Y por otro lado, el modelo donde el \ninoone decae $50\%$ a $\gamma + \gravino$ y $50\%$ a $\gamma + Z$, denominado modelo <<ph+Z>> donde $w(0.5, 0.5, 0)$.

Las masas del \ninotwo y \chinopm se eligen levemente degeneradas, e iguales a la del \ninoone más \magn{10}{GeV} y \magn{11}{GeV} respectivamente. 
Sus decaimientos se fijan en $100\%$ al \ninoone a través de un $Z$ o $W$ virtual, con sus respectivos decaimientos del SM. Todas las demás partículas SUSY se desacoplan con una masa de \magn{4500}{GeV}. A partir de ellos, se consideran todos los posibles canales de producción electrodébil, los cuales son: $\ninoone \ninotwo$, $\ninoone \chinoonepm$, $\ninotwo \chinoonepm$ y $\chinoonep \chinoonem$. La sección eficaz de producción de cada uno se calcula utilizando \texttt{RESUMMINO-3.0.0} \cite{Beenakker:1999xh,Debove:2010kf,Fuks:2012qx,Fuks:2013vua,Fiaschi:2018hgm} con una precisión de NLO+NLL, utilizando la familia de PDFs \texttt{CTEQ6.6} y \texttt{MSTW2008}, siguiendo las recomendaciones de \texttt{PDF4LHC} \cite{Butterworth:2015oua}. La Figura \ref{fig:SUSY_EWK_xs} muestra las secciones eficaces de cada proceso junto con la total.



Tanto las simulaciones de fondos del SM, como las técnicas para modelar fondos a partir de datos descriptas en la Sección \ref{sec:sm_backgrounds}, se emplean de forma equivalente para esta búsqueda. Adicionalmente, se incluye el fondo de producción de tops y bosones de Higgs decayendo a fotones \cite{tesis_jose}, listadas en la Tabla \ref{tab:higgs_bkg}. Esto está motivado por el valor del factor de normalización obtenido para la región de control CRT, que al ser superior a la unidad, puede ser un indicio de que algún proceso con el mismo estado final no está siendo considerado.



Para la presente búsqueda se emplea el conjunto de datos con centro de masa de \magn{13}{TeV}, y una luminosidad integrada de \magn{139}{\ifb} tomados durante el Run 2. Los datos son seleccionados con el trigger \texttt{HLT\_g140\_loose}, y empleando la derivación \texttt{SUSY1} tanto para los datos como para las simulaciones. La selección de objetos es similar a la descripta en la Sección \ref{sec:selection}, con leves modificaciones en los leptones y jets. Dentro de la colaboración existen grupos dedicados a combinar los resultados obtenidos para distintos análisis de SUSY. Para ello, los análisis combinados deben tener selecciones ortogonales, por lo que se establece una selección particular de leptones para lograr dicho objetivo. La presente búsqueda, en base a una posible combinación con otros análisis, selecciona electrones baseline con un $\pt>4.5\ \gev$ y los signal con un $\pt>10 \ \gev$, mientras que los muones baseline se seleccionan con un $\pt>3\ \gev$ y los signal con un $\pt>10\ \gev$. A su vez en todas las regiones del análisis se solicita que el número de leptones baseline sea igual al número de leptones signal. Los jets empleados en la selección de eventos son \texttt{PFlow} jets con un $\pt>20\ \gev$. El algoritmo para identificar $b$-jets es el \texttt{DL1r}, con un $77\%$ de eficiencia.




La definición de las regiones de señal se realiza siguiendo metodología de optimización similar a la descripta en la Sección \ref{sec:sig_selection}.
Si bien el estado final de esta búsqueda coincide con la búsqueda anterior, la cinemática de los mismos tiene algunos aspectos distintos. Dada la baja masa de los gauginos producidos en la colisión, con respecto a la producción de gluinos, se espera que los productos generados en el decaimiento sean menos energéticos. Con el mismo criterio, el número de jets reconstruidos de la cadena de decaimiento se espera que sea menor.

Se diseñan cuatro regiones de señal dedicadas a cubrir todo el espectro de masas de \ninoone de las muestras de señal. Las regiones se definen con cortes comunes para todas, salvo por un corte inclusivo en \met. Estos cortes consisten en al menos un fotón con $\pt>145\ \gev$, un veto de leptones, al menos un jet, y separaciones angulares de \dphijetmet y \dphigammet mayores a $0.4$. A su vez, se incluyen cortes en dos variables adicionales para incrementar la separación de señal y fondo. La primera definida como:


La misma se entiende como la fracción de \met presente con respecto a la energía total presente en el evento, que para una señal de SUSY con gran actividad de partículas no interactuantes es esperable que sea mayor a $0.5$.
La segunda variable empleada es la Significancia de \met ($\mathcal{S}$), con una reconstrucción denominada \textit{object-based} descripta en la Referencia \cite{ATLAS-CONF-2018-038}. La misma sirve para discriminar eventos con \met proveniente de partículas poco interactuantes, de aquella producto de la reconstrucción ineficiente de las partículas (instrumental), ya que valores altos de $\mathcal{S}$ son un indicio del primer caso. Finalmente, y solo en el caso de eventos con dos fotones, se aplica un corte en la masa invariante de los mismos, excluyendo el rango de valores centrados en la masa del Higgs. Esto se hace con el objetivo de ser ortogonales al análisis con estado final similar, que selecciona dos fotones provenientes del decaimiento del bosón de Higgs. La selección empleada para las distintas regiones de señal se muestra en la Tabla \ref{tab:sr_ewk}.




Las regiones de señal anteriormente mencionadas, están diseñadas para obtener la mejor significancia esperada, lo que no implica que en el caso hipotético de no observar algún descubrimiento, los límites esperados que se impongan con las mismas sean los más óptimos. 
En base a eso, se diseña un conjunto de regiones señal destinadas exclusivamente a imponer límites de exclusión. Para poder distinguirlas, se denomina a las primeras como regiones de señal para descubrimiento (SRd\_XXX), y a las segundas para exclusión (SRe\_XXX). Las regiones para exclusión se obtienen en base a las de descubrimiento, modificando sus cortes para obtener la combinación que permita tener los límites esperados más altos. Para tener una estimación preliminar de dichos límites, se emplea la aproximación asintótica enunciada en la Sección \ref{sec:aprox_test}, y se omiten los sistemáticos para simplificar los cálculos. Se encuentra que para obtener mejores límites, los cortes en \met no deben ser inclusivos (sino ortogonales), y que además se debe omitir el corte en $\met/\meff$. La definición de las regiones de señal para exclusión se muestran en la Tabla \ref{tab:sr_ewk_excl}.


Los fondos del SM con mayor impacto en el análisis son \wph, \ttbarph, $\ttbar h$ y \znunuph, para las cuales se diseñan regiones de control dedicadas. El fondo \phj no se considera importante, debido a la presente selección en los objetos del estado final buscado, y por ende no tiene su respectiva región de control. Todas las regiones de control emplean selecciones similares a las descriptas en la Sección \ref{sec:cr_vr_sel} para favorecer cada fondo, pero adaptadas a las actuales SRs. En todas se omite el corte en $\met/\meff$ y $\mathcal{S}$, debido a la elevada reducción de fondo que producen.


Los fondos \ttbarph y $\ttbar h$, son normalizados simultáneamente en una región de control (CRT) que selecciona al menos un leptón y dos $b$-jets. Para el fondo de \wph se diseña una región de control (CRW) que selecciona un leptón con un veto a los $b$-jets, junto con un corte en la masa transversa de \met y el leptón\footnote{$\mtlep = \sqrt{2p_{\text{T}}^{\text{leptón}}\met[1-\cos(\phi^{\text{miss}}-\phi^{\text{leptón}})]}$}, para incrementar la pureza del fondo. Dado que el fondo de \znunuph presenta un estado final similar al de la señal, resulta prácticamente imposible diseñar una región de control para ese fondo que no tenga contaminación de señal. Para ello, se emplea un método alternativo, en donde se diseñan regiones de control para los procesos \zeeph y \zmumuph (CRZ\_el y CRZ\_mu), y se asume que las correcciones que necesita la simulación son equivalentes para la simulación de \znunuph, por lo que se aplica el mismo factor de normalización a los tres fondos simultáneamente. Como los fondos \zeeph y \zmumuph tienen \met despreciable, se construye una una energía transversa alternativa pero omitiendo del cálculo a los leptones. De esta forma se obtiene una \met alineada con los leptones del evento, a la cual se le aplica un corte para ser lo más similar posible a las SRs. A su vez, los cortes empleados para \dphijetmet y \dphigammet se obtienen en base a este cálculo de \met. En la Figura \ref{fig:met_inv} se puede observar una comparación entre \met de \znunuph y la reconstruida en las simulaciones de \zeeph y \zmumuph, en la que se demuestra un buen acuerdo entre las distintas estimaciones de \met. La definición de las regiones de control empleadas para el presente análisis se muestran en la Tabla \ref{tab:cr_ewk}.




Finalmente se incluye un diseño preliminar para regiones de validación. Las mismas se diseñan para validar la extrapolación de los distintos fondos normalizados en las control regiones, a las regiones de señal. Se diseñan cinco regiones de señal, cuatro asociadas a las regiones de control (VRW, VRT, VRZee y VRZmm) y una adicional para el fondo de electrones reconstruidos erróneamente como fotones (VRE). Las regiones basadas en las CRs incluyen ahora el corte en $\met/\meff$ y $\mathcal{S}$ para acercarse cinemáticamente a las SRs, y son ortogonales a estas últimas debido a la selección de leptones. La VRE se construye invirtiendo el corte en \dphigammet, favoreciendo al fondo de \textit{efakes}, y adicionalmente siendo ortogonal a las regiones de señal. En la Tabla \ref{tab:vr_ewk} se muestran las definiciones de las regiones de validación.





En la Tabla \ref{tab:bkg_only_fit_ewk} se muestran los resultados preliminares del ajuste de solo fondo, empleando las cuatro regiones de control. En las mismas se muestra el aporte de cada fondo antes y después del ajuste, junto con la pureza del fondo y el factor de normalización. Los fondos \ttbarph y $\ttbar h$ se ajustan con el mismo factor de normalización en la CRT, mientras que los fondos de \znunuph, \zeeph y \zmumuph lo hacen con su respectivo factor en las CRZ\_el y CRZ\_mu. De forma preliminar se emplea un sistemático \textit{dummy} del $25\%$ para todos los fondos en todas las regiones, emulando el aporte de los sistemáticos teóricos y del detector, observado en el análisis descripto en el Capítulo \ref{cap:analysis}. En general se obtuvieron factores de normalización cercanos a la unidad, lo que evidencia un correcto modelado por parte de las simulaciones. En las Figuras \ref{fig:crw_crt_dist_ewk} y \ref{fig:crz_el_mu_dist_ewk} se puede observar el buen acuerdo entre las simulaciones y los datos luego del ajuste.





En la Tabla \ref{tab:fit_result_vr} se muestran los resultados de las estimación de los fondos y su comparación con los datos en las regiones de validación, mientras que en la Figura \ref{fig:vr_ewk} se muestran algunas de las distribuciones más significativas. El acuerdo en dichas regiones se considera que es apropiado. 


En la Tabla \ref{tab:fit_result_sr} se muestran los resultados blinded en las regiones de señal de descubrimiento para el presente análisis. En la Figura \ref{fig:srd_ewk} se observan distribuciones para las regiones de señal de descubrimiento. Se puede apreciar cómo cada región de señal es sensible a un cierto rango de masas de \ninoone, llegando a ser sensible hasta masas de \magn{750}{GeV}. Cabe mencionar, que al observar la distribución de $\mathcal{S}$, se puede apreciar que con cortes más estrictos de dicha variable, se obtienen valores de significancia más altos. Esto tiene como consecuencia una reducción importante del fondo, lo que conlleva a diversas problemáticas, como no encontrarse en el régimen asintótico para la aproximación de los estadísticos de prueba, o inconvenientes al ajustar los sistemáticos para PLR. Se priorizó entonces tener un número de fondo significante, al costo de una reducción de sensibilidad.
La Figura \ref{fig:sre_ewk} se muestran los límites esperados empleando ambos conjuntos de regiones de señal, donde se puede observar que con las regiones de señal de exclusión los límites son mejores.
Dichos límites son del orden de \magn{1200}{GeV}, lo que podría significar un importante aporte a los límites impuestos en la actualidad a dichos procesos, los cuales se observan en la Figura \ref{fig:ewk_limits}.

En la Figura \ref{fig:pull_ewk} se muestra el resumen de todos los fondos estimados y datos observados (excepto en las SRs) en todas las regiones del análisis.



Los datos recolectados por el LHC durante todo su período de actividad han sido de una relevancia sorprendente para diversos análisis en el área de la física de partículas. Dadas sus condiciones de elevada energía de centro de masa, junto con la capacidad de producir un cuantioso número de eventos de colisión, resulta un escenario ideal para la realización de diferentes experimentos que exploren regiones del universo no estudiadas hasta el momento. El detector ATLAS es uno de los principales experimentos del LHC cuyo sofisticado diseño se describe en el presente trabajo. Con el mismo, se han realizado distintas búsqueda de nuevas partículas, entre las que se destaca el descubrimiento del bosón de Higgs en 2012. Una vez entendido y corroborado el mecanismo de Higgs dentro del Modelo Estándar, nuevos horizontes quedan por explorar en extensiones del modelo que permitan explicar incógnitas del universo y la naturaleza aun no entendidas. Uno de los modelos teóricos de mayor interés para su estudio en el área de la física de partículas durante los últimos años es Supersimetría. El mismo es una extensión del Modelo Estándar que propone la existencia de nuevas partículas, aún no observadas, y que se espera que tengan masas del orden del TeV. 


Para cualquier tipo de análisis realizado en el experimento, es clave un correcto y eficiente funcionamiento del detector. Las desafiantes condiciones del Run 2 del LHC requieren una constante optimización y mejora de la selección de eventos de colisión, junto con la reconstrucción de los objetos que se desprenden de la misma, siendo la de fotones de particular interés para este trabajo.
El sistema de Trigger de fotones de ATLAS ha demostrado funcionar cumpliendo estos requisitos, llegando a valores de eficiencia superiores al $90\%$ durante la toma de datos del Run 2. Estos valores fueron determinados en el presente trabajo, para distintos parámetros del detector e incluso en las condiciones de alta luminosidad, que superan dos veces el valor de diseño. Los resultados obtenidos son luego utilizados por toda la colaboración, en todos los estudios que involucran la selección de fotones online en distintos estados finales, incluyendo análisis con Higgs decayendo a dos fotones. A su vez, se presentaron los factores de escala obtenidos para dichas eficiencias, empleadas para la corrección de las simulaciones de MC, los cuales se determinaron con valores cercanos a la unidad, para los distintos parámetros que los caracterizan.

En el presente trabajo se realizó búsqueda de nueva física utilizando los datos de colisión protón-protón con $\sqrt{s}=13\,\tev$ correspondiente a una luminosidad integrada de $139\,\ifb$ registrada, tomadas por el detector ATLAS en el LHC durante el Run 2. La misma estaba motivada por un modelo de ruptura de supersimetría denominado General Gauge Mediation, con producción de gluinos y su subsecuente decaimiento a neutralinos como NSLP, para luego decaer a fotones, Higgs y gravitinos. El estado final experimental que caracteriza a este modelo es entonces de al menos un fotón energético, jets y un alto momento transverso faltante. El método empleado para realizar la búsqueda consistió en la definición de regiones abundantes en eventos de señal, y la estimación de los procesos del Modelo Estándar que podían emular un estado final similar. Posibles discrepancias entre los datos observados y la estimación de esos procesos significaría una evidencia de un fenómeno no contemplado en el SM. 
Se definieron tres regiones de señal diseñadas para cubrir el espacio de parámetros de los modelos estudiados, que estaban caracterizados por la masa del gluino y del neutralino más liviano. Las mismas resultaron con una predicción de $2.67 \pm 0.75$ eventos de fondo y $2$ eventos observados, otra con $2.55 \pm 0.64$ eventos de fondo y sin eventos observados, y la última que predice $2.55 \pm 0.44$ eventos de fondo con $5$ eventos observados.
Los resultados son compatibles entonces con la estimación de fondos del SM. Los límites superiores de $95\%$ CL dependientes del modelo se establecen en las posibles contribuciones de la nueva física en un escenario GGM con un neutralino NLSP que es una mezcla de higgsino y bino. Para la correspondiente producción de gluino, las masas se excluyen a valores de hasta \magn{2.2}{TeV} para la mayoría de las masas NLSP investigadas. Los límites superiores de $95\%$ CL independientes del modelo se establecen en el número de eventos de nueva física, llegando a ser de $4.73$, $3$ y $7.55$ para cada región de señal. Dichos límites se establecen de forma equivalente para la sección eficaz visible asociada de las contribuciones de la nueva física, llegando a ser $0.034\,\ifb$, $0.022\,\ifb$ y $0.054\,\ifb$ respectivamente. 

Finalmente se presentó el diseño completo de una estrategia para una búsqueda de nueva física motivada por un modelo supersimétrico, cuyo estado final contenía al menos un fotón energético, jets y un alto momento transverso faltante, con producción electrodébil de gauginos. La metodología empleada diseña regiones sensibles a dicho modelo junto con su respectiva estimación de fondos del SM. La sensibilidad de descubrimiento del análisis se encuentra para neutralinos con masa cercana a los \magn{750}{GeV}, y establece límites esperados para las masas de los mismos del orden de los \magn{1.2}{TeV}.