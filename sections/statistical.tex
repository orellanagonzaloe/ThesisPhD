\chapter{Estrategia general y elementos estadísticos para una búsqueda de nueva física}\label{cap:statistical}
% \addcontentsline{toc}{chapter}{Elementos estadísticos para la búsqueda de nueva física}
\chaptermark{Estrategia general y elementos estadísticos para una búsqueda de nueva física}

% \section{Estrategia general}\label{sec:statistical}

Una estrategia para llevar a cabo una búsqueda general de nueva física, consiste a grandes rasgos en realizar un experimento de conteo de eventos con características asociadas al modelo de estudio, y su comparación con las predicciones que el Modelo Estándar hace de eventos con las mismas características. En caso de que haya un `buen acuerdo' entre las predicciones del SM y los datos observados, es posible afirmar que bajo las condiciones del experimento no hay evidencia de nuevos procesos físicos y que las predicciones del SM son correctas. En el caso de observar una discrepancia entre las predicciones del SM y
los eventos observados (en la jerga denominada `exceso'), se puede afirmar que el SM tiene una carencia en sus predicciones y que se podría estar en presencia de un nuevo fenómeno físico. Los criterios para definir `buen acuerdo' y `exceso' requieren evaluaciones estadísticas rigurosas y se explican en el presente Capítulo. 

En el contexto de esta Tesis se denomina \textbf{señal} a los procesos del modelo teórico que motivan dicha búsqueda, y \textbf{fondo} a las predicciones del SM. Para poder identificar los eventos de señal es necesario conocer las características del mismo, y luego así, discriminarlos de otros procesos físicos presentes en el experimento. Se utilizan simulaciones de Monte Carlo para modelar la señal, reconstruyendo los observables cinemáticos que caracterizan a los eventos. Aplicando diferentes cortes en esas variables se puede favorecer ciertos procesos y desfavorecer otros, y el conjunto de dichos cortes define una \textbf{región} en el espacio de observables. Las regiones donde la señal abunda con respecto al fondo, y por ende donde se esperaría observar un exceso significativo en los datos, se denominan \textbf{Regiones de Señal (SR)}.

En este tipo de experimentos es fundamental un correcto modelado de los procesos de fondo. Existen diferentes técnicas para modelar estos procesos: basadas exclusivamente en datos, exclusivamente en simulaciones de Monte Carlo o basadas en simulaciones y corregidas con datos. La motivación de esta última se debe a que las simulaciones en general son validadas en regiones asociadas al proceso que modelan (SM por ejemplo), y como en este caso es necesario utilizarlas en regiones de señal, probablemente alejadas o más extremas de donde se validó, es esperable que esas predicciones en esas regiones no sean del todo correctas. Para ello se definen \textbf{Regiones de Control (CR)} donde abundan eventos de algún proceso de fondo de interés, dedicadas a normalizar las simulaciones de ese proceso en particular a los datos observados en la misma.

Finalmente se definen \textbf{Regiones de Validación (VR)} que justamente se utilizan para validar la estimación de los fondos anteriormente mencionados. Es importante destacar que el diseño de todas las regiones se realiza sin utilizar en ningún momento los datos en las SRs (\textit{blinding}) para evitar todo tipo de sesgo en el resultado del experimento. Por este motivo el diseño de todas las CRs y VRs debe ser ortogonal a las SRs, de tal forma que ningún evento de las mismas esté incluido en las SRs. Una vez que se tiene confianza en la estimación de los fondos mediante su validación en las distintas VRs, se procede a observar los datos en las SRs (\textit{unblinding}).

El concepto central en cualquier resultado estadístico es la probabilidad del modelo, que asigna una probabilidad a cada resultado posible del mismo. Un ejemplo muy utilizado en física de partículas es el modelo de Poisson que describe el resultado de un experimento de conteo:

\begin{equation}
	\text{Pois}(N|\mu) = \frac{\mu^{N}e^{-\mu}}{N!}
\end{equation}

\noindent
que define la probabilidad de observar $N$ veces cierto proceso aleatorio, medido en un intervalo fijo de tiempo, donde $\mu$ es el número medio de eventos esperado. La distribución de Poisson es utilizada para describir múltiples fenómenos como decaimientos radiactivos o cualquier experimento de partículas que conste de contar eventos en un intervalo de tiempo. Es importante mencionar que las probabilidades obtenidas en esta distribución dependen estrictamente del modelo asumido como hipótesis, en este caso representado por el número medio de eventos esperados ($\mu$). De tal forma que la probabilidad de obtener el número observado de eventos en el experimento va a depender del modelo a estudiar, los cuales por ejemplo pueden ser un modelo que sólo espera fondo, o un modelo que considera la composición de fondo y señal. La probabilidad de ocurrencia de los datos observados ($x$) bajo la hipótesis bajo estudio se denomina \textit{likelihood}:

\begin{equation}
	\mathcal{L}(x|\mu) = \text{Pois}(N=x|\mu) 
\end{equation}

\section{Likelihood máximo}

En el marco de esta Tesis se emplearon un conjunto de SRs, las cuales pueden ser consideradas como experimentos de conteo independientes con distribuciones de Poisson, cuyas predicciones se obtienen a partir de simulaciones. 
Es posible a su vez, realizar la búsqueda empleando distribuciones en alguna variable, las cuales requieren construir el modelo probabilístico que permita realizar predicciones de la misma. Si bien muchas distribuciones pueden ser derivadas de la teoría analíticamente, en general se utilizan simulaciones para generarlas. Esas simulaciones se describen mediante histogramas de la variable observada, y cada clase puede ser considerada como un experimento de conteo independiente con una distribución de Poisson. 
Para ambos métodos el \textit{likelihood} se escribe como el producto de las probabilidades de cada experimento:


\begin{equation}
	\mathcal{L}((x_1,x_2,...,x_N)|\mu) = \prod_{i=1}^N \text{Pois}(N=x_i|\mu) 
\end{equation}


El likelihood puede ser utilizado adicionalmente para estimar parámetros de la teoría que estamos estudiando. Por ejemplo, si nuestro modelo está caracterizado por un conjunto de parámetros $\bm{\theta}$, y asumimos que el mismo es verdadero, se esperaría que la probabilidad de observar dichos datos para ese modelo sea máxima cuando los parámetros $\bm{\theta}$ sean lo más próximo a los valores reales del modelo. El estimador de máximo likelihood (MLE) consiste en obtener los valores, $\hat{\bm{\theta}}$, de $\bm{\theta}$ que maximicen a la función likelihood. En general, para facilitar a los algoritmos computacionales de maximización, se busca en realidad el mínimo del logaritmo del likelihood (LLH):

\begin{equation}
	-\ln{\mathcal{L}(\bm{\theta}} = - \sum_{i=1}^{N}\ln{\text{Pois}(x_i|\bm{\theta})}
\end{equation}

Los estimadores tienen un sesgo proporcional a $1/N$. Cuando se tiende el número de mediciones a infinito el mismo se vuelve consistente, por lo que el valor estimado de cada parámetro converge al valor verdadero ($\bm{\theta}_0$). En dicho límite, el valor esperado del estimador coincide con $\bm{\theta}_0$ (no sesgado) y adquiere su menor varianza (eficiente).




\section{Contrastación de hipótesis}

Como se menciono anteriormente el experimento esta caracterizado por el conjunto de parámetros del modelo, denominados hipótesis. Con el objetivo de descubrir procesos de nueva señal se define la hipótesis nula ($H_0$), la cual se asume como verdadera y va a estar sujeta a prueba, evaluándola contra la hipótesis alternativa ($H_1$). En el contexto de esta búsqueda, la hipótesis nula (denominada también hipótesis de `solo fondo') asume que no hay eventos de señal y que todo lo observado debería ser fondo. A diferencia de la alternativa (hipótesis `señal+fondo') que sí predice eventos de señal. Si los resultados observados en el experimento difieren de los esperados bajo la hipótesis nula, es posible rechazar a la misma, y estar en presencia de un descubrimiento. Caso contrario de no poder rechazarla, es posible poner límites al modelo donde los roles de las hipótesis se invierten, y ahora la hipótesis nula incluye a la señal y la alternativa solo al fondo.

Las hipótesis pueden estar completamente determinadas o estar caracterizadas por distintos parámetros, y las mismas definen a las PDFs de los distintos observables. Para poder discriminar una hipótesis de otra se define un estadístico de prueba que es una función de los observables, $t(\textbf{x})$. El mismo tendrá asociada una función de densidad de probabilidad (pdf) dependiendo de la hipótesis ($g(t|H)$), que al aplicarle un corte en un dado valor ($t_c$) define una región critica en el espacio de observables. Si el valor de $t$ evaluado en los datos observados ($t_{\text{obs}}$) se encuentra dentro de esa región, la hipótesis nula es rechazada, como se muestra en la Figura \ref{fig:nullh}. En la misma se puede observar que la región pintada de azul ($\alpha$), denominada error de tipo I, representa la probabilidad de rechazar $H_0$ siendo esta es verdadera. Mientras que el área roja ($\beta$), denominada error de tipo II, representa la probabilidad de aceptar $H_0$ siendo esta es falsa. En un caso ideal, el estadístico de prueba obtendría los valores más pequeños posibles para $\alpha$ y $\beta$.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{images/statistics/hypo_test.pdf}
  \caption{Distribución del estadístico de prueba bajo la hipótesis nula $H_0$ (azul) y la alternativa $H_1$ (roja). El corte en $t_c$ define a la región crítica y de aceptancia. El área azul ($\alpha$) se denomina error de tipo I, y representa la probabilidad de rechazar $H_0$ siendo esta es verdadera. El área roja ($\beta$) se denomina error de tipo II, y representa la probabilidad de acepta $H_0$ siendo esta es falsa.}
  \label{fig:nullh}
\end{figure}


Alternativamente, se puede cuantizar el acuerdo entre el resultado de dicha búsqueda y una hipótesis dada, calculando el \textit{p-value}. El mismo se define como la probabilidad bajo dicha hipótesis de obtener un resultado igual o peor de incompatible con las predicciones de la hipótesis:

\begin{equation}
	p_H = \int_{t_{obs}}^{\infty} g(t|H)dt
\end{equation}

Un p-value chico implica una evidencia importante en contra de dicha hipótesis, y la misma se excluye si el p-value observado es menor a un cierto valor previamente definido.  Alternativamente se puede convertir el p-value en la significancia equivalente, $Z$. La misma se define como el número de desviaciones estándar ($\sigma$) que se debe encontrar por encima de su media, una variable con distribución gaussiana para tener una probabilidad superior igual al p-value:

\begin{equation}
	Z=\Phi^{-1}(1-p)
	\label{ec:sign}
\end{equation}

\noindent
donde $\Phi^{-1}$ es la inversa de la cumulativa (cuantil) de la distribución gaussiana. 


\section{Estadísticos de prueba}

% refrasear pues sacado de fran!!! Done!

El lema de Neyman-Pearson \cite{10.2307/91247} establece que el estadístico de prueba con mayor poder en la contrastación de hipótesis de `solo fondo' frente a hipótesis de `señal+fondo' (y viceversa), es el cociente de likelihoods:


\begin{equation}
	t(\textbf{x}) = \frac{\mathcal{L}(\textbf{x}|H_1)}{\mathcal{L}(\textbf{x}|H_0)}
\end{equation}

En este caso la mejor región crítica son aquellos \textbf{x} que satisfacen $t(\textbf{x})>c_\alpha$, donde $c_\alpha$ es una constante que se ajusta para que el tamaño de dicha muestra sea $\alpha$.

El procedimiento común entonces para establecer un descubrimiento en física de partículas, se basa en un experimento frecuentista de significancia, empleando el cociente de likelihoods como estadístico de prueba. Como se mencionó anteriormente, los modelos bajo estudio están caracterizados por un conjunto de parámetros. De dichos parámetros se separan aquellos de interés ($\mu$), de aquellos que a priori no se conoce su valor y deben ser ajustados a los datos, denominados \textit{nuisance} ($\bm{\theta}$). En el contexto de una búsqueda de nueva física, el parámetro $\mu$ representa la intensidad de la señal, de tal forma que la hipótesis de `solo-fondo' corresponde a $\mu = 0$, y la hipótesis de `señal+fondo' a $\mu = 1$. En cada región del análisis uno esperaría tener un una media de eventos dada por: $\langle n_i \rangle = \mu s_i + b_i$, donde $s_i$ ($b_i$) es el número esperado de eventos de señal (fondo) en la región $i$. Por otro lado, los parámetros nuisance representan las incertezas sistemáticas, provenientes de defectos en el modelado del detector o de la teoría, que idealmente se esperarían que fuesen despreciables. Para acercarse lo máximo posible a este escenario, los mismos son incluidos como parámetros a ajustar, con la consecuencia de reducir la sensibilidad del experimento.

En el caso donde hay un único parámetro de interés $\mu$, y el resto de parámetros son nuisance $\bm{\theta}$, es conveniente definir el \textit{Profile Likelihood Ratio} (PLR) \cite{Cowan:2010js}:

\begin{equation}
	\lambda(\mu)=\frac{\mathcal{L}(\mu, \doublehat{\bm{\theta}})}{\mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}
	\label{ec:plr}
\end{equation}

\noindent
donde en el denominador, los valores $\hat{\mu}$ y $\hat{\bm{\theta}})$ son los estimadores de MLE. De la misma forma en el numerador, los
parámetros $\doublehat{\bm{\theta}}$ son los valores que maximizan la función likelihood pero para un valor fijo de $\mu$. Este proceso de elegir valores específicos de los parámetros nuisance para un valor dado de $\mu$ se lo conoce como \textit{profiling}. El PLR depende explícitamente de $\mu$ pero es independiente de los parámetros nuisance que han sido `eliminados'
vía el profiling. La presencia de los parámetros nuisance que son ajustados a los datos ensanchan la función likelihood como función de $\mu$, respecto a la distribución si sus valores estuvieran fijos. De cierta forma reflejan una pérdida de información sobre $\mu$ debido a estos parámetros desconocidos.

\section{Descubrimiento}

De la definición de $\lambda(\mu)$ se puede observar que la misma puede tomar valores solamente entre 0 y 1, donde 1 implica un buen acuerdo entre los datos y el valor hipotetizado de $\mu$. De forma equivalente es conveniente usar el estadístico de prueba:

\begin{equation}
	t_{\mu} = -2\ln{\lambda(\mu)}
\end{equation}

\noindent
donde ahora valores grandes de $t_{\mu}$ implica una gran incompatibilidad entre datos y $\mu$.

\tosolve{El paper de Cowan define estadísticos de prueba alternativos, pero no sé si son los que se usan con HF. Pongo esos pero capaz no van, y lo que sigue a continuación está de más.}

En muchos análisis la contribución del proceso de señal al valor medio de eventos se asume como no negativo, lo que implica que cualquier estimador de 
$\mu$ debería ser no negativo. Aún si no fuese así el caso, es conveniente definir un estimador efectivo $\hat{\mu}$ que maximice el likelihood y que tenga la posibilidad de tomar valores negativos (siempre y cuando los valores medios de Poisson, $\mu s_i + b_i$, sean no negativos). Esto va a permitir más adelante modelar a $\hat{\mu}$ como una variable con distribución gaussiana. 

Para un modelo con $\mu\ge0$ si se encuentra que su estimador es negativo ($\hat{\mu}<0$) entonces el mejor nivel de acuerdo entre datos y cualquier valor físico de $\mu$ va a ser cuando $\mu=0$. Por lo que se puede redefinir al PLR ($\tilde{\lambda}$) para generar un test estadístico alternativo que tenga en cuenta esto:

\begin{equation}
	\tilde{t}_{\mu}=-2\ln{\tilde{\lambda}(\mu)}=
	\begin{cases}
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(0, \doublehattwo{\bm{\theta}}(0))}} & \hat{\mu}<0 \\
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}} & \hat{\mu}\ge0 
	\end{cases}
\end{equation}


Un caso especial de este estadístico de prueba es cuando se analiza la hipótesis de `solo fondo' ($\mu=0$), ya que el rechazo de la misma puede llevar al descubrimiento de nueva señal:

\begin{equation}
	q_{0}=
	\begin{cases}
		-2\ln{\lambda(0)} & \hat{\mu}\ge0\\
		0 & \hat{\mu}<0
	\end{cases}
	\label{eq:st_q0}
\end{equation}

Si los datos observados resultan menores a las predicciones del fondo, se tiene $\hat{\mu}<0$. Esto podría significar una evidencia en contra de la hipótesis de `solo fondo', pero en realidad no muestra que los datos estén compuestos de eventos de señal. Con esta definición entonces la posibilidad de descartar la hipótesis de `solo fondo' ocurre solo cuando $\hat{\mu}>0$, y en caso contrario $q_{0}=$. El p-value para este estadístico de prueba queda entonces:

\begin{equation}
	p_0 = \int_{q_{0, \text{obs}}}^{\infty} f(q_0|0)dq_0
	\label{ec:pvalue_0}
\end{equation}

La comunidad de física de partículas tiende a definir un rechazo de hipótesis de `solo fondo' con una significancia superior a los $5\sigma$ ($p=2.87 \cdot 10^{-7}$) como un nivel apropiado para definir un descubrimiento. Para excluir hipótesis de señal se define en cambio a partir de $1.64$ sigmas ($p=0.05$). Cabe destacar que el rechazar la hipótesis de solo fondo es solo parte del proceso de descubrimiento de un nuevo fenómeno. La certeza de que un nuevo proceso está presente va a depender en general de otros factores, como la plausibilidad de una nueva hipótesis de señal, y el grado al cual la misma describe a los datos observados.

\section{Límites superiores de exclusión}

Cuando el p-value obtenido es mayor al límite definido para un descubrimiento, no es posible rechazar la hipótesis de `solo fondo', y en ese caso se desea establecer límites sobre el modelo caracterizado. Para ello se busca rechazar la hipótesis `señal+fondo', y encontrar el valor de $\mu$ para el cual no es más posible seguir haciendo ese rechazo (límite superior). Se define entonces un nuevo estadístico de prueba:

\tosolve{Nuevamente estoy poniendo la versión alternativa del estadístico de prueba, pero no se si efectivamente se usa ese...}

\begin{equation}
	\tilde{q}_{\mu}=
	\begin{cases}
		-2\ln{\tilde{\lambda}(\mu)} & \hat{\mu}\le\mu \\
		0 & \hat{\mu}>\mu \\
	\end{cases}=
	\begin{cases}
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(0, \doublehattwo{\bm{\theta}}(0))}} & \hat{\mu}<0 \\
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}} & 0\le\hat{\mu}\le \mu \\
		0 & \hat{\mu}>\mu \\
	\end{cases}
	\label{ec:st_qmu}
\end{equation}

% sacado de fran

\tosolve{Este párrafo no lo entiendo bien}

La razón para poner $\tilde{q}_{\mu} = 0$ para $\hat{\mu}>\mu$ es que cuando se establece un límite superior, el hecho de que $\hat{\mu}>\mu$ representa menos compatibilidad con $\mu$ que los datos obtenidos, y por lo tanto no se considera parte de la región de rechazo de la contrastación. Cabe destacar que $\tilde{q}_0$ anteriormente definido no es un caso particular de este estadístico de prueba, ya que $q_0$ es cero si los datos fluctúan hacia abajo ($\hat{\mu}\langle 0\rangle$), pero $\tilde{q}_{\mu}$ es cero si los datos fluctúan hacia arriba ($\hat{\mu}>\mu$).

Con este estadístico de prueba se busca encontrar el valor de $\mu$ para el cual deja de ser posible el rechazo de la hipótesis `señal+fondo' (o viceversa, hasta que valor de $\mu$ es posible un rechazo de la hipótesis). Para ello se definen el nivel de confianza \cite{Read:2002hq}:

\begin{equation}
	\text{CL}_{s} = \frac{p_{\mu}}{1-p_{b}} \equiv \frac{\text{CL}_{s+b}}{\text{CL}_{b}}
\end{equation}


\noindent 
donde

\begin{equation}
	p_{\mu} = \int_{q_{\mu, \text{obs}}}^{\infty} f(q_\mu|\mu)dq_\mu \quad\quad\quad \text{y} \quad\quad\quad 1-p_b = \int_{q_{\mu, obs}}^{\infty} f(q_\mu|0)dq_\mu \equiv \text{CL}_{b}
	\label{ec:pvalue_mu}
\end{equation}

\noindent
siendo $f(q_\mu|\mu)$ la pdf del estadístico de prueba $q_\mu$, y $f(q_\mu|0)$ la pdf bajo la hipótesis de `solo fondo'.

Cuanto más bajo es el $\text{CL}_{s}$, menos compatibilidad entre los datos y la hipótesis de `señal+fondo'. Se define por convención al límite superior ($\mu_{\text{up}}$) como aquel $\mu$ que tiene un $\text{CL}_{s}=5\%$, y se rechazan entonces lo modelos con $\mu$ menores a $\mu_{\text{up}}$.




\section{Aproximación de las distribuciones de los estadísticos de prueba}

Para hallar el p-value de una hipótesis es necesaria la función densidad de probabilidad del estadístico de prueba. Por ejemplo, para rechazar hipótesis nula se necesitaría se necesita calcular el p-value, que depende de $f(q_{0}|0)$ como muestra la Ecuación \ref{ec:pvalue_0}. Para poner límites superiores al modelo se necesitaría $f(q_{\mu}|\mu)$ y $f(q_{\mu}|0)$, como se ve en la Ecuación \ref{ec:pvalue_mu}. Inclusive se requiere de $f(q_{\mu}|\mu')$ con $\mu\neq\mu'$, para hallar la significancia esperada, empleada en la evaluación a priori de la sensibilidad del análisis (Sección \ref{sec:exp_sig}). En un principio dichas pdfs son desconocidas analíticamente, pero es posible obtenerlas empleando métodos alternativos.

Considerando al PLR de la Ecuación \ref{ec:plr}, determinado por el parámetro $\mu$, que puede ser cero (descubrimiento), o no (límites superiores), y suponiendo que los datos se distribuyen de acuerdo a un parámetro $\mu'$. La distribución $f(q_{\mu}|\mu')$ se puede aproximar utilizando el teorema de Wald \cite{10.2307/1990256}, que muestra que para el caso de un solo parámetro de interés:

\begin{equation}
	-2\ln{\lambda(\mu)}=\frac{(\mu-\hat{\mu})^{2}}{\sigma^{2}}+\mathcal{O}(1/\sqrt{N})
\end{equation}

\noindent
donde $\mu$ sigue una distribución Gaussiana con una media $\mu'$ y una desviación estándar $\sigma$, y $N$ representa el tamaño de la muestra. 
% Si despreciamos el término $\mathcal{O}(1/\sqrt{N})$ se puede mostrar que el estadístico de prueba $t_{\mu}$ sigue una distribución de $\chi^{2}$ no central con un grado de libertad.
En el límite asintótico se puede mostrar que el estadístico de prueba $t_{\mu}$ sigue una distribución de $\chi^{2}$ no central con un grado de libertad, y en ese caso el estadístico de prueba para descubrimiento ($q_{0}$) de la Ecuación \ref{eq:st_q0} puede aproximarse como:

\begin{equation}
	q_{0}=
	\begin{cases}
		\frac{(\mu-\hat{\mu})^{2}}{\sigma^{2}} & \hat{\mu}\le 0 \\
		0 & \hat{\mu}>0 \\
	\end{cases}
\end{equation}

De la misma forma, al p-value se lo puede aproximar como $p_{\mu}=1-\Phi(\sqrt{q_{\mu}})$ y a su correspondiente significancia equivalente como $Z_{\mu}=\sqrt{q_{\mu}}$. La misma aproximación vale para el estadístico de prueba para límites superiores ($q_\mu$) de la Ecuación \ref{ec:st_qmu}.

Cuando la estadística es reducida, se abandona el régimen asintótico, y las aproximaciones anteriores dejan de tener validez. \tosolve{Explicar toys: todavía no entiendo bien qué es lo que se varía} \tosolve{Debería explicar Asimov?} % O(\sim 10)
 
% Estas aproximaciones permiten conocer las distribuciones muéstrales y calcular valores-p y significancias en el caso de un gran número de datos, de una forma simple y computacionalmente poco
% costosa. A pesar de que estrictamente es válido para $N\to\infty$, esta aproximación es suficientemente
% precisa para un número de eventos de fondo $\sim \mathcal{O}(10)$.
% Para muestras de datos muy pequeñas, o en casos donde la precisión es importante, siempre pueden
% validarse estas aproximaciones utilizando la generación Monte Carlo. Para esto es necesario utilizar
% simulaciones Monte Carlo para generar lo que se denomina `pseudo-experimentos'. El procedimiento
% consiste en generar el conjunto de observables x utilizando la pdf $f(x|H)$ y calcular el valor del
% estadístico de prueba $t(x)$ para cada conjunto. Este proceso se repite hasta acumular suficiente
% estadística en la distribución muestral del estadístico $g(t|H)$.

\section{Significancia esperada}\label{sec:exp_sig}

El diseño de las regiones de señal para el análisis es un proceso denominado `optimización', que define el conjunto de cortes óptimo para discriminar el fondo de la señal. Tal discriminación es cuantizada mediante la significancia de la Ecuación \ref{ec:sign}, la cual a priori es desconocida. Para estimar el valor de significancia ($Z$) que uno esperaría tener un experimento, que espera observar un número de eventos igual a la suma de las estimaciones de fondo y señal ($n=s+b$), se puede emplear la significancia esperada, que se define como la mediana de $Z$. Como el p-value y $Z$ tienen una relación lineal y monotónica \cite{Cowan:2010js}, la mediana de $Z$ se puede obtener a partir de la mediana del p-value.


Por ejemplo, si tenemos una variable $n$ que sigue una distribución de Poisson y tiene media $s+b$, si la media es lo suficientemente grande, es posible aproximar a la misma mediante una distribución gaussiana, con media $s+b$ y desviación estándar $\sqrt{s+b}$. El p-value para la hipótesis de `solo fondo' ($s=0$) dada a observación $x$ es:

\begin{equation}
	p = 1 - \Phi\left( \frac{x-\mu}{\sigma} \right) = 1 - \Phi\left( \frac{x-b}{\sqrt{b}} \right)
\end{equation}

A partir de esto se puede obtener la significancia para descubrimiento:

\begin{equation}
	Z = \frac{x-b}{\sqrt{b}}
\end{equation}

La media de $Z$ coincide con la mediana, y como la media de $x$ es $s+b$, se obtiene:

\begin{equation}
	\text{med}[Z|s] = \frac{s}{\sqrt{b}}
\end{equation}

Esta magnitud fue históricamente empleada en física de partículas para la estimación de la significancia esperada. La misma se puede interpretar como la fracción de eventos de señal esperados con respecto a la incerteza estadística del número esperado de eventos total asumiendo la ausencia de señal.

Si el numero esperado de eventos de fondo $b$ es desconocido, debe ser incluido como parámetro nuisance. En ese caso $b$ puede ser ajustado libremente al número de eventos observados, y sería imposible rechazar la hipótesis de `solo fondo' ($s=0$) a menos que se introduzca información adicional que limite dicho parámetro. Para ellos se incluyen las regiones de control, donde no hay eventos de señal y donde la estimación del fondo en esta región puede relacionarse con la estimación en las regiones de señal. Estas regiones son incluidas la función likelihood como distribuciones de Poisson, de igual forma que las regiones de señal. Procediendo de forma similar que en el ejemplo anterior, empleando la  hipótesis de `solo fondo' ($s=0$) y la aproximación mediante `datos de Asimov' \cite{Cowan:2010js}, se puede obtener una expresión para la significancia esperada en función de la estimación del fondo basada en la región de control y su incertidumbre ($\sigma_b$):

\begin{equation}
	Z = \sqrt{ 2 (s+b) \ln{\left[ \frac{(s+b)(b+\sigma_b^2)}{b^2 + (s+b)\sigma_b^2} \right]} - \frac{2 b^2}{\sigma_b^2} \ln{ \left[ 1 + \frac{s \sigma_b^2}{b(b+\sigma_b^2)} \right] } }
\end{equation}

La estimación de la significancia esperada depende también del tipo de experimento que se desea realizar. En la práctica se emplea un método descripto en las Referencias \cite{Linnemann:2003vw, stat_1, ATL-PHYS-PUB-2020-025}, y que se engloba en una función del framework \texttt{ROOT} \footnote{\texttt{ROOT.RooStats.NumberCountingUtils.BinomialExpZ}}, que depende de la estimación de la señal y fondo, junto con su incertidumbre. En general, para la incerteza del fondo se hace una estimación conservadora del 30\%. 

% La búsqueda comienza definiendo las regiones de señal. Las mismas están caracterizadas por un estado final (motivado por un modelo en particular) que determina los cortes preliminares de la región. Adicional a esos cortes se agregan otros que aumentan el poder discriminatorio de las regiones de señal. El proceso de definir el conjunto de SRs y los cortes más aptos de cada una se denomina optimización. Es posible definir un conjunto de SRs optimizadas para discriminar al mismo modelo pero con distintos parámetros (masas por ejemplo), pudiendo estas ser independientes entre sí u ortogonales. Esto último puede ser ventajoso dependiendo de si se está realizando la búsqueda con el objetivo de descubrir algún fenómeno, o si se quiere poner límites al modelo estudiado. Cabe mencionar que si bien se buscan las regiones con mayor poder discriminatorio, es importante evitar definirlas basándose fuertemente en las predicciones del modelo. En caso de realizar una búsqueda muy dependiente del modelo y de no observar un exceso, se estarían poniedo límites a un modelo muy particular resultando poco útil para la comunidad científica. En por eso que el proceso de optimización, si bien está motivado por un estado final determinado por el modelo, termina siendo un compromiso entre un buen poder discriminatorio sin perder la idependencia al mismo. Una forma de garantizar esa independiencia es utilizar cortes un poco más relajados y pedir un número mínimo de eventos de señal y fondo.




% \section{Ajuste de solo fondo}

% Key ingredients of the fitting procedure are the ratios of expected event counts, called transfer
% factors, or TFs, of each normalized background process between each SR and each CR. The TFs
% allow the observations in the CRs to be converted into background estimates in the SRs, using:

% \begin{equation}
%     N^{(SR)}_{p}(est.) = N^{(SR)}_{p}(raw) \times \frac{N^{(CR)}_{p}(obs.)}{N^{(CR)}_{p}(est.)} = \mu_{p} \times N^{(SR)}_{p}(raw)
% \end{equation}

% where Np(SR,est.) is the SR background estimate for each simulated physics processes p considered
% in the analysis, Np(CR,obs.) is the observed number of data events in the CR for the process, and
% MCp(SR,raw) and MCp(CR,raw) are raw and unnormalized estimates of the contributions from
% the process to the SR and CR respectively, as obtained from MC simulation. An important feature of using TFs is that systematic uncertainties on the predicted background
% processes can be canceled in the extrapolation; a virtue of using the ratio of MC estimates. The total uncertainty on the number of background events in the SR is then a combination of the
% statistical uncertainties in the CR(s) and the residual systematic uncertainties of the extrapolation.
% For this reason, CRs are often defined by somewhat looser cuts than the SR, in order to increase
% CR data event statistics without significantly increasing residual uncertainties in the TFs, which in
% turn reduces the extrapolation uncertainties to the SR


\section{Modelo estadístico y flujo de búsqueda}

La función de likelihood empleada para esta tesis se escribe como:

\tosolve{Traté de que incluya la mayor información posible en una sola expresión, inclusive el ajuste solo fondo. Como en algunos ajustes se incluye el muSig en las CR (es cierto?), me pareció que podía poner a todas las regiones en una misma productoria (en el paper separan SR de CR porque a las CR no les incluye señal). También puse explícito el TF para los fondos que corresponda.}

\begin{equation}
	\begin{split}
	\mathcal{L} (\textbf{n}, \bm{\theta}^0 | \mu_\text{sig}, \bm{\mu}_b, \bm{\theta}) & = P_\text{SR+CR} \times  C_\text{syst} (\bm{\theta}^0, \bm{\theta}) \\
	& = \prod_{i \in \text{SR+CR}} \frac{(\mu_\text{sig} s_i + \bm{\mu}_b \cdot \textbf{b}_i)^{n_i}}{n_i!} \prod_{i \in S} G(\theta_j^0 - \theta_j) \\
	\end{split}
	\label{eq:analysis_lh}
\end{equation}


La misma se compone de la distribución de los datos observados en cada SR y CR, y un factor adicional que engloba las incertezas sistemáticas. Los datos obedecen la distribución de Poisson con media $\mu_\text{sig} s_i + \bm{\mu}_b \cdot \textbf{b}_i$, donde $\mu_\text{sig}$ es la intensidad de señal, $s_i$ es la estimación de señal en la región $i$, $\textbf{b}_i$ es la estimación de cada fondo para la región $i$, y $\bm{\mu}_b$ son los factores de normalización de cada fondo (cuyo uso se explica más adelante). Las incertezas sistemáticas ($S$) son incluidas usando la pdf $C_\text{syst} (\bm{\theta}^0, \bm{\theta})$, donde $\bm{\theta}^0$ son los valores centrales medidos alrededor de los cuales el parámetro $\bm{\theta}$ puede fluctuar al realizar un ajuste. Las variaciones de estos parámetros nuisance tiene un impacto directo en las estimaciones de los fondos y señal ($\textbf{b}_i$ y $s_i$). Para parámetros nuisance independientes, esta pdf es simplemente el producto de cada incerteza, la cual corresponde con una gaussiana con media $\theta_j^0 - \theta_j$. \tosolve{Mencionar pulls}

El estadístico de prueba empleado es el PLR descripto en la Ecuación \ref{ec:plr} a partir del likelihood de la Ecuación \ref{eq:analysis_lh}, y modificado para la evaluación del descubrimiento (Ecuación \ref{eq:st_q0}) o para los límites de exclusión (Ecuación \ref{ec:st_qmu}).

La búsqueda comienza con el diseño de las SRs a partir de la estimación de los procesos de señal y fondo, buscando maximizar la significancia esperada descripta en la Sección \ref{sec:exp_sig}. Luego se diseñan las CRs con el objetivo de normalizar los fondos de MC principales a los datos observados, realizando lo que se denomina el \textbf{Ajuste de solo fondo}. Para ello se omite en el likelihood a las SRs en la productoria y se fija $\mu_\text{sig}=0$. El objetivo de dicho ajuste es obtener el \textbf{factor de transferencia} ($\bm{\mu}_b$), que se aplica a cada fondo para corregir la estimación de los mismos a los datos en las CRs, y luego extrapolar dicha corrección a todas las regiones del análisis de la forma:

\tosolve{Acá mes desvié un poco con la notación, corregir!}

\begin{equation}
    N^{(SR)}_{p}(est.) = N^{(SR)}_{p}(raw) \times \frac{N^{(CR)}_{p}(obs.)}{N^{(CR)}_{p}(est.)} = \mu_{p} \times N^{(SR)}_{p}(raw)
\end{equation}

% los que no normalizamos los dejamos en 1
Una ventaja de emplear este método es que las incertezas sistemáticas en las predicciones de los fondos se cancelan en dicha extrapolación. La incerteza total en el número de fondo en cada SR es una combinación de la incerteza estadística de las CRs, y el residual de las incertezas sistemáticas. Para ello se emplea en las CR cortes más relajados, con la idea de aumentar la estadística sin incrementar las incertezas residuales, lo cual reduce la extrapolación de las incertezas a las SRs. El factor de transferencia se emplea también para las VRs, que de alguna forma evalúan la calidad de dicha extrapolación. Una vez que se considera que el modelado de los fondos es el adecuado, se procede a observar los datos en las SRs (\textit{unblinding}). A partir del unblind se puede evaluar si se está en presencia de un descubrimiento o no. El ajuste ahora es similar al anterior, con la hipótesis de `solo fondo' $\mu_\text{sig}=0$, pero incluyendo también las SRs y empleando los factores de transferencia calculados anteriormente \tosolve{Esto es así?}. Finalmente se calcula el p-value del estadístico de prueba y dependiendo de su valor se determina si se puede afirmar la observación de un nuevo fenómeno.

En caso de no haber descubrimiento es posible imponer límites sobre el modelo estudiado, en lo que se denomina \textbf{Ajuste dependiente del modelo}. Entonces se realiza un ajuste similar al de solo fondo, solo que ahora se considera la contribución de la señal tanto en las SRs como en las CRs, corregidos por el parámetro de intensidad de señal $\mu_\text{sig}\ne0$. Este ajuste se realiza para cada modelo de señal en cada SR, obteniéndose el $\mu_\text{sig}$, empleado para obtener aquellos modelos con $\text{CL}_{s}=5\%$. Los modelos en general se realizan variando algún parámetro (masa por ejemplo) y las simulaciones se realizan para valores discretos de dicho parámetro. En caso de que el límite se encuentre entre dos modelos, se realiza una extrapolación para calcular el límite adecuado.

Es posible a su vez establecer límites independientes del modelo, mediante lo que se denomina \textbf{Ajuste independiente del modelo}. Para ello se establecen límites superiores al número de eventos en cada SR, de tal forma de poder saber si algún modelo alternativo ya está excluido por el análisis actual, simplemente estimando el número de eventos de dicho modelo en las SRs. Para ello se realiza un ajuste similar a los anteriores, solo que ahora no se permite contaminación de señal en las CR \tosolve{Por qué?}, y como no se conoce los eventos esperados de señal, se emplea una señal \textit{dummy} dejando la estimación $s_i=1$. \tosolve{Consultar efectivamente qué es lo que se obtiene de este fit} \tosolve{Y ahora me surgió la duda: diferencias entre los expected y observed limits???}


% queda entonces:
% toys
% asimov
% consultar que estadistico de prueba se usa
% si esta bien el likelihood
% exclusion vs observed
% independent limit, que se busca en realidad?

