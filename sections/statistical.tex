\chapter{Estrategia general y elementos estadísticos para una búsqueda de nueva física}\label{cap:statistical}
% \addcontentsline{toc}{chapter}{Elementos estadísticos para la búsqueda de nueva física}
\chaptermark{Estrategia general y elementos estadísticos para una búsqueda de nueva física}

% \section{Estrategia general}\label{sec:statistical}

Una estrategia para llevar a cabo una búsqueda general de nueva física, consiste a grandes rasgos en realizar un experimento de conteo de eventos con características asociadas al modelo de estudio, y su comparación con las predicciones que el Modelo Estándar hace de eventos con las mismas características. En caso de que haya un `buen acuerdo' entre las predicciones del SM y los datos observados, es posible afirmar que bajo las condiciones del experimento no hay evidencia de nuevos procesos físicos y que las predicciones del SM son correctas. En el caso de observar una discrepancia entre las predicciones del SM y
los eventos observados (en la jerga denominada `exceso'), se puede afirmar que el SM tiene una carencia en sus predicciones y que se podría estar en presencia de un nuevo fenómeno físico. Los criterios para definir `buen acuerdo' y `exceso' requieren evaluaciones estadísticas rigurosas y se explican en el presente Capítulo. 

En el contexto de esta Tesis se denomina \textbf{señal} a los procesos del modelo teórico que motivan dicha búsqueda, y \textbf{fondo} a las predicciones del SM. Para poder identificar los eventos de señal es necesario conocer las características del mismo, y luego así, discriminarlos de otros procesos físicos presentes en el experimento. Se utilizan simulaciones de Monte Carlo para modelar la señal, reconstruyendo los observables cinemáticos que caracterizan a los eventos. Aplicando diferentes cortes en esas variables se puede favorecer ciertos procesos y desfavorecer otros, y el conjunto de dichos cortes define una \textbf{región} en el espacio de observables. Las regiones donde la señal abunda con respecto al fondo, y por ende donde se esperaría observar un exceso significativo en los datos, se denominan \textbf{Regiones de Señal (SR)}.

En este tipo de experimentos es fundamental un correcto modelado de los procesos de fondo. Existen diferentes técnicas para modelar estos procesos: basadas exclusivamente en datos, exclusivamente en simulaciones de Monte Carlo o basadas en simulaciones y corregidas con datos. La motivación de esta última se debe a que las simulaciones en general son validadas en regiones asociadas al proceso que modelan (SM por ejemplo), y como en este caso es necesario utilizarlas en regiones de señal, probablemente alejadas o más extremas de donde se validó, es esperable que esas predicciones en esas regiones no sean del todo correctas. Para ello se definen \textbf{Regiones de Control (CR)} donde abundan eventos de algún proceso de fondo de interés, dedicadas a normalizar las simulaciones de ese proceso en particular a los datos observados en la misma.

Finalmente se definen \textbf{Regiones de Validación (VR)} que justamente se utilizan para validar la estimación de los fondos anteriormente mencionados. Es importante destacar que el diseño de todas las regiones se realiza sin utilizar en ningún momento los datos en las SRs (\textit{blinding}) para evitar todo tipo de sesgo en el resultado del experimento. Por este motivo el diseño de todas las CRs y VRs debe ser ortogonal a las SRs, de tal forma que ningún evento de las mismas esté incluido en las SRs. Una vez que se tiene confianza en la estimación de los fondos mediante su validación en las distintas VRs, se procede a observar los datos en las SRs (\textit{unblinding}).

El concepto central en cualquier resultado estadístico es la probabilidad del modelo, que asigna una probabilidad a cada resultado posible del mismo. Un ejemplo muy utilizado en física de partículas es el modelo de Poisson que describe el resultado de un experimento de conteo:

\begin{equation}
	\text{Pois}(N|\mu) = \frac{\mu^{N}e^{-\mu}}{N!}
\end{equation}

\noindent
que define la probabilidad de observar $N$ veces cierto proceso aleatorio, medido en un intervalo fijo de tiempo, donde $\mu$ es el número medio de eventos esperado. La distribución de Poisson es utilizada para describir múltiples fenómenos como decaimientos radiactivos o cualquier experimento de partículas que conste de contar eventos en un intervalo de tiempo. Es importante mencionar que las probabilidades obtenidas en esta distribución dependen estrictamente del modelo asumido como hipótesis, en este caso representado por el número medio de eventos esperados ($\mu$). De tal forma que la probabilidad de obtener el número observado de eventos en el experimento va a depender del modelo a estudiar, los cuales por ejemplo pueden ser un modelo que sólo espera fondo, o un modelo que considera la composición de fondo y señal. La probabilidad de ocurrencia de los datos observados ($x$) bajo la hipótesis bajo estudio se denomina \textit{likelihood}:

\begin{equation}
	\mathcal{L}(x|\mu) = \text{Pois}(N=x|\mu) 
\end{equation}

\section{Likelihood máximo}

En el marco de esta Tesis se emplearon un conjunto de SRs, las cuales pueden ser consideradas como experimentos de conteo independientes con distribuciones de Poisson, cuyas predicciones se obtienen a partir de simulaciones. 
Es posible a su vez, realizar la búsqueda empleando distribuciones en alguna variable, las cuales requieren construir el modelo probabilístico que permita realizar predicciones de la misma. Si bien muchas distribuciones pueden ser derivadas de la teoría analíticamente, en general se utilizan simulaciones para generarlas. Esas simulaciones se describen mediante histogramas de la variable observada, y cada clase puede ser considerada como un experimento de conteo independiente con una distribución de Poisson. 
Para ambos métodos el \textit{likelihood} se escribe como el producto de las probabilidades de cada experimento:


\begin{equation}
	\mathcal{L}((x_1,x_2,...,x_N)|\mu) = \prod_{i=1}^N \text{Pois}(N=x_i|\mu) 
\end{equation}


El likelihood puede ser utilizado adicionalmente para estimar parámetros de la teoría que estamos estudiando. Por ejemplo, si nuestro modelo está caracterizado por un conjunto de parámetros $\bm{\theta}$, y asumimos que el mismo es verdadero, se esperaría que la probabilidad de observar dichos datos para ese modelo sea máxima cuando los parámetros $\bm{\theta}$ sean lo más próximo a los valores reales del modelo. El estimador de máximo likelihood (MLE) consiste en obtener los valores, $\hat{\bm{\theta}}$, de $\bm{\theta}$ que maximicen a la función likelihood. En general, para facilitar a los algoritmos computacionales de maximización, se busca en realidad el mínimo del logaritmo del likelihood (LLH):

\begin{equation}
	-\ln{\mathcal{L}(\bm{\theta}} = - \sum_{i=1}^{N}\ln{\text{Pois}(x_i|\bm{\theta})}
\end{equation}

Los estimadores tienen un sesgo proporcional a $1/N$. Cuando se tiende el número de mediciones a infinito el mismo se vuelve consistente, por lo que el valor estimado de cada parámetro converge al valor verdadero ($\bm{\theta}_0$). En dicho límite, el valor esperado del estimador coincide con $\bm{\theta}_0$ (no sesgado) y adquiere su menor varianza (eficiente).




\section{Contrastación de hipótesis}

Como se menciono anteriormente el experimento esta caracterizado por el conjunto de parámetros del modelo, denominados hipótesis. Con el objetivo de descubrir procesos de nueva señal se define la hipótesis nula ($H_0$), la cual se asume como verdadera y va a estar sujeta a prueba, evaluándola contra la hipótesis alternativa ($H_1$). En el contexto de esta búsqueda, la hipótesis nula (denominada también hipótesis de `solo fondo') asume que no hay eventos de señal y que todo lo observado debería ser fondo. A diferencia de la alternativa (hipótesis `señal+fondo') que sí predice eventos de señal. Si los resultados observados en el experimento difieren de los esperados bajo la hipótesis nula, es posible rechazar a la misma, y estar en presencia de un descubrimiento. Caso contrario de no poder rechazarla, es posible poner límites al modelo donde los roles de las hipótesis se invierten, y ahora la hipótesis nula incluye a la señal y la alternativa solo al fondo.

Las hipótesis pueden estar completamente determinadas o estar caracterizadas por distintos parámetros, y las mismas definen a las PDFs de los distintos observables. Para poder discriminar una hipótesis de otra se define un estadístico de prueba que es una función de los observables, $t(\textbf{x})$. El mismo tendrá asociada una función de densidad de probabilidad (pdf) dependiendo de la hipótesis ($g(t|H)$), que al aplicarle un corte en un dado valor ($t_c$) define una región critica en el espacio de observables. Si el valor de $t$ evaluado en los datos observados ($t_{\text{obs}}$) se encuentra dentro de esa región, la hipótesis nula es rechazada, como se muestra en la Figura \ref{fig:nullh}. En la misma se puede observar que la región pintada de azul ($\alpha$), denominada error de tipo I, representa la probabilidad de rechazar $H_0$ siendo esta es verdadera. Mientras que el área roja ($\beta$), denominada error de tipo II, representa la probabilidad de aceptar $H_0$ siendo esta es falsa. En un caso ideal, el estadístico de prueba obtendría los valores más pequeños posibles para $\alpha$ y $\beta$.

\begin{figure}
  \centering
  \includegraphics[width=0.65\textwidth]{images_tmp/nullh.png}
  \caption{Distribución del estadístico de prueba bajo la hipótesis nula $H_0$ (azul) y la alternativa$H_1$ (roja). El corte en $t_c$ define a la región crítica y de aceptancia. El área azul ($\alpha$) se denomina error de tipo I, y representa la probabilidad de rechazar $H_0$ siendo esta es verdadera. El área roja ($\beta$) se denomina error de tipo II, y representa la probabilidad de acepta $H_0$ siendo esta es falsa. \tosolve{hacer imagen propia}}
  \label{fig:nullh}
\end{figure}


Alternativamente, se puede cuantizar el acuerdo entre el resultado de dicha búsqueda y una hipótesis dada, calculando el \textit{p-value}. El mismo se define como la probabilidad bajo dicha hipótesis de obtener un resultado igual o peor de incompatible con las predicciones de la hipótesis:

\begin{equation}
	p_H = \int_{t_{obs}}^{\infty} g(t|H)dt
\end{equation}

Un p-value chico implica una evidencia importante en contra de dicha hipótesis, y la misma se excluye si el p-value observado es menor a un cierto valor previamente definido.  Alternativamente se puede convertir el p-value en la significancia equivalente, $Z$. La misma se define como el número de desviaciones estándar ($\sigma$) que se debe encontrar por encima de su media, una variable con distribución gaussiana para tener una probabilidad superior igual al p-value:

\begin{equation}
	Z=\Phi^{-1}(1-p)
\end{equation}

\noindent
donde $\Phi^{-1}$ es la inversa de la cumulativa (cuantil) de la distribución gaussiana. La comunidad de física de partículas tiende a definir un rechazo de hipótesis de solo fondo con una significancia superior a los $5\sigma$ ($p=2.87 \cdot 10^{-7}$) como un nivel apropiado para definir un descubrimiento. Para excluir hipótesis de señal se define en cambio a partir de $1.64$ sigmas ($p=0.05$). Cabe destacar que el rechazar la hipótesis de solo fondo es solo parte del proceso de descubrimiento de un nuevo fenómeno. La certeza de que un nuevo proceso está presente va a depender en general de otros factores, como la plausibilidad de una nueva hipótesis de señal, y el grado al cual la misma describe a los datos observados.


\section{Estadísticos de prueba}

% refrasear pues sacado de fran!!! Done!

El lema de Neyman-Pearson \cite{10.2307/91247} establece que el estadístico de prueba con mayor poder en la contrastación de hipótesis de `solo fondo' frente a hipótesis de `señal+fondo' (y viceversa), es el cociente de likelihoods:


\begin{equation}
	t(\textbf{x}) = \frac{\mathcal{L}(\textbf{x}|H_1)}{\mathcal{L}(\textbf{x}|H_0)}
\end{equation}

En este caso la mejor región crítica son aquellos \textbf{x} que satisfacen $t(\textbf{x})>c_\alpha$, donde $c_\alpha$ es una constante que se ajusta para que el tamaño de dicha muestra sea $\alpha$.

El procedimiento común entonces para establecer un descubrimiento en física de partículas, se basa en un experimento frecuentista de significancia, empleando el cociente de likelihoods como estadístico de prueba. Como se mencionó anteriormente, los modelos bajo estudio están caracterizados por un conjunto de parámetros. De dichos parámetros se separan aquellos de interés ($\mu$), de aquellos que a priori no se conoce su valor y deben ser ajustados a los datos, denominados \textit{nuisance} ($\bm{\theta}$). En el contexto de una búsqueda de nueva física, el parámetro $\mu$ representa la intensidad de la señal, de tal forma que la hipótesis de `solo-fondo' corresponde a $\mu = 0$, y la hipótesis de `señal+fondo' a $\mu = 1$. En cada región del análisis uno esperaría tener un una media de eventos dada por: $\langle n_i \rangle = \mu s_i + b_i$, donde $s_i$ ($b_i$) es el número esperado de eventos de señal (fondo) en la región $i$. Por otro lado, los parámetros nuisance representan las incertezas sistemáticas, provenientes de defectos en el modelado del detector o de la teoría, que idealmente se esperarían que fuesen despreciables. Para acercarse lo máximo posible a este escenario, los mismos son incluidos como parámetros a ajustar, con la consecuencia de reducir la sensibilidad del experimento.

En el caso donde hay un único parámetro de interés $\mu$, y el resto de parámetros son nuisance $\bm{\theta}$, es conveniente definir el \textit{Profile Likelihood Ratio} (PLR) \cite{Cowan:2010js}:

\begin{equation}
	\lambda(\mu)=\frac{\mathcal{L}(\mu, \doublehat{\bm{\theta}})}{\mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}
	\label{ec:plr}
\end{equation}

\noindent
donde en el denominador, los valores $\hat{\mu}$ y $\hat{\bm{\theta}})$ son los estimadores de MLE. De la misma forma en el numerador, los
parámetros $\doublehat{\bm{\theta}}$ son los valores que maximizan la función likelihood pero para un valor fijo de $\mu$. Este proceso de elegir valores específicos de los parámetros nuisance para un valor dado de $\mu$ se lo conoce como \textit{profiling}. El PLR depende explícitamente de $\mu$ pero es independiente de los parámetros nuisance que han sido `eliminados'
vía el profiling. La presencia de los parámetros nuisance que son ajustados a los datos ensanchan la función likelihood como función de $\mu$, respecto a la distribución si sus valores estuvieran fijos. De cierta forma reflejan una pérdida de información sobre $\mu$ debido a estos parámetros desconocidos.

\section{Descubrimiento}

De la definición de $\lambda(\mu)$ se puede observar que la misma puede tomar valores solamente entre 0 y 1, donde 1 implica un buen acuerdo entre los datos y el valor hipotetizado de $\mu$. De forma equivalente es conveniente usar el estadístico de prueba:

\begin{equation}
	t_{\mu} = -2\ln{\lambda(\mu)}
\end{equation}

\noindent
donde ahora valores grandes de $t_{\mu}$ implica una gran incompatibilidad entre datos y $\mu$.

\tosolve{El paper de Cowan define estadísticos de prueba alternativos, pero no sé si son los que se usan con HF}

En muchos análisis la contribución del proceso de señal al valor medio de eventos se asume como no negativo, lo que implica que cualquier estimador de 
$\mu$ debería ser no negativo. Aún si no fuese así el caso, es conveniente definir un estimador efectivo $\hat{\mu}$ que maximice el likelihood y que tenga la posibilidad de tomar valores negativos (siempre y cuando los valores medios de Poisson, $\mu s_i + b_i$, sean no negativos). Esto va a permitir más adelante modelar a $\hat{\mu}$ como una variable con distribución gaussiana. 

Para un modelo con $\mu\ge0$ si se encuentra que su estimador es negativo ($\hat{\mu}<0$) entonces el mejor nivel de acuerdo entre datos y cualquier valor físico de $\mu$ va a ser cuando $\mu=0$. Por lo que se puede redefinir al PLR ($\tilde{\lambda}$) para generar un test estadístico alternativo que tenga en cuenta esto:

\begin{equation}
	\tilde{t}_{\mu}=-2\ln{\tilde{\lambda}(\mu)}=
	\begin{cases}
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(0, \doublehattwo{\bm{\theta}}(0))}} & \hat{\mu}<0 \\
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}} & \hat{\mu}\ge0 
	\end{cases}
\end{equation}


Un caso especial de este estadístico de prueba es cuando se analiza la hipótesis de `solo fondo' ($\mu=0$), ya que el rechazo de la misma puede llevar al descubrimiento de nueva señal:

\begin{equation}
	q_{0}=
	\begin{cases}
		-2\ln{\lambda(0)} & \hat{\mu}\ge0\\
		0 & \hat{\mu}<0
	\end{cases}
	\label{eq:st_q0}
\end{equation}

Si los datos observados resultan menores a las predicciones del fondo, se tiene $\hat{\mu}<0$. Esto podría significar una evidencia en contra de la hipótesis de `solo fondo', pero en realidad no muestra que los datos estén compuestos de eventos de señal. Con esta definición entonces la posibilidad de descartar la hipótesis de `solo fondo' ocurre solo cuando $\hat{\mu}>0$, y en caso contrario $q_{0}=$. El p-value para este estadístico de prueba queda entonces:

\begin{equation}
	p_0 = \int_{q_{0, \text{obs}}}^{\infty} f(q_0|0)dq_0
	\label{ec:pvalue_0}
\end{equation}


\section{Límites superiores de exclusión}

Cuando el p-value obtenido es mayor al límite definido para un descubrimiento, no es posible rechazar la hipótesis de `solo fondo', y en ese caso se desea establecer límites sobre el modelo caracterizado. Para ello se busca rechazar la hipótesis `señal+fondo', y encontrar el valor de $\mu$ para el cual no es más posible seguir haciendo ese rechazo (límite superior). Se define entonces un nuevo estadístico de prueba:

\tosolve{Nuevamente estoy poniendo la versión alternativa del estadístico de prueba, pero no se si efectivamente se usa ese...}

\begin{equation}
	\tilde{q}_{\mu}=
	\begin{cases}
		-2\ln{\tilde{\lambda}(\mu)} & \hat{\mu}\le\mu \\
		0 & \hat{\mu}>\mu \\
	\end{cases}=
	\begin{cases}
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(0, \doublehattwo{\bm{\theta}}(0))}} & \hat{\mu}<0 \\
		-2\ln{\frac{\mathcal{L}(\mu, \doublehattwo{\bm{\theta}}(\mu))}{\rule{0pt}{0.49em} \mathcal{L}(\hat{\mu}, \hat{\bm{\theta}})}} & 0\le\hat{\mu}\le \mu \\
		0 & \hat{\mu}>\mu \\
	\end{cases}
	\label{ec:st_qmu}
\end{equation}

% sacado de fran

\tosolve{Este párrafo no lo entiendo bien}

La razón para poner $\tilde{q}_{\mu} = 0$ para $\hat{\mu}>\mu$ es que cuando se establece un límite superior, el hecho de que $\hat{\mu}>\mu$ representa menos compatibilidad con $\mu$ que los datos obtenidos, y por lo tanto no se considera parte de la región de rechazo de la contrastación. Cabe destacar que $\tilde{q}_0$ anteriormente definido no es un caso particular de este estadístico de prueba, ya que $q_0$ es cero si los datos fluctúan hacia abajo ($\hat{\mu}\langle 0\rangle$), pero $\tilde{q}_{\mu}$ es cero si los datos fluctúan hacia arriba ($\hat{\mu}>\mu$).

Con este estadístico de prueba se busca encontrar el valor de $\mu$ para el cual deja de ser posible el rechazo de la hipótesis `señal+fondo' (o viceversa, hasta que valor de $\mu$ es posible un rechazo de la hipótesis). Para ello se definen el nivel de confianza \cite{Read:2002hq}:

\begin{equation}
	\text{CL}_{s} = \frac{p_{\mu}}{1-p_{b}} \equiv \frac{\text{CL}_{s+b}}{\text{CL}_{b}}
\end{equation}


\noindent 
donde

\begin{equation}
	p_{\mu} = \int_{q_{\mu, \text{obs}}}^{\infty} f(q_\mu|\mu)dq_\mu \quad\quad\quad \text{y} \quad\quad\quad 1-p_b = \int_{q_{\mu, obs}}^{\infty} f(q_\mu|0)dq_\mu \equiv \text{CL}_{b}
	\label{ec:pvalue_mu}
\end{equation}

\noindent
siendo $f(q_\mu|\mu)$ la pdf del estadístico de prueba $q_\mu$, y $f(q_\mu|0)$ la pdf bajo la hipótesis de `solo fondo'.

Cuanto más bajo es el $\text{CL}_{s}$, menos compatibilidad entre los datos y la hipótesis de `señal+fondo'. Se define por convención al límite superior ($\mu_{\text{up}}$) como aquel $\mu$ que tiene un $\text{CL}_{s}=95\%$, y se rechazan entonces lo modelos con $\mu$ menores a $\mu_{\text{up}}$.




\section{Aproximación de las distribuciones de los estadísticos de prueba}

Para hallar el p-value de una hipótesis es necesaria la función densidad de probabilidad del estadístico de prueba. Por ejemplo, para rechazar hipótesis nula se necesitaría se necesita calcular el p-value, que depende de $f(q_{0}|0)$ como muestra la Ecuación \ref{ec:pvalue_0}. Para poner límites superiores al modelo se necesitaría $f(q_{\mu}|\mu)$ y $f(q_{\mu}|0)$, como se ve en la Ecuación \ref{ec:pvalue_mu}. Inclusive se requiere de $f(q_{\mu}|\mu')$ con $\mu\neq\mu'$, para hallar la significancia esperada, empleada en la evaluación a priori de la sensibilidad del análisis (Sección \ref{sec:exp_sig}). En un principio dichas pdfs son desconocidas analíticamente, pero es posible obtenerlas empleando métodos alternativos.

Considerando al PLR de la Ecuación \ref{ec:plr}, determinado por el parámetro $\mu$, que puede ser cero (descubrimiento), o no (límites superiores), y suponiendo que los datos se distribuyen de acuerdo a un parámetro $\mu'$. La distribución $f(q_{\mu}|\mu')$ se puede aproximar utilizando el teorema de Wald \cite{10.2307/1990256}, que muestra que para el caso de un solo parámetro de interés:

\begin{equation}
	-2\ln{\lambda(\mu)}=\frac{(\mu-\hat{\mu})^{2}}{\sigma^{2}}+\mathcal{O}(1/\sqrt{N})
\end{equation}

\noindent
donde $\mu$ sigue una distribución Gaussiana con una media $\mu'$ y una desviación estándar $\sigma$, y $N$ representa el tamaño de la muestra. 
% Si despreciamos el término $\mathcal{O}(1/\sqrt{N})$ se puede mostrar que el estadístico de prueba $t_{\mu}$ sigue una distribución de $\chi^{2}$ no central con un grado de libertad.
En el límite asintótico, $N\to\infty$, se puede mostrar que el estadístico de prueba $t_{\mu}$ sigue una distribución de $\chi^{2}$ no central con un grado de libertad, y en ese caso el estadístico de prueba para descubrimiento ($q_{0}$) de la Ecuación \ref{eq:st_q0} puede aproximarse como:

\begin{equation}
	q_{0}=
	\begin{cases}
		\frac{(\mu-\hat{\mu})^{2}}{\sigma^{2}} & \hat{\mu}\le 0 \\
		0 & \hat{\mu}>0 \\
	\end{cases}
\end{equation}

Al p-value se lo puede aproximar como $p_{\mu}=1-\Phi(\sqrt{q_{\mu}})$ y a su correspondiente significancia equivalente como $Z_{\mu}=\sqrt{q_{\mu}}$. La misma aproximación vale para el estadístico de prueba para límites superiores ($q_\mu$) de la Ecuación \ref{ec:st_qmu}.

Estas aproximaciones permiten conocer las distribuciones muéstrales y calcular valores-p y significancias en el caso de un gran número de datos, de una forma simple y computacionalmente poco
costosa. A pesar de que estrictamente es válido para $N\to\infty$, esta aproximación es suficientemente
precisa para un número de eventos de fondo $\sim \mathcal{O}(10)$.
Para muestras de datos muy pequeñas, o en casos donde la precisión es importante, siempre pueden
validarse estas aproximaciones utilizando la generación Monte Carlo. Para esto es necesario utilizar
simulaciones Monte Carlo para generar lo que se denomina `pseudo-experimentos'. El procedimiento
consiste en generar el conjunto de observables x utilizando la pdf $f(x|H)$ y calcular el valor del
estadístico de prueba $t(x)$ para cada conjunto. Este proceso se repite hasta acumular suficiente
estadística en la distribución muestral del estadístico $g(t|H)$.

\section{Optimización de las regiones de señal}\label{sec:exp_sig}

La búsqueda comienza definiendo las regiones de señal. Las mismas están caracterizadas por un estado final (motivado por un modelo en particular) que determina los cortes preliminares de la región. Adicional a esos cortes se agregan otros que aumentan el poder discriminatorio de las regiones de señal. El proceso de definir el conjunto de SRs y los cortes más aptos de cada una se denomina optimización. Es posible definir un conjunto de SRs optimizadas para discriminar al mismo modelo pero con distintos parámetros (masas por ejemplo), pudiendo estas ser independientes entre sí u ortogonales. Esto último puede ser ventajoso dependiendo de si se está realizando la búsqueda con el objetivo de descubrir algún fenómeno, o si se quiere poner límites al modelo estudiado. Cabe mencionar que si bien se buscan las regiones con mayor poder discriminatorio, es importante evitar definirlas basándose fuertemente en las predicciones del modelo. En caso de realizar una búsqueda muy dependiente del modelo y de no observar un exceso, se estarían poniedo límites a un modelo muy particular resultando poco útil para la comunidad científica. En por eso que el proceso de optimización, si bien está motivado por un estado final determinado por el modelo, termina siendo un compromiso entre un buen poder discriminatorio sin perder la idependencia al mismo. Una forma de garantizar esa independiencia es utilizar cortes un poco más relajados y pedir un número mínimo de eventos de señal y fondo.

 ...

% Una parte clave del proceso de optimización es poder definir a priori el poder discriminatorio de una dada signal region.
%  To
% establish a discovery of the signal, one can calculate the p-value of the hypothesis that s = 0,
% or equivalently it is convenient to use the Gaussian significance, Z. This is related to the
% p-value by
% Z = Φ−1(1 − p)
% where Φ−1 is the quantile of the standard Gaussian (inverse of the cumulative distribution).
% For the case where the p-value refers to the background-only (s = 0) hypothesis, we will
% refer to the corresponding Z value as the discovery significance. In particle physics a widely
% used threshold for a discovery has been a p-value of 2.9 × 10−7 or less, corresponding to a
% significance of Z = 5 or more
% When designing a new experiment it is important to know what discovery significance to
% expect if a certain signal model is in fact true. For this one can report the mean or median
% value of Z under assumption of some nominal value of s, i.e., assuming that n will have a
% mean of s + b. As noted in Ref. [1], because the p-value and significance Z have a nonlinear,
% monotonic relation, it is convenient to take “expected significance” to refer to the median,
% so that the median Z is given by Z evaluated with the median p.

%  a Poisson distributed quantity n with
% a large mean value s+b can be approximated by a Gaussian distributed variable x with mean
% s + b and standard deviation √s + b. The p-value of the background-only hypothesis given
% an observation x is therefore

% where μ = b and σ = √b refer to the mean and standard deviation of x under assumption of
% s = 0. Using this with Eq. (1) gives the discovery significance



\section{Ajuste de solo fondo}

Key ingredients of the fitting procedure are the ratios of expected event counts, called transfer
factors, or TFs, of each normalized background process between each SR and each CR. The TFs
allow the observations in the CRs to be converted into background estimates in the SRs, using:

\begin{equation}
    N^{(SR)}_{p}(est.) = N^{(SR)}_{p}(raw) \times \frac{N^{(CR)}_{p}(obs.)}{N^{(CR)}_{p}(est.)} = \mu_{p} \times N^{(SR)}_{p}(raw)
\end{equation}

where Np(SR,est.) is the SR background estimate for each simulated physics processes p considered
in the analysis, Np(CR,obs.) is the observed number of data events in the CR for the process, and
MCp(SR,raw) and MCp(CR,raw) are raw and unnormalized estimates of the contributions from
the process to the SR and CR respectively, as obtained from MC simulation. An important feature of using TFs is that systematic uncertainties on the predicted background
processes can be canceled in the extrapolation; a virtue of using the ratio of MC estimates. The total uncertainty on the number of background events in the SR is then a combination of the
statistical uncertainties in the CR(s) and the residual systematic uncertainties of the extrapolation.
For this reason, CRs are often defined by somewhat looser cuts than the SR, in order to increase
CR data event statistics without significantly increasing residual uncertainties in the TFs, which in
turn reduces the extrapolation uncertainties to the SR
